#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chatbot "O Guarani" - Vers√£o Simplificada e Robusta
Implementa√ß√£o das melhorias sem depend√™ncias problem√°ticas
"""

import numpy as np
import re
from datetime import datetime
from typing import List, Dict, Optional
import time

# Tentar importar bibliotecas opcionais
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è scikit-learn n√£o dispon√≠vel. Usando similaridade simplificada.")
    SKLEARN_AVAILABLE = False

try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import sent_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è NLTK n√£o dispon√≠vel. Usando processamento simplificado.")
    NLTK_AVAILABLE = False

class GuaraniChatbotSimplified:
    """
    Chatbot especializado em "O Guarani" - Vers√£o simplificada e robusta
    """
    
    def __init__(self):
        print("üöÄ Inicializando Chatbot O Guarani (Vers√£o Simplificada)")
        print("=" * 60)
        
        # Configura√ß√µes otimizadas baseadas nas melhorias
        self.chunk_size = 150
        self.overlap = 0.3
        self.similarity_threshold = 0.15
        self.top_chunks = 3
        self.sentence_level_search = True
        
        # Inicializar estruturas de dados
        self.conversation_history = []
        self.processing_log = []
        self.performance_metrics = []
        self.text_chunks = []
        self.chunk_sentences = []
        
        # Stop words em portugu√™s (lista expandida)
        self.stop_words = {
            'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'com', 'para',
            'por', 'que', 'se', 'na', 'no', 'ao', 'aos', 'as', 'os', 'mais',
            'mas', 'ou', 'ter', 'ser', 'estar', 'seu', 'sua', 'seus', 'suas',
            'foi', 's√£o', 'dos', 'das', 'pela', 'pelo', 'sobre', 'at√©', 'sem',
            'muito', 'bem', 'j√°', 'ainda', 's√≥', 'pode', 'tem', 'vai', 'vem',
            'ele', 'ela', 'eles', 'elas', 'isso', 'isto', 'aquilo', 'quando',
            'onde', 'como', 'porque', 'ent√£o', 'assim', 'aqui', 'ali', 'l√°'
        }
        
        # Texto expandido de O Guarani para demonstra√ß√£o
        self.texto_guarani = """
        O Guarani √© um romance indianista de Jos√© de Alencar, publicado em 1857. A narrativa se desenvolve no s√©culo XVII, 
        durante o per√≠odo colonial brasileiro, nas montanhas fluminenses pr√≥ximas ao rio Paquequer.
        
        Peri √© o protagonista da obra, um √≠ndio goitac√° de for√ßa herc√∫lea e lealdade inabal√°vel. Ele √© descrito como um 
        guerreiro corajoso, de estatura imponente e car√°ter nobre. Peri demonstra uma devo√ß√£o absoluta a Cec√≠lia (Ceci), 
        filha do fidalgo portugu√™s Dom Ant√¥nio de Mariz. Esta devo√ß√£o representa o amor imposs√≠vel entre duas ra√ßas distintas.
        
        Cec√≠lia, chamada carinhosamente de Ceci, √© uma jovem portuguesa de beleza singular e car√°ter doce. Ela √© filha 
        de Dom Ant√¥nio de Mariz e representa a pureza e a inoc√™ncia feminina idealizadas pelo Romantismo. Ceci desenvolve 
        sentimentos fraternais por Peri, vendo nele um protetor dedicado.
        
        Dom Ant√¥nio de Mariz √© um nobre portugu√™s, fidalgo da Casa Real, que se estabeleceu no Brasil ap√≥s cometer um crime 
        de honra em Portugal. Ele construiu um castelo fortificado nas margens do rio Paquequer, onde vive com sua fam√≠lia. 
        Dom Ant√¥nio √© caracterizado como um homem honrado, mas marcado pelo passado.
        
        Dona Lauriana √© a esposa de Dom Ant√¥nio, uma senhora portuguesa de origem nobre. Ela representa os valores 
        aristocr√°ticos europeus e inicialmente demonstra preconceito em rela√ß√£o aos ind√≠genas.
        
        √Ålvaro √© um jovem portugu√™s, primo de Cec√≠lia, que tamb√©m habita o castelo. Ele encarna o ideal do cavaleiro 
        medieval, sendo corajoso, nobre e apaixonado por Ceci. √Ålvaro representa a civiliza√ß√£o europeia em contraste 
        com a natureza selvagem de Peri.
        
        Isabel √© irm√£ de Cec√≠lia, uma jovem impetuosa e apaixonada. Ela se enamora de √Ålvaro, criando um tri√¢ngulo 
        amoroso que adiciona complexidade √†s rela√ß√µes familiares. Isabel possui um temperamento mais forte que sua irm√£.
        
        Loredano √© um dos antagonistas da hist√≥ria, um aventureiro italiano que se infiltra no castelo com inten√ß√µes 
        mal√©volas. Ele planeja assassinar Dom Ant√¥nio e se apossar de suas riquezas, representando a trai√ß√£o e a vilania.
        
        Os aimor√©s s√£o a tribo ind√≠gena antagonista, inimigos mortais de Peri e de sua tribo goitac√°. Eles representam 
        o perigo constante que amea√ßa a seguran√ßa dos habitantes do castelo. Os aimor√©s s√£o descritos como selvagens 
        e canibais, contrastando com a nobreza de Peri.
        
        A natureza brasileira desempenha papel fundamental na narrativa, sendo descrita com exuber√¢ncia e riqueza de 
        detalhes. Alencar retrata as florestas, rios e montanhas como cen√°rio √©pico que reflete o car√°ter dos personagens. 
        A paisagem tropical serve como pano de fundo para os conflitos entre civiliza√ß√£o e barb√°rie.
        
        O romance explora temas centrais como o amor imposs√≠vel entre ra√ßas diferentes, representado pela rela√ß√£o entre 
        Peri e Ceci. A lealdade e o sacrif√≠cio s√£o exemplificados pela devo√ß√£o absoluta do √≠ndio √† fam√≠lia Mariz. 
        O choque entre civiliza√ß√µes aparece no contraste entre os valores europeus e ind√≠genas.
        
        A linguagem de Alencar combina o portugu√™s erudito com tentativas de recriar a fala ind√≠gena, criando um estilo 
        √∫nico que busca expressar a realidade brasileira. O autor emprega descri√ß√µes rom√¢nticas e idealizadas tanto 
        dos personagens quanto da natureza.
        
        O desfecho tr√°gico da obra culmina com a destrui√ß√£o do castelo e a fuga de Peri e Ceci, simbolizando o nascimento 
        de uma nova ra√ßa brasileira atrav√©s da uni√£o simb√≥lica entre o √≠ndio e a portuguesa. Esta uni√£o representa a 
        forma√ß√£o da identidade nacional brasileira segundo a vis√£o rom√¢ntica de Alencar.
        """
        
        # Inicializar componentes
        self._init_vectorizer()
        
        self._log("Sistema inicializado com sucesso")
    
    def _log(self, message: str):
        """Registra eventos com timestamp"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.processing_log.append(log_entry)
        print(f"üìù {log_entry}")
    
    def _init_vectorizer(self):
        """Inicializa sistema de vetoriza√ß√£o"""
        if SKLEARN_AVAILABLE:
            try:
                self.vectorizer = TfidfVectorizer(
                    max_features=3000,
                    stop_words=list(self.stop_words),
                    ngram_range=(1, 2),
                    min_df=1,
                    max_df=0.95,
                    lowercase=True
                )
                self.use_tfidf = True
                self._log("TF-IDF inicializado com sucesso")
            except Exception as e:
                self._log(f"Erro TF-IDF: {e}. Usando similaridade simples.")
                self.use_tfidf = False
        else:
            self.use_tfidf = False
            self._log("Usando similaridade Jaccard simplificada")
    
    def fase1_carregar_texto(self):
        """Fase 1: An√°lise e carregamento do texto"""
        self._log("=== FASE 1: AN√ÅLISE DO TEXTO ===")
        
        # Estat√≠sticas b√°sicas
        chars = len(self.texto_guarani)
        words = self.texto_guarani.split()
        
        # An√°lise de senten√ßas (com ou sem NLTK)
        if NLTK_AVAILABLE:
            try:
                sentences = sent_tokenize(self.texto_guarani, language='portuguese')
            except:
                sentences = re.split(r'[.!?]+', self.texto_guarani)
                sentences = [s.strip() for s in sentences if s.strip()]
        else:
            sentences = re.split(r'[.!?]+', self.texto_guarani)
            sentences = [s.strip() for s in sentences if s.strip()]
        
        # Vocabul√°rio √∫nico
        word_tokens = re.findall(r'\\b\\w+\\b', self.texto_guarani.lower())
        unique_words = set(word_tokens)
        content_words = unique_words - self.stop_words
        
        # Log de estat√≠sticas
        self._log(f"Caracteres: {chars}")
        self._log(f"Palavras: {len(words)}")
        self._log(f"Senten√ßas: {len(sentences)}")
        self._log(f"Vocabul√°rio √∫nico: {len(unique_words)}")
        self._log(f"Palavras de conte√∫do: {len(content_words)}")
        
        return True
    
    def fase2_criar_chunks(self):
        """Fase 2: Cria√ß√£o de chunks otimizados"""
        self._log("=== FASE 2: CRIA√á√ÉO DE CHUNKS ===")
        
        # Limpeza do texto
        text = re.sub(r'\\n+', ' ', self.texto_guarani)
        text = re.sub(r'\\s+', ' ', text).strip()
        
        # Divis√£o em senten√ßas
        if NLTK_AVAILABLE:
            try:
                sentences = sent_tokenize(text, language='portuguese')
            except:
                sentences = re.split(r'[.!?]+', text)
                sentences = [s.strip() for s in sentences if s.strip()]
        else:
            sentences = re.split(r'[.!?]+', text)
            sentences = [s.strip() for s in sentences if s.strip()]
        
        # Cria√ß√£o de chunks com sobreposi√ß√£o
        chunks = []
        chunk_sentences_map = []
        current_chunk_sentences = []
        current_word_count = 0
        
        for sentence in sentences:
            words = sentence.split()
            sentence_word_count = len(words)
            
            if current_word_count + sentence_word_count <= self.chunk_size:
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
            else:
                # Finalizar chunk atual
                if current_chunk_sentences:
                    chunk_text = '. '.join(current_chunk_sentences) + '.'
                    chunks.append(chunk_text)
                    chunk_sentences_map.append(current_chunk_sentences.copy())
                
                # Sobreposi√ß√£o
                overlap_size = int(len(current_chunk_sentences) * self.overlap)
                if overlap_size > 0 and len(current_chunk_sentences) > overlap_size:
                    current_chunk_sentences = current_chunk_sentences[-overlap_size:]
                    current_word_count = sum(len(s.split()) for s in current_chunk_sentences)
                else:
                    current_chunk_sentences = []
                    current_word_count = 0
                
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
        
        # √öltimo chunk
        if current_chunk_sentences:
            chunk_text = '. '.join(current_chunk_sentences) + '.'
            chunks.append(chunk_text)
            chunk_sentences_map.append(current_chunk_sentences.copy())
        
        self.text_chunks = chunks
        self.chunk_sentences = chunk_sentences_map
        
        # Estat√≠sticas
        chunk_sizes = [len(chunk.split()) for chunk in chunks]
        self._log(f"Chunks criados: {len(chunks)}")
        self._log(f"Tamanho m√©dio: {np.mean(chunk_sizes):.1f} palavras")
        
        return True
    
    def fase3_indexar(self):
        """Fase 3: Indexa√ß√£o dos chunks"""
        self._log("=== FASE 3: INDEXA√á√ÉO ===")
        
        if self.use_tfidf and SKLEARN_AVAILABLE:
            try:
                # Preprocessar chunks para TF-IDF
                processed_chunks = [self._preprocess_for_tfidf(chunk) for chunk in self.text_chunks]
                self.chunk_vectors = self.vectorizer.fit_transform(processed_chunks)
                self._log(f"Vetores TF-IDF criados: {self.chunk_vectors.shape}")
                return True
            except Exception as e:
                self._log(f"Erro na vetoriza√ß√£o TF-IDF: {e}")
                self.use_tfidf = False
        
        # Fallback: usar similaridade simples
        self._log("Usando √≠ndice simplificado (sem vetoriza√ß√£o)")
        return True
    
    def _preprocess_for_tfidf(self, text: str) -> str:
        """Pr√©-processamento para TF-IDF"""
        text = text.lower()
        words = re.findall(r'\\b\\w+\\b', text)
        filtered_words = [word for word in words if word not in self.stop_words and len(word) > 2]
        return ' '.join(filtered_words)
    
    def calcular_similaridade_simples(self, pergunta: str, texto: str) -> float:
        """Similaridade Jaccard melhorada"""
        # Preprocessamento
        pergunta_words = set(re.findall(r'\\b\\w+\\b', pergunta.lower()))
        pergunta_clean = pergunta_words - self.stop_words
        
        texto_words = set(re.findall(r'\\b\\w+\\b', texto.lower()))
        texto_clean = texto_words - self.stop_words
        
        # Similaridade Jaccard
        if not pergunta_clean or not texto_clean:
            return 0.0
        
        intersection = len(pergunta_clean & texto_clean)
        union = len(pergunta_clean | texto_clean)
        jaccard = intersection / union if union > 0 else 0
        
        # Bonus para matches de palavras importantes
        important_words = pergunta_clean - {'quem', 'qual', 'onde', 'como', 'quando', 'sobre', 'fale'}
        exact_matches = len(important_words & texto_clean)
        bonus = min(exact_matches * 0.1, 0.3)
        
        return min(jaccard + bonus, 1.0)
    
    def fase4_responder(self, pergunta: str) -> str:
        """Fase 4: Gera√ß√£o de resposta"""
        start_time = time.time()
        self._log(f"=== CONSULTA: {pergunta} ===")
        
        if not self.text_chunks:
            return "‚ùå Sistema n√£o processado. Execute as fases anteriores."
        
        # Calcular similaridades
        if self.use_tfidf and hasattr(self, 'chunk_vectors'):
            try:
                # Usar TF-IDF
                processed_question = self._preprocess_for_tfidf(pergunta)
                question_vector = self.vectorizer.transform([processed_question])
                similarities = cosine_similarity(question_vector, self.chunk_vectors).flatten()
            except Exception as e:
                self._log(f"Erro TF-IDF, usando similaridade simples: {e}")
                similarities = [self.calcular_similaridade_simples(pergunta, chunk) for chunk in self.text_chunks]
        else:
            # Usar similaridade simples
            similarities = [self.calcular_similaridade_simples(pergunta, chunk) for chunk in self.text_chunks]
        
        # Processar resultados
        chunk_results = []
        for i, similarity in enumerate(similarities):
            chunk_results.append({
                'chunk_id': i,
                'chunk': self.text_chunks[i],
                'similarity': similarity,
                'sentences': self.chunk_sentences[i] if i < len(self.chunk_sentences) else []
            })
        
        # Ordenar por similaridade
        chunk_results.sort(key=lambda x: x['similarity'], reverse=True)
        
        max_sim = chunk_results[0]['similarity'] if chunk_results else 0
        mean_sim = np.mean(similarities) if similarities else 0
        
        self._log(f"Similaridade m√°xima: {max_sim:.3f}")
        self._log(f"Similaridade m√©dia: {mean_sim:.3f}")
        
        # Filtrar chunks relevantes
        relevant_chunks = [chunk for chunk in chunk_results if chunk['similarity'] >= self.similarity_threshold]
        
        self._log(f"Chunks relevantes: {len(relevant_chunks)}")
        
        # Gerar resposta
        if not relevant_chunks:
            response = self._resposta_nao_encontrada(pergunta, max_sim)
        else:
            response = self._gerar_resposta(pergunta, relevant_chunks[:self.top_chunks])
        
        # M√©tricas
        processing_time = time.time() - start_time
        self.performance_metrics.append({
            'pergunta': pergunta,
            'tempo': processing_time,
            'max_similarity': max_sim,
            'chunks_relevantes': len(relevant_chunks)
        })
        
        # Hist√≥rico
        self.conversation_history.append({
            'pergunta': pergunta,
            'resposta': response,
            'similaridade_max': max_sim,
            'chunks_usados': len(relevant_chunks),
            'tempo_resposta': processing_time,
            'timestamp': datetime.now()
        })
        
        self._log(f"Resposta gerada em {processing_time:.3f}s")
        return response
    
    def _resposta_nao_encontrada(self, pergunta: str, max_sim: float) -> str:
        """Resposta quando n√£o encontra informa√ß√µes relevantes"""
        base_msg = "N√£o encontrei informa√ß√µes espec√≠ficas sobre sua pergunta no texto de 'O Guarani'."
        
        if max_sim > 0.1:
            suggestion = "\\n\\nüí° Tente reformular usando termos mais espec√≠ficos da obra."
        elif max_sim > 0.05:
            suggestion = "\\n\\nüí° Use nomes de personagens ou eventos espec√≠ficos."
        else:
            suggestion = "\\n\\nüí° Sua pergunta pode estar fora do escopo da obra."
        
        examples = """
\\nüìù Exemplos de perguntas eficazes:
‚Ä¢ "Quem √© Peri?" ou "Fale sobre Peri"
‚Ä¢ "Quem √© Cec√≠lia?" ou "Descreva Ceci"  
‚Ä¢ "Qual a rela√ß√£o entre Peri e Cec√≠lia?"
‚Ä¢ "Quem s√£o os aimor√©s?"
‚Ä¢ "Onde se passa a hist√≥ria?"
‚Ä¢ "Quem √© Dom Ant√¥nio de Mariz?"
"""
        
        confidence = f"\\n\\nüî¥ Confian√ßa muito baixa (similaridade: {max_sim:.3f})"
        
        return base_msg + suggestion + examples + confidence
    
    def _gerar_resposta(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta baseada nos chunks relevantes"""
        if not chunks:
            return self._resposta_nao_encontrada(pergunta, 0)
        
        best_chunk = chunks[0]
        
        # Busca por senten√ßa se dispon√≠vel
        if self.sentence_level_search and best_chunk.get('sentences'):
            sentences = best_chunk['sentences']
            best_sentence = max(sentences, 
                              key=lambda s: self.calcular_similaridade_simples(pergunta, s))
            
            sentence_sim = self.calcular_similaridade_simples(pergunta, best_sentence)
            
            if sentence_sim > 0.2:
                # Usar senten√ßa espec√≠fica
                confidence = self._calcular_confianca(sentence_sim)
                return f"Com base em 'O Guarani':\\n\\n{best_sentence}\\n\\n{confidence}"
        
        # Usar chunk completo
        if len(chunks) == 1:
            main_content = chunks[0]['chunk']
            intro = "Com base no texto de 'O Guarani':\\n\\n"
        else:
            combined_content = ". ".join([chunk['chunk'] for chunk in chunks[:2]])
            main_content = combined_content
            intro = "Combinando informa√ß√µes de 'O Guarani':\\n\\n"
        
        # Truncar se muito longo
        if len(main_content) > 500:
            main_content = main_content[:500] + "..."
        
        confidence = self._calcular_confianca(best_chunk['similarity'])
        return intro + main_content + "\\n\\n" + confidence
    
    def _calcular_confianca(self, similarity: float) -> str:
        """Calcula indicador de confian√ßa"""
        if similarity > 0.5:
            return "üü¢ Confian√ßa muito alta"
        elif similarity > 0.35:
            return "üü¢ Confian√ßa alta"
        elif similarity > 0.25:
            return "üü° Confian√ßa moderada"
        elif similarity > 0.15:
            return "üü† Confian√ßa baixa - considere reformular"
        else:
            return "üî¥ Confian√ßa muito baixa"
    
    def executar_sistema_completo(self):
        """Executa todas as fases do sistema"""
        try:
            self._log("üöÄ EXECUTANDO SISTEMA COMPLETO")
            
            if not self.fase1_carregar_texto():
                raise Exception("Erro na Fase 1")
            
            if not self.fase2_criar_chunks():
                raise Exception("Erro na Fase 2")
            
            if not self.fase3_indexar():
                raise Exception("Erro na Fase 3")
            
            self._log("‚úÖ Sistema pronto para consultas!")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erro na execu√ß√£o: {e}")
            return False
    
    def executar_testes_automaticos(self):
        """Executa testes autom√°ticos"""
        perguntas_teste = [
            "Quem √© Peri?",
            "Fale sobre Cec√≠lia",
            "Quem √© Dom Ant√¥nio de Mariz?",
            "Qual a rela√ß√£o entre Peri e Cec√≠lia?",
            "Quem s√£o os aimor√©s?",
            "Onde se passa a hist√≥ria?",
            "Quando foi publicado O Guarani?",
            "Quais s√£o os temas da obra?",
            "Como fazer um bolo?",  # Deve ser rejeitada
            "Qual a capital da Fran√ßa?"  # Deve ser rejeitada
        ]
        
        print(f"\\nüß™ EXECUTANDO TESTES AUTOM√ÅTICOS ({len(perguntas_teste)} perguntas)")
        print("=" * 70)
        
        resultados = []
        
        for i, pergunta in enumerate(perguntas_teste, 1):
            print(f"\\nüìã Teste {i:2d}/{len(perguntas_teste)}: {pergunta}")
            
            resposta = self.fase4_responder(pergunta)
            ultimo_historico = self.conversation_history[-1]
            
            qualidade = self._avaliar_qualidade(ultimo_historico['similaridade_max'])
            
            resultado = {
                'pergunta': pergunta,
                'tempo': ultimo_historico['tempo_resposta'],
                'similaridade': ultimo_historico['similaridade_max'],
                'qualidade': qualidade
            }
            resultados.append(resultado)
            
            print(f"   ‚è±Ô∏è  {ultimo_historico['tempo_resposta']:.3f}s | üìä {ultimo_historico['similaridade_max']:.3f} | {qualidade}")
            
            if ultimo_historico['similaridade_max'] > 0.1:
                print(f"   üí¨ {resposta[:80]}...")
        
        self._relatorio_testes(resultados)
        return resultados
    
    def _avaliar_qualidade(self, similaridade: float) -> str:
        """Avalia qualidade da resposta"""
        if similaridade > 0.35:
            return "üü¢ Excelente"
        elif similaridade > 0.25:
            return "üü° Boa"
        elif similaridade > 0.15:
            return "üü† Regular"
        elif similaridade > 0.05:
            return "üî¥ Ruim"
        else:
            return "‚ùå Irrelevante"
    
    def _relatorio_testes(self, resultados: List[Dict]):
        """Relat√≥rio dos testes"""
        print(f"\\nüìã RELAT√ìRIO DOS TESTES")
        print("=" * 50)
        
        tempos = [r['tempo'] for r in resultados]
        similaridades = [r['similaridade'] for r in resultados]
        qualidades = [r['qualidade'] for r in resultados]
        
        print(f"üìä M√âTRICAS:")
        print(f"   ‚Ä¢ Tempo m√©dio: {np.mean(tempos):.3f}s")
        print(f"   ‚Ä¢ Similaridade m√©dia: {np.mean(similaridades):.3f}")
        
        excelentes = qualidades.count("üü¢ Excelente")
        boas = qualidades.count("üü° Boa")
        regulares = qualidades.count("üü† Regular")
        ruins = qualidades.count("üî¥ Ruim")
        irrelevantes = qualidades.count("‚ùå Irrelevante")
        
        total = len(resultados)
        print(f"\\nüéØ QUALIDADE:")
        print(f"   ‚Ä¢ Excelentes: {excelentes}/{total} ({excelentes/total*100:.1f}%)")
        print(f"   ‚Ä¢ Boas: {boas}/{total} ({boas/total*100:.1f}%)")
        print(f"   ‚Ä¢ Regulares: {regulares}/{total} ({regulares/total*100:.1f}%)")
        print(f"   ‚Ä¢ Ruins: {ruins}/{total} ({ruins/total*100:.1f}%)")
        print(f"   ‚Ä¢ Irrelevantes: {irrelevantes}/{total} ({irrelevantes/total*100:.1f}%)")
    
    def mostrar_estatisticas(self):
        """Mostra estat√≠sticas do sistema"""
        print(f"\\nüìä ESTAT√çSTICAS DO SISTEMA")
        print("=" * 40)
        print(f"üìù Chunks: {len(self.text_chunks)}")
        print(f"üîß Threshold: {self.similarity_threshold}")
        print(f"üìè Tamanho chunks: {self.chunk_size} palavras")
        print(f"üîÑ Sobreposi√ß√£o: {self.overlap * 100}%")
        print(f"üí¨ Consultas: {len(self.conversation_history)}")
        print(f"üõ†Ô∏è M√©todo: {'TF-IDF' if self.use_tfidf else 'Jaccard'}")
        
        if self.performance_metrics:
            tempos = [m['tempo'] for m in self.performance_metrics]
            print(f"‚è±Ô∏è Tempo m√©dio: {np.mean(tempos):.3f}s")
    
    def interface_chat(self):
        """Interface de chat interativa"""
        print(f"\\nü§ñ CHATBOT O GUARANI - CHAT INTERATIVO")
        print("=" * 50)
        print("Comandos: 'sair', 'stats', 'teste'")
        print("=" * 50)
        
        while True:
            try:
                pergunta = input("\\nüí¨ Sua pergunta: ").strip()
                
                if pergunta.lower() in ['sair', 'exit', 'quit']:
                    print("üëã At√© logo!")
                    break
                elif pergunta.lower() in ['stats', 'estatisticas']:
                    self.mostrar_estatisticas()
                    continue
                elif pergunta.lower() in ['teste', 'testes']:
                    self.executar_testes_automaticos()
                    continue
                
                if not pergunta:
                    continue
                
                resposta = self.fase4_responder(pergunta)
                print(f"\\nü§ñ {resposta}")
                
            except KeyboardInterrupt:
                print("\\nüëã Encerrando...")
                break

def main():
    """Fun√ß√£o principal"""
    print("üéØ CHATBOT O GUARANI - VERS√ÉO SIMPLIFICADA E ROBUSTA")
    print("=" * 60)
    
    chatbot = GuaraniChatbotSimplified()
    
    if chatbot.executar_sistema_completo():
        print("\\n‚úÖ Sistema inicializado com sucesso!")
        
        # Menu
        while True:
            print("\\nüéØ MENU:")
            print("1. üí¨ Chat interativo")
            print("2. üß™ Testes autom√°ticos")
            print("3. üìä Estat√≠sticas")
            print("4. üö™ Sair")
            
            try:
                opcao = input("\\nEscolha (1-4): ").strip()
                
                if opcao == '1':
                    chatbot.interface_chat()
                elif opcao == '2':
                    chatbot.executar_testes_automaticos()
                elif opcao == '3':
                    chatbot.mostrar_estatisticas()
                elif opcao == '4':
                    print("üëã Encerrando...")
                    break
                else:
                    print("‚ùå Op√ß√£o inv√°lida.")
                    
            except KeyboardInterrupt:
                print("\\nüëã Encerrando...")
                break
    else:
        print("‚ùå Falha na inicializa√ß√£o")

if __name__ == "__main__":
    main()