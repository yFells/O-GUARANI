"""
Chatbot "O Guarani" - Sistema de PLN para consultas sobre a obra de Jos√© de Alencar
Vers√£o melhorada com embeddings sem√¢nticos e otimiza√ß√µes
"""

import os
import numpy as np
import pandas as pd
import re
import pickle
from typing import List, Dict, Tuple
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Bibliotecas de PLN
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.stem import RSLPStemmer
except ImportError:
    print("‚ö†Ô∏è NLTK n√£o encontrado. Usando processamento simplificado.")
    nltk = None

# Sentence Transformers para embeddings sem√¢nticos
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Sentence-transformers n√£o encontrado. Usando TF-IDF como fallback.")
    from sklearn.feature_extraction.text import TfidfVectorizer
    EMBEDDINGS_AVAILABLE = False

class GuaraniChatbotImproved:
    """
    Chatbot especializado em responder perguntas sobre "O Guarani" de Jos√© de Alencar
    Vers√£o melhorada com embeddings sem√¢nticos
    """
    
    def __init__(self):
        print("üöÄ Inicializando Chatbot O Guarani (Vers√£o Melhorada)...")
        
        # Configura√ß√µes otimizadas
        self.chunk_size = 150      # Reduzido para chunks mais focados
        self.overlap = 0.3         # Sobreposi√ß√£o reduzida mas suficiente
        self.similarity_threshold = 0.15  # Limiar mais restritivo
        self.top_chunks = 3        # Top chunks para resposta
        self.sentence_level = True # Nova feature: busca no n√≠vel de senten√ßa
        
        # Inicializar estruturas de dados primeiro
        self.conversation_history = []
        self.processing_log = []
        self.performance_metrics = []
        
        # Dados do sistema
        self.text_chunks = []
        self.chunk_vectors = None
        self.original_text = ""
        self.sentences_per_chunk = []  # Novo: mapeamento de senten√ßas por chunk
        
        # Inicializa√ß√£o de componentes PLN (ap√≥s inicializar processing_log)
        self._init_nlp_components()
        
        self._log("Sistema inicializado com sucesso")
    
    def _init_nlp_components(self):
        """Inicializa componentes de PLN com fallbacks"""
        # Stop words
        try:
            if nltk:
                nltk.download('punkt', quiet=True)
                nltk.download('stopwords', quiet=True)
                nltk.download('rslp', quiet=True)
                self.stop_words = set(stopwords.words('portuguese'))
                self.stemmer = RSLPStemmer()
                self._log("NLTK inicializado com sucesso")
            else:
                raise ImportError("NLTK n√£o dispon√≠vel")
        except Exception as e:
            # Fallback: stop words b√°sicas em portugu√™s
            self.stop_words = {
                'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'com', 'para',
                'por', 'que', 'se', 'na', 'no', 'ao', 'aos', 'as', 'os', 'mais',
                'mas', 'ou', 'ter', 'ser', 'estar', 'seu', 'sua', 'seus', 'suas'
            }
            self.stemmer = None
            self._log(f"Usando stop words b√°sicas (NLTK indispon√≠vel: {e})")
        
        # Modelo de embeddings ou TF-IDF
        if EMBEDDINGS_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                self.use_embeddings = True
                self._log("Usando embeddings sem√¢nticos (SentenceTransformers)")
            except Exception as e:
                self._log(f"Erro ao carregar SentenceTransformers: {e}")
                self._init_tfidf_fallback()
        else:
            self._log("SentenceTransformers n√£o dispon√≠vel, usando TF-IDF")
            self._init_tfidf_fallback()
    
    def _init_tfidf_fallback(self):
        """Inicializa TF-IDF como fallback"""
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            stop_words=list(self.stop_words),  # Melhoramento: reintroduzir stop words
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.95,
            lowercase=True
        )
        self.use_embeddings = False
        self._log("Usando TF-IDF como m√©todo de vetoriza√ß√£o")
    
    def _log(self, message: str):
        """Registra eventos no hist√≥rico de processamento"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        
        # Verifica√ß√£o de seguran√ßa para evitar erros de inicializa√ß√£o
        if hasattr(self, 'processing_log'):
            self.processing_log.append(log_entry)
        else:
            # Fallback se processing_log ainda n√£o foi inicializado
            print(f"‚ö†Ô∏è  Log antes da inicializa√ß√£o: {log_entry}")
        
        print(f"üìù {log_entry}")
    
    def fase1_preparar_ambiente(self):
        """Fase 1: Prepara√ß√£o do ambiente e obten√ß√£o dos dados"""
        self._log("=== FASE 1: PREPARA√á√ÉO DO AMBIENTE ===")
        
        # Carregamento do texto
        texto_carregado = self._carregar_texto_guarani()
        
        if texto_carregado:
            self.original_text = texto_carregado
            self._log(f"Texto carregado: {len(texto_carregado)} caracteres")
            
            # Estat√≠sticas detalhadas
            stats = self._analyze_text_stats(texto_carregado)
            for key, value in stats.items():
                self._log(f"{key}: {value}")
            
            return True
        else:
            self._log("Erro: N√£o foi poss√≠vel carregar o texto")
            return False
    
    def _analyze_text_stats(self, text: str) -> Dict:
        """Analisa estat√≠sticas detalhadas do texto"""
        palavras = text.split()
        linhas = text.splitlines()
        
        # An√°lise de senten√ßas
        try:
            if nltk:
                sentences = sent_tokenize(text, language='portuguese')
            else:
                sentences = re.split(r'[.!?]+', text)
                sentences = [s.strip() for s in sentences if s.strip()]
        except:
            sentences = text.split('.')
        
        return {
            "Palavras": len(palavras),
            "Linhas": len(linhas),
            "Senten√ßas": len(sentences),
            "Caracteres": len(text),
            "Palavras √∫nicas": len(set([p.lower() for p in palavras if p.isalpha()])),
            "M√©dia palavras/senten√ßa": round(len(palavras) / len(sentences), 2) if sentences else 0
        }
    
    def _carregar_texto_guarani(self):
        """Carrega o texto com melhor tratamento de encoding"""
        # Primeiro, tenta carregar de arquivo
        for filename in ['guarani.txt', 'o_guarani.txt', 'alencar_guarani.txt']:
            result = self._try_load_file(filename)
            if result:
                return result
        
        # Se n√£o encontrou arquivo, usa texto de demonstra√ß√£o expandido
        self._log("Arquivo n√£o encontrado. Usando texto de demonstra√ß√£o expandido.")
        return self._texto_demo_expandido()
    
    def _try_load_file(self, filename: str):
        """Tenta carregar arquivo com diferentes encodings"""
        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-8-sig']
        
        for encoding in encodings:
            try:
                with open(filename, 'r', encoding=encoding) as arquivo:
                    texto = arquivo.read()
                    if len(texto) > 1000:  # Valida√ß√£o de tamanho m√≠nimo
                        self._log(f"Arquivo '{filename}' carregado com encoding: {encoding}")
                        return texto
                    else:
                        self._log(f"Arquivo '{filename}' muito pequeno ({len(texto)} chars)")
            except FileNotFoundError:
                continue
            except UnicodeDecodeError:
                continue
            except Exception as e:
                self._log(f"Erro ao ler '{filename}': {e}")
                continue
        
        return None
    
    def _texto_demo_expandido(self):
        """Texto de demonstra√ß√£o mais rico e detalhado"""
        return """
        O Guarani √© um romance indianista de Jos√© de Alencar, publicado em 1857. A narrativa se desenvolve no s√©culo XVII, 
        durante o per√≠odo colonial brasileiro, nas montanhas fluminenses pr√≥ximas ao rio Paquequer.
        
        Peri √© o protagonista da obra, um √≠ndio goitac√° de for√ßa herc√∫lea e lealdade inabal√°vel. Ele √© descrito como um 
        guerreiro corajoso, de estatura imponente e car√°ter nobre. Peri demonstra uma devo√ß√£o absoluta a Cec√≠lia (Ceci), 
        filha do fidalgo portugu√™s Dom Ant√¥nio de Mariz. Esta devo√ß√£o representa o amor imposs√≠vel entre duas ra√ßas distintas.
        
        Cec√≠lia, chamada carinhosamente de Ceci, √© uma jovem portuguesa de beleza singular e car√°ter doce. Ela √© filha 
        de Dom Ant√¥nio de Mariz e representa a pureza e a inoc√™ncia feminina idealizadas pelo Romantismo. Ceci desenvolve 
        sentimentos fraternais por Peri, vendo nele um protetor dedicado.
        
        Dom Ant√¥nio de Mariz √© um nobre portugu√™s, fidalgo da Casa Real, que se estabeleceu no Brasil ap√≥s cometer um crime 
        de honra em Portugal. Ele construiu um castelo fortificado nas margens do rio Paquequer, onde vive com sua fam√≠lia. 
        Dom Ant√¥nio √© caracterizado como um homem honrado, mas marcado pelo passado.
        
        Dona Lauriana √© a esposa de Dom Ant√¥nio, uma senhora portuguesa de origem nobre. Ela representa os valores 
        aristocr√°ticos europeus e inicialmente demonstra preconceito em rela√ß√£o aos ind√≠genas.
        
        √Ålvaro √© um jovem portugu√™s, primo de Cec√≠lia, que tamb√©m habita o castelo. Ele encarna o ideal do cavaleiro 
        medieval, sendo corajoso, nobre e apaixonado por Ceci. √Ålvaro representa a civiliza√ß√£o europeia em contraste 
        com a natureza selvagem de Peri.
        
        Isabel √© irm√£ de Cec√≠lia, uma jovem impetuosa e apaixonada. Ela se enamora de √Ålvaro, criando um tri√¢ngulo 
        amoroso que adiciona complexidade √†s rela√ß√µes familiares. Isabel possui um temperamento mais forte que sua irm√£.
        
        Loredano √© um dos antagonistas da hist√≥ria, um aventureiro italiano que se infiltra no castelo com inten√ß√µes 
        mal√©volas. Ele planeja assassinar Dom Ant√¥nio e se apossar de suas riquezas, representando a trai√ß√£o e a vilania.
        
        Os aimor√©s s√£o a tribo ind√≠gena antagonista, inimigos mortais de Peri e de sua tribo goitac√°. Eles representam 
        o perigo constante que amea√ßa a seguran√ßa dos habitantes do castelo. Os aimor√©s s√£o descritos como selvagens 
        e canibais, contrastando com a nobreza de Peri.
        
        A natureza brasileira desempenha papel fundamental na narrativa, sendo descrita com exuber√¢ncia e riqueza de 
        detalhes. Alencar retrata as florestas, rios e montanhas como cen√°rio √©pico que reflete o car√°ter dos personagens. 
        A paisagem tropical serve como pano de fundo para os conflitos entre civiliza√ß√£o e barb√°rie.
        
        O romance explora temas centrais como o amor imposs√≠vel entre ra√ßas diferentes, representado pela rela√ß√£o entre 
        Peri e Ceci. A lealdade e o sacrif√≠cio s√£o exemplificados pela devo√ß√£o absoluta do √≠ndio √† fam√≠lia Mariz. 
        O choque entre civiliza√ß√µes aparece no contraste entre os valores europeus e ind√≠genas.
        
        A linguagem de Alencar combina o portugu√™s erudito com tentativas de recriar a fala ind√≠gena, criando um estilo 
        √∫nico que busca expressar a realidade brasileira. O autor emprega descri√ß√µes rom√¢nticas e idealizadas tanto 
        dos personagens quanto da natureza.
        
        O desfecho tr√°gico da obra culmina com a destrui√ß√£o do castelo e a fuga de Peri e Ceci, simbolizando o nascimento 
        de uma nova ra√ßa brasileira atrav√©s da uni√£o simb√≥lica entre o √≠ndio e a portuguesa. Esta uni√£o representa a 
        forma√ß√£o da identidade nacional brasileira segundo a vis√£o rom√¢ntica de Alencar.
        
        O Guarani tornou-se uma das obras mais importantes do Romantismo brasileiro, influenciando a literatura nacional 
        e contribuindo para a constru√ß√£o do mito do "bom selvagem" e da identidade cultural brasileira.
        """
    
    def fase2_processar_dados(self):
        """Fase 2: Processamento otimizado dos dados"""
        self._log("=== FASE 2: PROCESSAMENTO AVAN√áADO DOS DADOS ===")
        
        # Limpeza aprimorada
        cleaned_text = self._advanced_text_cleaning(self.original_text)
        self._log("Texto limpo e normalizado")
        
        # Cria√ß√£o de chunks otimizada
        self.text_chunks = self._create_optimized_chunks(cleaned_text)
        self._log(f"Criados {len(self.text_chunks)} chunks otimizados")
        
        # Mapeamento de senten√ßas por chunk
        self._map_sentences_to_chunks()
        
        return self.text_chunks
    
    def _advanced_text_cleaning(self, text: str) -> str:
        """Limpeza avan√ßada preservando contexto importante"""
        # Normaliza√ß√£o de espa√ßos e quebras de linha
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Preservar pontua√ß√£o importante para segmenta√ß√£o
        text = re.sub(r'([.!?])\s*', r'\1 ', text)
        
        # Remover caracteres especiais mas preservar acentos
        text = re.sub(r'[^\w\s√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√ß√Å√â√ç√ì√ö√Ç√ä√é√î√õ√É√ï√á.!?;:,\-]', ' ', text)
        
        # Normaliza√ß√£o final
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _create_optimized_chunks(self, text: str) -> List[str]:
        """Cria√ß√£o otimizada de chunks com melhor contexto"""
        # Segmenta√ß√£o em senten√ßas
        try:
            if nltk:
                sentences = sent_tokenize(text, language='portuguese')
            else:
                sentences = re.split(r'[.!?]+', text)
                sentences = [s.strip() for s in sentences if s.strip()]
        except:
            sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        chunks = []
        current_chunk = []
        current_word_count = 0
        
        for sentence in sentences:
            words = sentence.split()
            sentence_word_count = len(words)
            
            # Verificar se a senten√ßa cabe no chunk atual
            if current_word_count + sentence_word_count <= self.chunk_size:
                current_chunk.append(sentence)
                current_word_count += sentence_word_count
            else:
                # Finalizar chunk atual se n√£o estiver vazio
                if current_chunk:
                    chunks.append('. '.join(current_chunk) + '.')
                
                # Come√ßar novo chunk com sobreposi√ß√£o
                overlap_sentences = int(len(current_chunk) * self.overlap)
                if overlap_sentences > 0 and len(current_chunk) > overlap_sentences:
                    current_chunk = current_chunk[-overlap_sentences:]
                    current_word_count = sum(len(s.split()) for s in current_chunk)
                else:
                    current_chunk = []
                    current_word_count = 0
                
                # Adicionar nova senten√ßa
                current_chunk.append(sentence)
                current_word_count += sentence_word_count
        
        # Adicionar √∫ltimo chunk
        if current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
        
        return chunks
    
    def _map_sentences_to_chunks(self):
        """Mapeia senten√ßas dentro de cada chunk para busca refinada"""
        self.sentences_per_chunk = []
        
        for chunk in self.text_chunks:
            try:
                if nltk:
                    sentences = sent_tokenize(chunk, language='portuguese')
                else:
                    sentences = re.split(r'[.!?]+', chunk)
                    sentences = [s.strip() for s in sentences if s.strip()]
            except:
                sentences = [s.strip() for s in chunk.split('.') if s.strip()]
            
            self.sentences_per_chunk.append(sentences)
    
    def fase3_armazenar_indexar(self, chunks: List[str]):
        """Fase 3: Indexa√ß√£o com embeddings sem√¢nticos ou TF-IDF melhorado"""
        self._log("=== FASE 3: INDEXA√á√ÉO SEM√ÇNTICA AVAN√áADA ===")
        
        if self.use_embeddings:
            # Usar embeddings sem√¢nticos
            self.chunk_vectors = self.embedding_model.encode(
                chunks, 
                show_progress_bar=True,
                convert_to_numpy=True
            )
            self._log(f"Embeddings criados: {self.chunk_vectors.shape}")
        else:
            # Usar TF-IDF melhorado
            processed_chunks = [self._preprocess_for_tfidf(chunk) for chunk in chunks]
            self.chunk_vectors = self.vectorizer.fit_transform(processed_chunks)
            self._log(f"Vetores TF-IDF criados: {self.chunk_vectors.shape}")
        
        # Salvar dados
        self._save_enhanced_data()
        
        return True
    
    def _preprocess_for_tfidf(self, text: str) -> str:
        """Pr√©-processamento espec√≠fico para TF-IDF"""
        text = text.lower()
        
        # Tokeniza√ß√£o
        words = re.findall(r'\b\w+\b', text)
        
        # Remo√ß√£o de stop words e palavras muito curtas
        filtered_words = [
            word for word in words 
            if word not in self.stop_words and len(word) > 2
        ]
        
        # Stemming se dispon√≠vel
        if self.stemmer:
            try:
                filtered_words = [self.stemmer.stem(word) for word in filtered_words]
            except:
                pass  # Continuar sem stemming se houver erro
        
        return ' '.join(filtered_words)
    
    def fase4_mecanismo_resposta(self, pergunta: str) -> str:
        """Fase 4: Mecanismo avan√ßado de recupera√ß√£o e gera√ß√£o"""
        start_time = datetime.now()
        self._log(f"=== CONSULTA: {pergunta} ===")
        
        if self.use_embeddings:
            response = self._resposta_com_embeddings(pergunta)
        else:
            response = self._resposta_com_tfidf(pergunta)
        
        # M√©tricas de performance
        processing_time = (datetime.now() - start_time).total_seconds()
        self.performance_metrics.append({
            'pergunta': pergunta,
            'tempo_processamento': processing_time,
            'metodo': 'embeddings' if self.use_embeddings else 'tfidf'
        })
        
        return response
    
    def _resposta_com_embeddings(self, pergunta: str) -> str:
        """Gera√ß√£o de resposta usando embeddings sem√¢nticos"""
        # Codificar pergunta
        question_vector = self.embedding_model.encode([pergunta])
        
        # Calcular similaridades
        similarities = cosine_similarity(question_vector, self.chunk_vectors).flatten()
        
        return self._process_similarities(pergunta, similarities)
    
    def _resposta_com_tfidf(self, pergunta: str) -> str:
        """Gera√ß√£o de resposta usando TF-IDF"""
        # Pr√©-processar pergunta
        processed_question = self._preprocess_for_tfidf(pergunta)
        question_vector = self.vectorizer.transform([processed_question])
        
        # Calcular similaridades
        similarities = cosine_similarity(question_vector, self.chunk_vectors).flatten()
        
        return self._process_similarities(pergunta, similarities)
    
    def _process_similarities(self, pergunta: str, similarities: np.ndarray) -> str:
        """Processa similaridades e gera resposta"""
        max_sim = np.max(similarities) if len(similarities) > 0 else 0
        mean_sim = np.mean(similarities) if len(similarities) > 0 else 0
        
        self._log(f"Similaridade m√°xima: {max_sim:.3f}, m√©dia: {mean_sim:.3f}")
        
        # Encontrar chunks relevantes
        relevant_chunks = []
        for i, similarity in enumerate(similarities):
            if similarity >= self.similarity_threshold:
                relevant_chunks.append({
                    'chunk_id': i,
                    'text': self.text_chunks[i],
                    'similarity': similarity,
                    'sentences': self.sentences_per_chunk[i] if i < len(self.sentences_per_chunk) else []
                })
        
        # Ordenar por similaridade
        relevant_chunks.sort(key=lambda x: x['similarity'], reverse=True)
        top_chunks = relevant_chunks[:self.top_chunks]
        
        self._log(f"Encontrados {len(relevant_chunks)} chunks relevantes")
        
        # Gerar resposta
        if not top_chunks:
            response = self._no_answer_response(pergunta, max_sim)
        else:
            if self.sentence_level and top_chunks:
                response = self._generate_sentence_level_response(pergunta, top_chunks)
            else:
                response = self._generate_chunk_level_response(pergunta, top_chunks)
        
        # Registrar no hist√≥rico
        self._update_conversation_history(pergunta, response, top_chunks, max_sim)
        
        return response
    
    def _generate_sentence_level_response(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta no n√≠vel de senten√ßa (mais precisa)"""
        # Encontrar a senten√ßa mais relevante dentro dos melhores chunks
        best_sentences = []
        
        for chunk_data in chunks[:2]:  # Usar top 2 chunks
            sentences = chunk_data['sentences']
            if not sentences:
                continue
            
            # Calcular similaridade para cada senten√ßa
            if self.use_embeddings:
                sentence_vectors = self.embedding_model.encode(sentences)
                question_vector = self.embedding_model.encode([pergunta])
                sentence_similarities = cosine_similarity(question_vector, sentence_vectors).flatten()
            else:
                # Para TF-IDF, usar uma aproxima√ß√£o simples
                sentence_similarities = [
                    len(set(pergunta.lower().split()) & set(sent.lower().split())) / 
                    len(set(pergunta.lower().split()) | set(sent.lower().split()))
                    for sent in sentences
                ]
            
            # Encontrar melhor senten√ßa neste chunk
            if sentence_similarities:
                best_idx = np.argmax(sentence_similarities)
                best_sentences.append({
                    'text': sentences[best_idx],
                    'similarity': sentence_similarities[best_idx],
                    'chunk_similarity': chunk_data['similarity']
                })
        
        # Selecionar a melhor senten√ßa geral
        if best_sentences:
            best_sentences.sort(key=lambda x: x['similarity'], reverse=True)
            best_sentence = best_sentences[0]
            
            confidence = self._calculate_confidence(best_sentence['similarity'])
            
            response = f"Com base em 'O Guarani':\n\n{best_sentence['text']}"
            response += f"\n\n{confidence}"
            
            return response
        else:
            # Fallback para resposta por chunk
            return self._generate_chunk_level_response(pergunta, chunks)
    
    def _generate_chunk_level_response(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta no n√≠vel de chunk"""
        if len(chunks) == 1:
            main_content = chunks[0]['text']
            intro = "Com base no texto de 'O Guarani':\n\n"
        else:
            # Combinar informa√ß√µes dos melhores chunks
            combined_text = " ".join([chunk['text'] for chunk in chunks[:2]])
            main_content = combined_text
            intro = "Combinando informa√ß√µes de 'O Guarani':\n\n"
        
        # Truncar se muito longo
        if len(main_content) > 600:
            main_content = main_content[:600] + "..."
        
        confidence = self._calculate_confidence(chunks[0]['similarity'])
        
        return intro + main_content + "\n\n" + confidence
    
    def _calculate_confidence(self, similarity: float) -> str:
        """Calcula e retorna indicador de confian√ßa"""
        if similarity > 0.7:
            return "‚úÖ (Confian√ßa muito alta)"
        elif similarity > 0.5:
            return "üü¢ (Confian√ßa alta)"
        elif similarity > 0.3:
            return "üü° (Confian√ßa moderada)"
        elif similarity > 0.15:
            return "üü† (Confian√ßa baixa - considere reformular)"
        else:
            return "üî¥ (Confian√ßa muito baixa)"
    
    def _no_answer_response(self, pergunta: str, max_similarity: float) -> str:
        """Resposta quando n√£o encontra informa√ß√µes relevantes"""
        base_msg = "N√£o encontrei informa√ß√µes espec√≠ficas sobre sua pergunta no texto de 'O Guarani'."
        
        if max_similarity > 0.05:
            suggestion = " Tente reformular a pergunta ou ser mais espec√≠fico sobre personagens, eventos ou temas da obra."
        else:
            suggestion = " Sua pergunta pode estar fora do escopo da obra ou usar termos muito diferentes do texto original."
        
        examples = "\n\nExemplos de perguntas: 'Quem √© Peri?', 'Fale sobre Cec√≠lia', 'Qual o enredo?', 'Onde se passa a hist√≥ria?'"
        
        return base_msg + suggestion + examples
    
    def _update_conversation_history(self, pergunta: str, resposta: str, chunks: List[Dict], max_sim: float):
        """Atualiza hist√≥rico de conversa com mais detalhes"""
        self.conversation_history.append({
            'timestamp': datetime.now(),
            'pergunta': pergunta,
            'resposta': resposta,
            'chunks_usados': len(chunks),
            'similaridade_max': max_sim,
            'metodo': 'embeddings' if self.use_embeddings else 'tfidf',
            'chunks_ids': [c['chunk_id'] for c in chunks] if chunks else []
        })
    
    def _save_enhanced_data(self):
        """Salvamento de dados aprimorado"""
        # Em ambiente real, salvaria em disco
        self._log("Dados indexados salvos em mem√≥ria")
    
    def fase5_interface_usuario(self):
        """Fase 5: Interface interativa melhorada"""
        self._log("=== FASE 5: INTERFACE INTERATIVA ===")
        
        print("\n" + "="*70)
        print("ü§ñ CHATBOT O GUARANI - VERS√ÉO MELHORADA")
        print("Assistente especializado na obra de Jos√© de Alencar")
        print(f"M√©todo: {'Embeddings Sem√¢nticos' if self.use_embeddings else 'TF-IDF'}")
        print("Comandos: 'sair', 'historico', 'stats', 'ajuda'")
        print("="*70)
        
        while True:
            try:
                pergunta = input("\nüí¨ Sua pergunta: ").strip()
                
                if pergunta.lower() in ['sair', 'exit', 'quit']:
                    print("üëã At√© logo!")
                    break
                elif pergunta.lower() in ['historico', 'hist√≥rico', 'history']:
                    self.mostrar_historico_resumido()
                    continue
                elif pergunta.lower() in ['stats', 'estat√≠sticas', 'estatisticas']:
                    self.mostrar_estatisticas()
                    continue
                elif pergunta.lower() in ['ajuda', 'help']:
                    self.mostrar_ajuda()
                    continue
                
                if not pergunta:
                    continue
                
                resposta = self.fase4_mecanismo_resposta(pergunta)
                print(f"\nü§ñ {resposta}")
                
            except KeyboardInterrupt:
                print("\nüëã Encerrando...")
                break
    
    def mostrar_ajuda(self):
        """Mostra ajuda para o usu√°rio"""
        help_text = """
        üÜò AJUDA - CHATBOT O GUARANI
        
        üìù Tipos de perguntas que funcionam bem:
        ‚Ä¢ Sobre personagens: "Quem √© Peri?", "Fale sobre Cec√≠lia"
        ‚Ä¢ Sobre enredo: "Qual √© a hist√≥ria?", "O que acontece no final?"
        ‚Ä¢ Sobre relacionamentos: "Qual a rela√ß√£o entre Peri e Ceci?"
        ‚Ä¢ Sobre cen√°rio: "Onde se passa a hist√≥ria?"
        ‚Ä¢ Sobre temas: "Quais s√£o os temas principais?"
        
        üéõÔ∏è Comandos especiais:
        ‚Ä¢ 'historico' - Ver hist√≥rico de conversas
        ‚Ä¢ 'stats' - Ver estat√≠sticas do sistema
        ‚Ä¢ 'ajuda' - Mostrar esta ajuda
        ‚Ä¢ 'sair' - Encerrar o chatbot
        
        üí° Dicas para melhores respostas:
        ‚Ä¢ Seja espec√≠fico em suas perguntas
        ‚Ä¢ Use nomes de personagens conhecidos
        ‚Ä¢ Reformule se a resposta n√£o for satisfat√≥ria
        """
        print(help_text)
    
    def mostrar_historico_resumido(self):
        """Mostra hist√≥rico resumido das conversas"""
        if not self.conversation_history:
            print("üì≠ Nenhuma conversa no hist√≥rico ainda.")
            return
        
        print(f"\nüìö HIST√ìRICO ({len(self.conversation_history)} conversas):")
        print("-" * 50)
        
        for i, conv in enumerate(self.conversation_history[-5:], 1):  # √öltimas 5
            timestamp = conv['timestamp'].strftime("%H:%M")
            print(f"{i}. [{timestamp}] {conv['pergunta']}")
            print(f"   Similarity: {conv['similaridade_max']:.3f} | Chunks: {conv['chunks_usados']}")
    
    def mostrar_estatisticas(self):
        """Mostra estat√≠sticas detalhadas do sistema"""
        print(f"\nüìä ESTAT√çSTICAS DO SISTEMA")
        print("=" * 40)
        print(f"Chunks de texto: {len(self.text_chunks)}")
        print(f"M√©todo de vetoriza√ß√£o: {'Embeddings' if self.use_embeddings else 'TF-IDF'}")
        print(f"Threshold de similaridade: {self.similarity_threshold}")
        print(f"Tamanho dos chunks: {self.chunk_size} palavras")
        print(f"Sobreposi√ß√£o: {self.overlap * 100}%")
        print(f"Consultas realizadas: {len(self.conversation_history)}")
        
        if self.performance_metrics:
            tempos = [m['tempo_processamento'] for m in self.performance_metrics]
            print(f"Tempo m√©dio de resposta: {np.mean(tempos):.3f}s")
        
        if self.conversation_history:
            similaridades = [c['similaridade_max'] for c in self.conversation_history]
            print(f"Similaridade m√©dia: {np.mean(similaridades):.3f}")
    
    def executar_sistema_completo(self):
        """Executa todas as fases do sistema com tratamento de erros"""
        try:
            # Fase 1: Prepara√ß√£o
            if not self.fase1_preparar_ambiente():
                raise Exception("Falha na prepara√ß√£o do ambiente")
            
            # Fase 2: Processamento
            chunks = self.fase2_processar_dados()
            if not chunks:
                raise Exception("Falha no processamento dos dados")
            
            # Fase 3: Indexa√ß√£o
            if not self.fase3_armazenar_indexar(chunks):
                raise Exception("Falha na indexa√ß√£o")
            
            self._log("‚úÖ Sistema inicializado com sucesso!")
            
            # Teste autom√°tico
            self.executar_testes_automaticos()
            
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erro na execu√ß√£o: {e}")
            return False
    
    def executar_testes_automaticos(self):
        """Executa testes autom√°ticos abrangentes"""
        perguntas_teste = [
            "Quem √© Peri?",
            "Fale sobre Cec√≠lia",
            "Qual √© o enredo do livro?",
            "Quem s√£o os personagens principais?",
            "Onde se passa a hist√≥ria?",
            "Qual a rela√ß√£o entre Peri e Ceci?",
            "Quem √© Dom Ant√¥nio de Mariz?",
            "O que s√£o os aimor√©s?",
            "Quando foi publicado O Guarani?",
            "Quais s√£o os temas da obra?"
        ]
        
        print("\n" + "="*60)
        print("üß™ EXECUTANDO TESTES AUTOM√ÅTICOS")
        print("="*60)
        
        resultados = []
        
        for i, pergunta in enumerate(perguntas_teste, 1):
            print(f"\nüîç Teste {i}/10: {pergunta}")
            
            start_time = datetime.now()
            resposta = self.fase4_mecanismo_resposta(pergunta)
            tempo = (datetime.now() - start_time).total_seconds()
            
            # An√°lise da qualidade da resposta
            ultimo_historico = self.conversation_history[-1]
            qualidade = self._avaliar_qualidade_resposta(ultimo_historico)
            
            resultados.append({
                'pergunta': pergunta,
                'tempo': tempo,
                'similaridade': ultimo_historico['similaridade_max'],
                'chunks': ultimo_historico['chunks_usados'],
                'qualidade': qualidade
            })
            
            print(f"‚è±Ô∏è  Tempo: {tempo:.3f}s | Similaridade: {ultimo_historico['similaridade_max']:.3f} | {qualidade}")
            print(f"üìù Resposta: {resposta[:100]}...")
        
        # Relat√≥rio final
        self._gerar_relatorio_testes(resultados)
    
    def _avaliar_qualidade_resposta(self, historico: Dict) -> str:
        """Avalia a qualidade da resposta baseada em m√©tricas"""
        sim = historico['similaridade_max']
        chunks = historico['chunks_usados']
        
        if sim > 0.4 and chunks > 0:
            return "üü¢ Excelente"
        elif sim > 0.25 and chunks > 0:
            return "üü° Boa"
        elif sim > 0.15 and chunks > 0:
            return "üü† Regular"
        else:
            return "üî¥ Ruim"
    
    def _gerar_relatorio_testes(self, resultados: List[Dict]):
        """Gera relat√≥rio final dos testes"""
        print("\n" + "="*60)
        print("üìã RELAT√ìRIO FINAL DOS TESTES")
        print("="*60)
        
        tempos = [r['tempo'] for r in resultados]
        similaridades = [r['similaridade'] for r in resultados]
        
        print(f"üìä M√©tricas Gerais:")
        print(f"   ‚Ä¢ Tempo m√©dio: {np.mean(tempos):.3f}s")
        print(f"   ‚Ä¢ Tempo m√°ximo: {np.max(tempos):.3f}s")
        print(f"   ‚Ä¢ Similaridade m√©dia: {np.mean(similaridades):.3f}")
        print(f"   ‚Ä¢ Similaridade m√≠nima: {np.min(similaridades):.3f}")
        
        qualidades = [r['qualidade'] for r in resultados]
        excelentes = qualidades.count("üü¢ Excelente")
        boas = qualidades.count("üü° Boa")
        regulares = qualidades.count("üü† Regular")
        ruins = qualidades.count("üî¥ Ruim")
        
        print(f"\nüéØ Qualidade das Respostas:")
        print(f"   ‚Ä¢ Excelentes: {excelentes}/10 ({excelentes*10}%)")
        print(f"   ‚Ä¢ Boas: {boas}/10 ({boas*10}%)")
        print(f"   ‚Ä¢ Regulares: {regulares}/10 ({regulares*10}%)")
        print(f"   ‚Ä¢ Ruins: {ruins}/10 ({ruins*10}%)")
        
        # Recomenda√ß√µes
        if np.mean(similaridades) < 0.2:
            print("\n‚ö†Ô∏è  Recomenda√ß√£o: Similaridades baixas. Considere:")
            print("    - Verificar se o texto est√° carregado corretamente")
            print("    - Ajustar threshold de similaridade")
            print("    - Melhorar o pr√©-processamento")


def main():
    """Fun√ß√£o principal melhorada"""
    print("üöÄ Iniciando Chatbot O Guarani - Vers√£o Melhorada")
    print("=" * 60)
    
    chatbot = GuaraniChatbotImproved()
    
    if chatbot.executar_sistema_completo():
        print("\n‚úÖ Sistema inicializado com sucesso!")
        
        # Menu de op√ß√µes
        while True:
            print("\nüéØ OP√á√ïES DISPON√çVEIS:")
            print("1. üí¨ Iniciar chat interativo")
            print("2. üìä Ver estat√≠sticas do sistema")
            print("3. üìö Ver hist√≥rico completo")
            print("4. üß™ Executar novos testes")
            print("5. üö™ Sair")
            
            try:
                opcao = input("\nEscolha uma op√ß√£o (1-5): ").strip()
                
                if opcao == '1':
                    chatbot.fase5_interface_usuario()
                elif opcao == '2':
                    chatbot.mostrar_estatisticas()
                elif opcao == '3':
                    chatbot.mostrar_historico_resumido()
                elif opcao == '4':
                    chatbot.executar_testes_automaticos()
                elif opcao == '5':
                    print("üëã Encerrando sistema...")
                    break
                else:
                    print("‚ùå Op√ß√£o inv√°lida. Tente novamente.")
                    
            except KeyboardInterrupt:
                print("\nüëã Encerrando...")
                break
    else:
        print("‚ùå Falha na inicializa√ß√£o do sistema")

if __name__ == "__main__":
    main()