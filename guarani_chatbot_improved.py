"""
Chatbot "O Guarani" - Sistema de PLN para consultas sobre a obra de JosÃ© de Alencar
VersÃ£o melhorada com embeddings semÃ¢nticos e otimizaÃ§Ãµes
"""

import os
import numpy as np
import pandas as pd
import re
import pickle
from typing import List, Dict, Tuple
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Bibliotecas de PLN
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.stem import RSLPStemmer
except ImportError:
    print("âš ï¸ NLTK nÃ£o encontrado. Usando processamento simplificado.")
    nltk = None

# Sentence Transformers para embeddings semÃ¢nticos
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Sentence-transformers nÃ£o encontrado. Usando TF-IDF como fallback.")
    from sklearn.feature_extraction.text import TfidfVectorizer
    EMBEDDINGS_AVAILABLE = False

class GuaraniChatbotImproved:
    """
    Chatbot especializado em responder perguntas sobre "O Guarani" de JosÃ© de Alencar
    VersÃ£o melhorada com embeddings semÃ¢nticos
    """
    
    def __init__(self):
        print("ğŸš€ Inicializando Chatbot O Guarani (VersÃ£o Melhorada)...")
        
        # ConfiguraÃ§Ãµes otimizadas
        self.chunk_size = 150      # Reduzido para chunks mais focados
        self.overlap = 0.3         # SobreposiÃ§Ã£o reduzida mas suficiente
        self.similarity_threshold = 0.15  # Limiar mais restritivo
        self.top_chunks = 3        # Top chunks para resposta
        self.sentence_level = True # Nova feature: busca no nÃ­vel de sentenÃ§a
        
        # Inicializar estruturas de dados primeiro
        self.conversation_history = []
        self.processing_log = []
        self.performance_metrics = []
        
        # Dados do sistema
        self.text_chunks = []
        self.chunk_vectors = None
        self.original_text = ""
        self.sentences_per_chunk = []  # Novo: mapeamento de sentenÃ§as por chunk
        
        # InicializaÃ§Ã£o de componentes PLN (apÃ³s inicializar processing_log)
        self._init_nlp_components()
        
        self._log("Sistema inicializado com sucesso")
    
    def _init_nlp_components(self):
        """Inicializa componentes de PLN com fallbacks"""
        # Stop words
        try:
            if nltk:
                nltk.download('punkt', quiet=True)
                nltk.download('stopwords', quiet=True)
                nltk.download('rslp', quiet=True)
                self.stop_words = set(stopwords.words('portuguese'))
                self.stemmer = RSLPStemmer()
                self._log("NLTK inicializado com sucesso")
            else:
                raise ImportError("NLTK nÃ£o disponÃ­vel")
        except Exception as e:
            # Fallback: stop words bÃ¡sicas em portuguÃªs
            self.stop_words = {
                'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'com', 'para',
                'por', 'que', 'se', 'na', 'no', 'ao', 'aos', 'as', 'os', 'mais',
                'mas', 'ou', 'ter', 'ser', 'estar', 'seu', 'sua', 'seus', 'suas'
            }
            self.stemmer = None
            self._log(f"Usando stop words bÃ¡sicas (NLTK indisponÃ­vel: {e})")
        
        # Modelo de embeddings ou TF-IDF
        if EMBEDDINGS_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                self.use_embeddings = True
                self._log("Usando embeddings semÃ¢nticos (SentenceTransformers)")
            except Exception as e:
                self._log(f"Erro ao carregar SentenceTransformers: {e}")
                self._init_tfidf_fallback()
        else:
            self._log("SentenceTransformers nÃ£o disponÃ­vel, usando TF-IDF")
            self._init_tfidf_fallback()
    
    def _init_tfidf_fallback(self):
        """Inicializa TF-IDF como fallback"""
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            stop_words=list(self.stop_words),  # Melhoramento: reintroduzir stop words
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.95,
            lowercase=True
        )
        self.use_embeddings = False
        self._log("Usando TF-IDF como mÃ©todo de vetorizaÃ§Ã£o")
    
    def _log(self, message: str):
        """Registra eventos no histÃ³rico de processamento"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        
        # VerificaÃ§Ã£o de seguranÃ§a para evitar erros de inicializaÃ§Ã£o
        if hasattr(self, 'processing_log'):
            self.processing_log.append(log_entry)
        else:
            # Fallback se processing_log ainda nÃ£o foi inicializado
            print(f"âš ï¸  Log antes da inicializaÃ§Ã£o: {log_entry}")
        
        print(f"ğŸ“ {log_entry}")
    
    def fase1_preparar_ambiente(self):
        """Fase 1: PreparaÃ§Ã£o do ambiente e obtenÃ§Ã£o dos dados"""
        self._log("=== FASE 1: PREPARAÃ‡ÃƒO DO AMBIENTE ===")
        
        # Carregamento do texto
        texto_carregado = self._carregar_texto_guarani()
        
        if texto_carregado:
            self.original_text = texto_carregado
            self._log(f"Texto carregado: {len(texto_carregado)} caracteres")
            
            # EstatÃ­sticas detalhadas
            stats = self._analyze_text_stats(texto_carregado)
            for key, value in stats.items():
                self._log(f"{key}: {value}")
            
            return True
        else:
            self._log("Erro: NÃ£o foi possÃ­vel carregar o texto")
            return False
    
    def _analyze_text_stats(self, text: str) -> Dict:
        """Analisa estatÃ­sticas detalhadas do texto"""
        palavras = text.split()
        linhas = text.splitlines()
        
        # AnÃ¡lise de sentenÃ§as
        try:
            if nltk:
                sentences = sent_tokenize(text, language='portuguese')
            else:
                sentences = re.split(r'[.!?]+', text)
                sentences = [s.strip() for s in sentences if s.strip()]
        except:
            sentences = text.split('.')
        
        return {
            "Palavras": len(palavras),
            "Linhas": len(linhas),
            "SentenÃ§as": len(sentences),
            "Caracteres": len(text),
            "Palavras Ãºnicas": len(set([p.lower() for p in palavras if p.isalpha()])),
            "MÃ©dia palavras/sentenÃ§a": round(len(palavras) / len(sentences), 2) if sentences else 0
        }
    
    def _carregar_texto_guarani(self):
        """Carrega o texto com melhor tratamento de encoding"""
        # Primeiro, tenta carregar de arquivo
        for filename in ['guarani.txt', 'o_guarani.txt', 'alencar_guarani.txt']:
            result = self._try_load_file(filename)
            if result:
                return result
        
        # Se nÃ£o encontrou arquivo, usa texto de demonstraÃ§Ã£o expandido
        self._log("Arquivo nÃ£o encontrado. Usando texto de demonstraÃ§Ã£o expandido.")
        return self._texto_demo_expandido()
    
    def _try_load_file(self, filename: str):
        """Tenta carregar arquivo com diferentes encodings"""
        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-8-sig']
        
        for encoding in encodings:
            try:
                with open(filename, 'r', encoding=encoding) as arquivo:
                    texto = arquivo.read()
                    if len(texto) > 1000:  # ValidaÃ§Ã£o de tamanho mÃ­nimo
                        self._log(f"Arquivo '{filename}' carregado com encoding: {encoding}")
                        return texto
                    else:
                        self._log(f"Arquivo '{filename}' muito pequeno ({len(texto)} chars)")
            except FileNotFoundError:
                continue
            except UnicodeDecodeError:
                continue
            except Exception as e:
                self._log(f"Erro ao ler '{filename}': {e}")
                continue
        
        return None
    
    def _texto_demo_expandido(self):
        """Texto de demonstraÃ§Ã£o mais rico e detalhado"""
        return """
        O Guarani Ã© um romance indianista de JosÃ© de Alencar, publicado em 1857. A narrativa se desenvolve no sÃ©culo XVII, 
        durante o perÃ­odo colonial brasileiro, nas montanhas fluminenses prÃ³ximas ao rio Paquequer.
        
        Peri Ã© o protagonista da obra, um Ã­ndio goitacÃ¡ de forÃ§a hercÃºlea e lealdade inabalÃ¡vel. Ele Ã© descrito como um 
        guerreiro corajoso, de estatura imponente e carÃ¡ter nobre. Peri demonstra uma devoÃ§Ã£o absoluta a CecÃ­lia (Ceci), 
        filha do fidalgo portuguÃªs Dom AntÃ´nio de Mariz. Esta devoÃ§Ã£o representa o amor impossÃ­vel entre duas raÃ§as distintas.
        
        CecÃ­lia, chamada carinhosamente de Ceci, Ã© uma jovem portuguesa de beleza singular e carÃ¡ter doce. Ela Ã© filha 
        de Dom AntÃ´nio de Mariz e representa a pureza e a inocÃªncia feminina idealizadas pelo Romantismo. Ceci desenvolve 
        sentimentos fraternais por Peri, vendo nele um protetor dedicado.
        
        Dom AntÃ´nio de Mariz Ã© um nobre portuguÃªs, fidalgo da Casa Real, que se estabeleceu no Brasil apÃ³s cometer um crime 
        de honra em Portugal. Ele construiu um castelo fortificado nas margens do rio Paquequer, onde vive com sua famÃ­lia. 
        Dom AntÃ´nio Ã© caracterizado como um homem honrado, mas marcado pelo passado.
        
        Dona Lauriana Ã© a esposa de Dom AntÃ´nio, uma senhora portuguesa de origem nobre. Ela representa os valores 
        aristocrÃ¡ticos europeus e inicialmente demonstra preconceito em relaÃ§Ã£o aos indÃ­genas.
        
        Ãlvaro Ã© um jovem portuguÃªs, primo de CecÃ­lia, que tambÃ©m habita o castelo. Ele encarna o ideal do cavaleiro 
        medieval, sendo corajoso, nobre e apaixonado por Ceci. Ãlvaro representa a civilizaÃ§Ã£o europeia em contraste 
        com a natureza selvagem de Peri.
        
        Isabel Ã© irmÃ£ de CecÃ­lia, uma jovem impetuosa e apaixonada. Ela se enamora de Ãlvaro, criando um triÃ¢ngulo 
        amoroso que adiciona complexidade Ã s relaÃ§Ãµes familiares. Isabel possui um temperamento mais forte que sua irmÃ£.
        
        Loredano Ã© um dos antagonistas da histÃ³ria, um aventureiro italiano que se infiltra no castelo com intenÃ§Ãµes 
        malÃ©volas. Ele planeja assassinar Dom AntÃ´nio e se apossar de suas riquezas, representando a traiÃ§Ã£o e a vilania.
        
        Os aimorÃ©s sÃ£o a tribo indÃ­gena antagonista, inimigos mortais de Peri e de sua tribo goitacÃ¡. Eles representam 
        o perigo constante que ameaÃ§a a seguranÃ§a dos habitantes do castelo. Os aimorÃ©s sÃ£o descritos como selvagens 
        e canibais, contrastando com a nobreza de Peri.
        
        A natureza brasileira desempenha papel fundamental na narrativa, sendo descrita com exuberÃ¢ncia e riqueza de 
        detalhes. Alencar retrata as florestas, rios e montanhas como cenÃ¡rio Ã©pico que reflete o carÃ¡ter dos personagens. 
        A paisagem tropical serve como pano de fundo para os conflitos entre civilizaÃ§Ã£o e barbÃ¡rie.
        
        O romance explora temas centrais como o amor impossÃ­vel entre raÃ§as diferentes, representado pela relaÃ§Ã£o entre 
        Peri e Ceci. A lealdade e o sacrifÃ­cio sÃ£o exemplificados pela devoÃ§Ã£o absoluta do Ã­ndio Ã  famÃ­lia Mariz. 
        O choque entre civilizaÃ§Ãµes aparece no contraste entre os valores europeus e indÃ­genas.
        
        A linguagem de Alencar combina o portuguÃªs erudito com tentativas de recriar a fala indÃ­gena, criando um estilo 
        Ãºnico que busca expressar a realidade brasileira. O autor emprega descriÃ§Ãµes romÃ¢nticas e idealizadas tanto 
        dos personagens quanto da natureza.
        
        O desfecho trÃ¡gico da obra culmina com a destruiÃ§Ã£o do castelo e a fuga de Peri e Ceci, simbolizando o nascimento 
        de uma nova raÃ§a brasileira atravÃ©s da uniÃ£o simbÃ³lica entre o Ã­ndio e a portuguesa. Esta uniÃ£o representa a 
        formaÃ§Ã£o da identidade nacional brasileira segundo a visÃ£o romÃ¢ntica de Alencar.
        
        O Guarani tornou-se uma das obras mais importantes do Romantismo brasileiro, influenciando a literatura nacional 
        e contribuindo para a construÃ§Ã£o do mito do "bom selvagem" e da identidade cultural brasileira.
        """
    
    def fase2_processar_dados(self):
        """Fase 2: Processamento otimizado dos dados"""
        self._log("=== FASE 2: PROCESSAMENTO AVANÃ‡ADO DOS DADOS ===")
        
        # Limpeza aprimorada
        cleaned_text = self._advanced_text_cleaning(self.original_text)
        self._log("Texto limpo e normalizado")
        
        # CriaÃ§Ã£o de chunks otimizada
        self.text_chunks = self._create_optimized_chunks(cleaned_text)
        self._log(f"Criados {len(self.text_chunks)} chunks otimizados")
        
        # Mapeamento de sentenÃ§as por chunk
        self._map_sentences_to_chunks()
        
        return self.text_chunks
    
    def _advanced_text_cleaning(self, text: str) -> str:
        """Limpeza avanÃ§ada preservando contexto importante"""
        # NormalizaÃ§Ã£o de espaÃ§os e quebras de linha
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Preservar pontuaÃ§Ã£o importante para segmentaÃ§Ã£o
        text = re.sub(r'([.!?])\s*', r'\1 ', text)
        
        # Remover caracteres especiais mas preservar acentos
        text = re.sub(r'[^\w\sÃ¡Ã©Ã­Ã³ÃºÃ¢ÃªÃ®Ã´Ã»Ã£ÃµÃ§ÃÃ‰ÃÃ“ÃšÃ‚ÃŠÃÃ”Ã›ÃƒÃ•Ã‡.!?;:,\-]', ' ', text)
        
        # NormalizaÃ§Ã£o final
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _create_optimized_chunks(self, text: str) -> List[str]:
        """CriaÃ§Ã£o otimizada de chunks com melhor contexto"""
        # SegmentaÃ§Ã£o em sentenÃ§as
        try:
            if nltk:
                sentences = sent_tokenize(text, language='portuguese')
            else:
                sentences = re.split(r'[.!?]+', text)
                sentences = [s.strip() for s in sentences if s.strip()]
        except:
            sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        chunks = []
        current_chunk = []
        current_word_count = 0
        
        for sentence in sentences:
            words = sentence.split()
            sentence_word_count = len(words)
            
            # Verificar se a sentenÃ§a cabe no chunk atual
            if current_word_count + sentence_word_count <= self.chunk_size:
                current_chunk.append(sentence)
                current_word_count += sentence_word_count
            else:
                # Finalizar chunk atual se nÃ£o estiver vazio
                if current_chunk:
                    chunks.append('. '.join(current_chunk) + '.')
                
                # ComeÃ§ar novo chunk com sobreposiÃ§Ã£o
                overlap_sentences = int(len(current_chunk) * self.overlap)
                if overlap_sentences > 0 and len(current_chunk) > overlap_sentences:
                    current_chunk = current_chunk[-overlap_sentences:]
                    current_word_count = sum(len(s.split()) for s in current_chunk)
                else:
                    current_chunk = []
                    current_word_count = 0
                
                # Adicionar nova sentenÃ§a
                current_chunk.append(sentence)
                current_word_count += sentence_word_count
        
        # Adicionar Ãºltimo chunk
        if current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
        
        return chunks
    
    def _map_sentences_to_chunks(self):
        """Mapeia sentenÃ§as dentro de cada chunk para busca refinada"""
        self.sentences_per_chunk = []
        
        for chunk in self.text_chunks:
            try:
                if nltk:
                    sentences = sent_tokenize(chunk, language='portuguese')
                else:
                    sentences = re.split(r'[.!?]+', chunk)
                    sentences = [s.strip() for s in sentences if s.strip()]
            except:
                sentences = [s.strip() for s in chunk.split('.') if s.strip()]
            
            self.sentences_per_chunk.append(sentences)
    
    def fase3_armazenar_indexar(self, chunks: List[str]):
        """Fase 3: IndexaÃ§Ã£o com embeddings semÃ¢nticos ou TF-IDF melhorado"""
        self._log("=== FASE 3: INDEXAÃ‡ÃƒO SEMÃ‚NTICA AVANÃ‡ADA ===")
        
        if self.use_embeddings:
            # Usar embeddings semÃ¢nticos
            self.chunk_vectors = self.embedding_model.encode(
                chunks, 
                show_progress_bar=True,
                convert_to_numpy=True
            )
            self._log(f"Embeddings criados: {self.chunk_vectors.shape}")
        else:
            # Usar TF-IDF melhorado
            processed_chunks = [self._preprocess_for_tfidf(chunk) for chunk in chunks]
            self.chunk_vectors = self.vectorizer.fit_transform(processed_chunks)
            self._log(f"Vetores TF-IDF criados: {self.chunk_vectors.shape}")
        
        # Salvar dados
        self._save_enhanced_data()
        
        return True
    
    def _preprocess_for_tfidf(self, text: str) -> str:
        """PrÃ©-processamento especÃ­fico para TF-IDF"""
        text = text.lower()
        
        # TokenizaÃ§Ã£o
        words = re.findall(r'\b\w+\b', text)
        
        # RemoÃ§Ã£o de stop words e palavras muito curtas
        filtered_words = [
            word for word in words 
            if word not in self.stop_words and len(word) > 2
        ]
        
        # Stemming se disponÃ­vel
        if self.stemmer:
            try:
                filtered_words = [self.stemmer.stem(word) for word in filtered_words]
            except:
                pass  # Continuar sem stemming se houver erro
        
        return ' '.join(filtered_words)
    
    def fase4_mecanismo_resposta(self, pergunta: str) -> str:
        """Fase 4: Mecanismo avanÃ§ado de recuperaÃ§Ã£o e geraÃ§Ã£o"""
        start_time = datetime.now()
        self._log(f"=== CONSULTA: {pergunta} ===")
        
        if self.use_embeddings:
            response = self._resposta_com_embeddings(pergunta)
        else:
            response = self._resposta_com_tfidf(pergunta)
        
        # MÃ©tricas de performance
        processing_time = (datetime.now() - start_time).total_seconds()
        self.performance_metrics.append({
            'pergunta': pergunta,
            'tempo_processamento': processing_time,
            'metodo': 'embeddings' if self.use_embeddings else 'tfidf'
        })
        
        return response
    
    def _resposta_com_embeddings(self, pergunta: str) -> str:
        """GeraÃ§Ã£o de resposta usando embeddings semÃ¢nticos"""
        # Codificar pergunta
        question_vector = self.embedding_model.encode([pergunta])
        
        # Calcular similaridades
        similarities = cosine_similarity(question_vector, self.chunk_vectors).flatten()
        
        return self._process_similarities(pergunta, similarities)
    
    def _resposta_com_tfidf(self, pergunta: str) -> str:
        """GeraÃ§Ã£o de resposta usando TF-IDF"""
        # PrÃ©-processar pergunta
        processed_question = self._preprocess_for_tfidf(pergunta)
        question_vector = self.vectorizer.transform([processed_question])
        
        # Calcular similaridades
        similarities = cosine_similarity(question_vector, self.chunk_vectors).flatten()
        
        return self._process_similarities(pergunta, similarities)
    
    def _process_similarities(self, pergunta: str, similarities: np.ndarray) -> str:
        """Processa similaridades e gera resposta"""
        max_sim = np.max(similarities) if len(similarities) > 0 else 0
        mean_sim = np.mean(similarities) if len(similarities) > 0 else 0
        
        self._log(f"Similaridade mÃ¡xima: {max_sim:.3f}, mÃ©dia: {mean_sim:.3f}")
        
        # Encontrar chunks relevantes
        relevant_chunks = []
        for i, similarity in enumerate(similarities):
            if similarity >= self.similarity_threshold:
                relevant_chunks.append({
                    'chunk_id': i,
                    'text': self.text_chunks[i],
                    'similarity': similarity,
                    'sentences': self.sentences_per_chunk[i] if i < len(self.sentences_per_chunk) else []
                })
        
        # Ordenar por similaridade
        relevant_chunks.sort(key=lambda x: x['similarity'], reverse=True)
        top_chunks = relevant_chunks[:self.top_chunks]
        
        self._log(f"Encontrados {len(relevant_chunks)} chunks relevantes")
        
        # Gerar resposta
        if not top_chunks:
            response = self._no_answer_response(pergunta, max_sim)
        else:
            if self.sentence_level and top_chunks:
                response = self._generate_sentence_level_response(pergunta, top_chunks)
            else:
                response = self._generate_chunk_level_response(pergunta, top_chunks)
        
        # Registrar no histÃ³rico
        self._update_conversation_history(pergunta, response, top_chunks, max_sim)
        
        return response
    
    def _generate_sentence_level_response(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta no nÃ­vel de sentenÃ§a (mais precisa)"""
        # Encontrar a sentenÃ§a mais relevante dentro dos melhores chunks
        best_sentences = []
        
        for chunk_data in chunks[:2]:  # Usar top 2 chunks
            sentences = chunk_data['sentences']
            if not sentences:
                continue
            
            # Calcular similaridade para cada sentenÃ§a
            if self.use_embeddings:
                sentence_vectors = self.embedding_model.encode(sentences)
                question_vector = self.embedding_model.encode([pergunta])
                sentence_similarities = cosine_similarity(question_vector, sentence_vectors).flatten()
            else:
                # Para TF-IDF, usar uma aproximaÃ§Ã£o simples
                sentence_similarities = [
                    len(set(pergunta.lower().split()) & set(sent.lower().split())) / 
                    len(set(pergunta.lower().split()) | set(sent.lower().split()))
                    for sent in sentences
                ]
            
            # Encontrar melhor sentenÃ§a neste chunk
            if sentence_similarities:
                best_idx = np.argmax(sentence_similarities)
                best_sentences.append({
                    'text': sentences[best_idx],
                    'similarity': sentence_similarities[best_idx],
                    'chunk_similarity': chunk_data['similarity']
                })
        
        # Selecionar a melhor sentenÃ§a geral
        if best_sentences:
            best_sentences.sort(key=lambda x: x['similarity'], reverse=True)
            best_sentence = best_sentences[0]
            
            confidence = self._calculate_confidence(best_sentence['similarity'])
            
            response = f"Com base em 'O Guarani':\n\n{best_sentence['text']}"
            response += f"\n\n{confidence}"
            
            return response
        else:
            # Fallback para resposta por chunk
            return self._generate_chunk_level_response(pergunta, chunks)
    
    def _generate_chunk_level_response(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta no nÃ­vel de chunk"""
        if len(chunks) == 1:
            main_content = chunks[0]['text']
            intro = "Com base no texto de 'O Guarani':\n\n"
        else:
            # Combinar informaÃ§Ãµes dos melhores chunks
            combined_text = " ".join([chunk['text'] for chunk in chunks[:2]])
            main_content = combined_text
            intro = "Combinando informaÃ§Ãµes de 'O Guarani':\n\n"
        
        # Truncar se muito longo
        if len(main_content) > 600:
            main_content = main_content[:600] + "..."
        
        confidence = self._calculate_confidence(chunks[0]['similarity'])
        
        return intro + main_content + "\n\n" + confidence
    
    def _calculate_confidence(self, similarity: float) -> str:
        """Calcula e retorna indicador de confianÃ§a"""
        if similarity > 0.7:
            return "âœ… (ConfianÃ§a muito alta)"
        elif similarity > 0.5:
            return "ğŸŸ¢ (ConfianÃ§a alta)"
        elif similarity > 0.3:
            return "ğŸŸ¡ (ConfianÃ§a moderada)"
        elif similarity > 0.15:
            return "ğŸŸ  (ConfianÃ§a baixa - considere reformular)"
        else:
            return "ğŸ”´ (ConfianÃ§a muito baixa)"
    
    def _no_answer_response(self, pergunta: str, max_similarity: float) -> str:
        """Resposta quando nÃ£o encontra informaÃ§Ãµes relevantes"""
        base_msg = "NÃ£o encontrei informaÃ§Ãµes especÃ­ficas sobre sua pergunta no texto de 'O Guarani'."
        
        if max_similarity > 0.05:
            suggestion = " Tente reformular a pergunta ou ser mais especÃ­fico sobre personagens, eventos ou temas da obra."
        else:
            suggestion = " Sua pergunta pode estar fora do escopo da obra ou usar termos muito diferentes do texto original."
        
        examples = "\n\nExemplos de perguntas: 'Quem Ã© Peri?', 'Fale sobre CecÃ­lia', 'Qual o enredo?', 'Onde se passa a histÃ³ria?'"
        
        return base_msg + suggestion + examples
    
    def _update_conversation_history(self, pergunta: str, resposta: str, chunks: List[Dict], max_sim: float):
        """Atualiza histÃ³rico de conversa com mais detalhes"""
        self.conversation_history.append({
            'timestamp': datetime.now(),
            'pergunta': pergunta,
            'resposta': resposta,
            'chunks_usados': len(chunks),
            'similaridade_max': max_sim,
            'metodo': 'embeddings' if self.use_embeddings else 'tfidf',
            'chunks_ids': [c['chunk_id'] for c in chunks] if chunks else []
        })
    
    def _save_enhanced_data(self):
        """Salvamento de dados aprimorado"""
        # Em ambiente real, salvaria em disco
        self._log("Dados indexados salvos em memÃ³ria")
    
    def fase5_interface_usuario(self):
        """Fase 5: Interface interativa melhorada"""
        self._log("=== FASE 5: INTERFACE INTERATIVA ===")
        
        print("\n" + "="*70)
        print("ğŸ¤– CHATBOT O GUARANI - VERSÃƒO MELHORADA")
        print("Assistente especializado na obra de JosÃ© de Alencar")
        print(f"MÃ©todo: {'Embeddings SemÃ¢nticos' if self.use_embeddings else 'TF-IDF'}")
        print("Comandos: 'sair', 'historico', 'stats', 'ajuda'")
        print("="*70)
        
        while True:
            try:
                pergunta = input("\nğŸ’¬ Sua pergunta: ").strip()
                
                if pergunta.lower() in ['sair', 'exit', 'quit']:
                    print("ğŸ‘‹ AtÃ© logo!")
                    break
                elif pergunta.lower() in ['historico', 'histÃ³rico', 'history']:
                    self.mostrar_historico_resumido()
                    continue
                elif pergunta.lower() in ['stats', 'estatÃ­sticas', 'estatisticas']:
                    self.mostrar_estatisticas()
                    continue
                elif pergunta.lower() in ['ajuda', 'help']:
                    self.mostrar_ajuda()
                    continue
                
                if not pergunta:
                    continue
                
                resposta = self.fase4_mecanismo_resposta(pergunta)
                print(f"\nğŸ¤– {resposta}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Encerrando...")
                break
    
    def mostrar_ajuda(self):
        """Mostra ajuda para o usuÃ¡rio"""
        help_text = """
        ğŸ†˜ AJUDA - CHATBOT O GUARANI
        
        ğŸ“ Tipos de perguntas que funcionam bem:
        â€¢ Sobre personagens: "Quem Ã© Peri?", "Fale sobre CecÃ­lia"
        â€¢ Sobre enredo: "Qual Ã© a histÃ³ria?", "O que acontece no final?"
        â€¢ Sobre relacionamentos: "Qual a relaÃ§Ã£o entre Peri e Ceci?"
        â€¢ Sobre cenÃ¡rio: "Onde se passa a histÃ³ria?"
        â€¢ Sobre temas: "Quais sÃ£o os temas principais?"
        
        ğŸ›ï¸ Comandos especiais:
        â€¢ 'historico' - Ver histÃ³rico de conversas
        â€¢ 'stats' - Ver estatÃ­sticas do sistema
        â€¢ 'ajuda' - Mostrar esta ajuda
        â€¢ 'sair' - Encerrar o chatbot
        
        ğŸ’¡ Dicas para melhores respostas:
        â€¢ Seja especÃ­fico em suas perguntas
        â€¢ Use nomes de personagens conhecidos
        â€¢ Reformule se a resposta nÃ£o for satisfatÃ³ria
        """
        print(help_text)
    
    def mostrar_historico_resumido(self):
        """Mostra histÃ³rico resumido das conversas"""
        if not self.conversation_history:
            print("ğŸ“­ Nenhuma conversa no histÃ³rico ainda.")
            return
        
        print(f"\nğŸ“š HISTÃ“RICO ({len(self.conversation_history)} conversas):")
        print("-" * 50)
        
        for i, conv in enumerate(self.conversation_history[-5:], 1):  # Ãšltimas 5
            timestamp = conv['timestamp'].strftime("%H:%M")
            print(f"{i}. [{timestamp}] {conv['pergunta']}")
            print(f"   Similarity: {conv['similaridade_max']:.3f} | Chunks: {conv['chunks_usados']}")
    
    def mostrar_estatisticas(self):
        """Mostra estatÃ­sticas detalhadas do sistema"""
        print(f"\nğŸ“Š ESTATÃSTICAS DO SISTEMA")
        print("=" * 40)
        print(f"Chunks de texto: {len(self.text_chunks)}")
        print(f"MÃ©todo de vetorizaÃ§Ã£o: {'Embeddings' if self.use_embeddings else 'TF-IDF'}")
        print(f"Threshold de similaridade: {self.similarity_threshold}")
        print(f"Tamanho dos chunks: {self.chunk_size} palavras")
        print(f"SobreposiÃ§Ã£o: {self.overlap * 100}%")
        print(f"Consultas realizadas: {len(self.conversation_history)}")
        
        if self.performance_metrics:
            tempos = [m['tempo_processamento'] for m in self.performance_metrics]
            print(f"Tempo mÃ©dio de resposta: {np.mean(tempos):.3f}s")
        
        if self.conversation_history:
            similaridades = [c['similaridade_max'] for c in self.conversation_history]
            print(f"Similaridade mÃ©dia: {np.mean(similaridades):.3f}")
    
    def executar_sistema_completo(self):
        """Executa todas as fases do sistema com tratamento de erros"""
        try:
            # Fase 1: PreparaÃ§Ã£o
            if not self.fase1_preparar_ambiente():
                raise Exception("Falha na preparaÃ§Ã£o do ambiente")
            
            # Fase 2: Processamento
            chunks = self.fase2_processar_dados()
            if not chunks:
                raise Exception("Falha no processamento dos dados")
            
            # Fase 3: IndexaÃ§Ã£o
            if not self.fase3_armazenar_indexar(chunks):
                raise Exception("Falha na indexaÃ§Ã£o")
            
            self._log("âœ… Sistema inicializado com sucesso!")
            
            # Teste automÃ¡tico
            self.executar_testes_automaticos()
            
            return True
            
        except Exception as e:
            self._log(f"âŒ Erro na execuÃ§Ã£o: {e}")
            return False
    
    def executar_testes_automaticos(self):
        """Executa testes automÃ¡ticos abrangentes"""
        perguntas_teste = [
            "Quem Ã© Peri?",
            "Fale sobre CecÃ­lia",
            "Qual Ã© o enredo do livro?",
            "Quem sÃ£o os personagens principais?",
            "Onde se passa a histÃ³ria?",
            "Qual a relaÃ§Ã£o entre Peri e Ceci?",
            "Quem Ã© Dom AntÃ´nio de Mariz?",
            "O que sÃ£o os aimorÃ©s?",
            "Quando foi publicado O Guarani?",
            "Quais sÃ£o os temas da obra?"
        ]
        
        print("\n" + "="*60)
        print("ğŸ§ª EXECUTANDO TESTES AUTOMÃTICOS")
        print("="*60)
        
        resultados = []
        
        for i, pergunta in enumerate(perguntas_teste, 1):
            print(f"\nğŸ” Teste {i}/10: {pergunta}")
            
            start_time = datetime.now()
            resposta = self.fase4_mecanismo_resposta(pergunta)
            tempo = (datetime.now() - start_time).total_seconds()
            
            # AnÃ¡lise da qualidade da resposta
            ultimo_historico = self.conversation_history[-1]
            qualidade = self._avaliar_qualidade_resposta(ultimo_historico)
            
            resultados.append({
                'pergunta': pergunta,
                'tempo': tempo,
                'similaridade': ultimo_historico['similaridade_max'],
                'chunks': ultimo_historico['chunks_usados'],
                'qualidade': qualidade
            })
            
            print(f"â±ï¸  Tempo: {tempo:.3f}s | Similaridade: {ultimo_historico['similaridade_max']:.3f} | {qualidade}")
            print(f"ğŸ“ Resposta: {resposta[:100]}...")
        
        # RelatÃ³rio final
        self._gerar_relatorio_testes(resultados)
    
    def _avaliar_qualidade_resposta(self, historico: Dict) -> str:
        """Avalia a qualidade da resposta baseada em mÃ©tricas"""
        sim = historico['similaridade_max']
        chunks = historico['chunks_usados']
        
        if sim > 0.4 and chunks > 0:
            return "ğŸŸ¢ Excelente"
        elif sim > 0.25 and chunks > 0:
            return "ğŸŸ¡ Boa"
        elif sim > 0.15 and chunks > 0:
            return "ğŸŸ  Regular"
        else:
            return "ğŸ”´ Ruim"
    
    def _gerar_relatorio_testes(self, resultados: List[Dict]):
        """Gera relatÃ³rio final dos testes"""
        print("\n" + "="*60)
        print("ğŸ“‹ RELATÃ“RIO FINAL DOS TESTES")
        print("="*60)
        
        tempos = [r['tempo'] for r in resultados]
        similaridades = [r['similaridade'] for r in resultados]
        
        print(f"ğŸ“Š MÃ©tricas Gerais:")
        print(f"   â€¢ Tempo mÃ©dio: {np.mean(tempos):.3f}s")
        print(f"   â€¢ Tempo mÃ¡ximo: {np.max(tempos):.3f}s")
        print(f"   â€¢ Similaridade mÃ©dia: {np.mean(similaridades):.3f}")
        print(f"   â€¢ Similaridade mÃ­nima: {np.min(similaridades):.3f}")
        
        qualidades = [r['qualidade'] for r in resultados]
        excelentes = qualidades.count("ğŸŸ¢ Excelente")
        boas = qualidades.count("ğŸŸ¡ Boa")
        regulares = qualidades.count("ğŸŸ  Regular")
        ruins = qualidades.count("ğŸ”´ Ruim")
        
        print(f"\nğŸ¯ Qualidade das Respostas:")
        print(f"   â€¢ Excelentes: {excelentes}/10 ({excelentes*10}%)")
        print(f"   â€¢ Boas: {boas}/10 ({boas*10}%)")
        print(f"   â€¢ Regulares: {regulares}/10 ({regulares*10}%)")
        print(f"   â€¢ Ruins: {ruins}/10 ({ruins*10}%)")
        
        # RecomendaÃ§Ãµes
        if np.mean(similaridades) < 0.2:
            print("\nâš ï¸  RecomendaÃ§Ã£o: Similaridades baixas. Considere:")
            print("    - Verificar se o texto estÃ¡ carregado corretamente")
            print("    - Ajustar threshold de similaridade")
            print("    - Melhorar o prÃ©-processamento")


def main():
    """FunÃ§Ã£o principal melhorada"""
    print("ğŸš€ Iniciando Chatbot O Guarani - VersÃ£o Melhorada")
    print("=" * 60)
    
    chatbot = GuaraniChatbotImproved()
    
    if chatbot.executar_sistema_completo():
        print("\nâœ… Sistema inicializado com sucesso!")
        
        # Menu de opÃ§Ãµes
        while True:
            print("\nğŸ¯ OPÃ‡Ã•ES DISPONÃVEIS:")
            print("1. ğŸ’¬ Iniciar chat interativo")
            print("2. ğŸ“Š Ver estatÃ­sticas do sistema")
            print("3. ğŸ“š Ver histÃ³rico completo")
            print("4. ğŸ§ª Executar novos testes")
            print("5. ğŸšª Sair")
            
            try:
                opcao = input("\nEscolha uma opÃ§Ã£o (1-5): ").strip()
                
                if opcao == '1':
                    chatbot.fase5_interface_usuario()
                elif opcao == '2':
                    chatbot.mostrar_estatisticas()
                elif opcao == '3':
                    chatbot.mostrar_historico_resumido()
                elif opcao == '4':
                    chatbot.executar_testes_automaticos()
                elif opcao == '5':
                    print("ğŸ‘‹ Encerrando sistema...")
                    break
                else:
                    print("âŒ OpÃ§Ã£o invÃ¡lida. Tente novamente.")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Encerrando...")
                break
    else:
        print("âŒ Falha na inicializaÃ§Ã£o do sistema")

if __name__ == "__main__":
    main()