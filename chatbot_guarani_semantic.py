#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chatbot "O Guarani" - Vers√£o com Compreens√£o Sem√¢ntica
Usando sentence-transformers para embeddings sem√¢nticos
"""

import numpy as np
import re
import os
from datetime import datetime
from typing import List, Dict, Optional
import time
import pickle
from pathlib import Path

# Importa√ß√µes para embeddings sem√¢nticos
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    SEMANTIC_AVAILABLE = True
    print("‚úÖ Bibliotecas sem√¢nticas carregadas com sucesso!")
except ImportError as e:
    SEMANTIC_AVAILABLE = False
    print(f"‚ùå Erro ao importar bibliotecas sem√¢nticas: {e}")
    print("üì¶ Instale com: pip install sentence-transformers scikit-learn")

class GuaraniChatbotSemantico:
    """
    Chatbot O Guarani - Vers√£o com Compreens√£o Sem√¢ntica Avan√ßada
    Usa sentence-transformers para entender o significado das perguntas
    """
    
    def __init__(self):
        print("üöÄ Inicializando Chatbot O Guarani (Vers√£o Sem√¢ntica)")
        print("=" * 60)
        
        # Verificar depend√™ncias
        if not SEMANTIC_AVAILABLE:
            raise ImportError("‚ùå Bibliotecas sem√¢nticas n√£o dispon√≠veis. Execute: pip install sentence-transformers scikit-learn")
        
        # Configura√ß√µes otimizadas
        self.chunk_size = 150
        self.overlap = 0.3
        self.similarity_threshold = 0.3  # Ajustado para embeddings (valores mais altos)
        self.top_chunks = 3
        
        # Estruturas de dados
        self.conversation_history = []
        self.processing_log = []
        self.performance_metrics = []
        self.text_chunks = []
        self.chunk_sentences = []
        self.chunk_embeddings = None  # Para armazenar embeddings dos chunks
        
        # Configura√ß√£o do modelo sem√¢ntico
        self.model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        self.sentence_model = None
        self.embeddings_cache_file = "guarani_embeddings_cache.pkl"
        
        # Stop words expandidas (ainda √∫teis para pr√©-processamento)
        self.stop_words = {
            'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'com', 'para',
            'por', 'que', 'se', 'na', 'no', 'ao', 'aos', 'as', 'os', 'mais',
            'mas', 'ou', 'ter', 'ser', 'estar', 'seu', 'sua', 'seus', 'suas',
            'foi', 's√£o', 'dos', 'das', 'pela', 'pelo', 'sobre', 'at√©', 'sem',
            'muito', 'bem', 'j√°', 'ainda', 's√≥', 'pode', 'tem', 'vai', 'vem',
            'ele', 'ela', 'eles', 'elas', 'isso', 'isto', 'aquilo', 'quando',
            'onde', 'como', 'porque', 'ent√£o', 'assim', 'aqui', 'ali', 'l√°',
            'me', 'te', 'nos', 'vos', 'lhe', 'lhes', 'meu', 'teu', 'nosso'
        }
        
        # Carregar texto do arquivo
        self.texto_guarani = self._carregar_texto_arquivo()
        
        if not self.texto_guarani:
            raise Exception("Falha ao carregar o arquivo guarani.txt")
        
        # Inicializar modelo sem√¢ntico
        self._inicializar_modelo_semantico()
        
        self._log("Sistema inicializado com sucesso")
    
    def _inicializar_modelo_semantico(self):
        """Inicializa o modelo de sentence transformers"""
        try:
            self._log("üß† Carregando modelo de embeddings sem√¢nticos...")
            self._log(f"üì¶ Modelo: {self.model_name}")
            
            # Carregar modelo (primeira vez pode demorar para download)
            start_time = time.time()
            self.sentence_model = SentenceTransformer(self.model_name)
            load_time = time.time() - start_time
            
            self._log(f"‚úÖ Modelo carregado em {load_time:.2f}s")
            
            # Teste r√°pido do modelo
            test_embedding = self.sentence_model.encode(["Teste de funcionamento"])
            self._log(f"üìè Dimens√£o dos embeddings: {test_embedding.shape[1]}")
            
        except Exception as e:
            self._log(f"‚ùå Erro ao carregar modelo: {e}")
            raise e
    
    def _carregar_texto_arquivo(self) -> str:
        """Carrega o texto de O Guarani do arquivo guarani.txt"""
        arquivo_path = "guarani.txt"
        
        try:
            self._log("Tentando carregar guarani.txt...")
            
            # Verificar se o arquivo existe
            if not os.path.exists(arquivo_path):
                self._log(f"‚ùå Arquivo {arquivo_path} n√£o encontrado!")
                self._log("Criando arquivo de exemplo...")
                self._criar_arquivo_exemplo(arquivo_path)
                return self._carregar_arquivo_exemplo()
            
            # Tentar diferentes encodings
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
            
            for encoding in encodings:
                try:
                    with open(arquivo_path, 'r', encoding=encoding) as file:
                        texto = file.read().strip()
                        
                    if texto and len(texto) > 100:  # Verificar se o texto n√£o est√° vazio
                        self._log(f"‚úÖ Arquivo carregado com encoding {encoding}")
                        self._log(f"üìÑ Tamanho do texto: {len(texto)} caracteres")
                        self._log(f"üìù Primeiras 100 chars: {texto[:100]}...")
                        return texto
                    else:
                        self._log(f"‚ö†Ô∏è Arquivo vazio ou muito pequeno com encoding {encoding}")
                        
                except UnicodeDecodeError:
                    self._log(f"‚ùå Falha com encoding {encoding}")
                    continue
                except Exception as e:
                    self._log(f"‚ùå Erro ao ler com {encoding}: {e}")
                    continue
            
            # Se chegou aqui, todas as tentativas falharam
            self._log("‚ùå Falha ao carregar com todos os encodings testados")
            self._log("üìù Usando texto de exemplo...")
            return self._carregar_arquivo_exemplo()
            
        except Exception as e:
            self._log(f"‚ùå Erro cr√≠tico ao carregar arquivo: {e}")
            self._log("üìù Usando texto de exemplo...")
            return self._carregar_arquivo_exemplo()
    
    def _criar_arquivo_exemplo(self, arquivo_path: str):
        """Cria um arquivo de exemplo com texto b√°sico de O Guarani"""
        texto_exemplo = """O Guarani √© um romance indianista de Jos√© de Alencar, publicado em 1857. A narrativa se desenvolve no s√©culo XVII, durante o per√≠odo colonial brasileiro, nas montanhas fluminenses pr√≥ximas ao rio Paquequer.

Peri √© o protagonista da obra, um √≠ndio goitac√° de for√ßa herc√∫lea e lealdade inabal√°vel. Ele √© descrito como um guerreiro corajoso, de estatura imponente e car√°ter nobre. Peri demonstra uma devo√ß√£o absoluta a Cec√≠lia (Ceci), filha do fidalgo portugu√™s Dom Ant√¥nio de Mariz.

Cec√≠lia, chamada carinhosamente de Ceci, √© uma jovem portuguesa de beleza singular e car√°ter doce. Ela √© filha de Dom Ant√¥nio de Mariz e representa a pureza e a inoc√™ncia feminina idealizadas pelo Romantismo.

Dom Ant√¥nio de Mariz √© um nobre portugu√™s, fidalgo da Casa Real, que se estabeleceu no Brasil ap√≥s cometer um crime de honra em Portugal. Ele construiu um castelo fortificado nas margens do rio Paquequer.

√Ålvaro √© um jovem portugu√™s, primo de Cec√≠lia, que tamb√©m habita o castelo. Ele encarna o ideal do cavaleiro medieval, sendo corajoso, nobre e apaixonado por Ceci.

Isabel √© irm√£ de Cec√≠lia, uma jovem impetuosa e apaixonada. Ela se enamora de √Ålvaro, criando um tri√¢ngulo amoroso que adiciona complexidade √†s rela√ß√µes familiares.

Os aimor√©s s√£o a tribo ind√≠gena antagonista, inimigos mortais de Peri e de sua tribo goitac√°. Eles representam o perigo constante que amea√ßa a seguran√ßa dos habitantes do castelo.

Loredano √© um dos antagonistas da hist√≥ria, um aventureiro italiano que se infiltra no castelo com inten√ß√µes mal√©volas. Ele planeja assassinar Dom Ant√¥nio e se apossar de suas riquezas.

A natureza brasileira desempenha papel fundamental na narrativa, sendo descrita com exuber√¢ncia e riqueza de detalhes. Alencar retrata as florestas, rios e montanhas como cen√°rio √©pico.

O romance explora temas centrais como o amor imposs√≠vel entre ra√ßas diferentes, representado pela rela√ß√£o entre Peri e Ceci. A lealdade e o sacrif√≠cio s√£o exemplificados pela devo√ß√£o absoluta do √≠ndio √† fam√≠lia Mariz."""
        
        try:
            with open(arquivo_path, 'w', encoding='utf-8') as file:
                file.write(texto_exemplo)
            self._log(f"‚úÖ Arquivo de exemplo criado: {arquivo_path}")
        except Exception as e:
            self._log(f"‚ùå Erro ao criar arquivo de exemplo: {e}")
    
    def _carregar_arquivo_exemplo(self) -> str:
        """Retorna texto de exemplo quando o arquivo n√£o pode ser carregado"""
        return """O Guarani √© um romance indianista de Jos√© de Alencar, publicado em 1857. A narrativa se desenvolve no s√©culo XVII, durante o per√≠odo colonial brasileiro, nas montanhas fluminenses pr√≥ximas ao rio Paquequer.

Peri √© o protagonista da obra, um √≠ndio goitac√° de for√ßa herc√∫lea e lealdade inabal√°vel. Ele √© descrito como um guerreiro corajoso, de estatura imponente e car√°ter nobre. Peri demonstra uma devo√ß√£o absoluta a Cec√≠lia (Ceci), filha do fidalgo portugu√™s Dom Ant√¥nio de Mariz. Esta devo√ß√£o representa o amor imposs√≠vel entre duas ra√ßas distintas.

Cec√≠lia, chamada carinhosamente de Ceci, √© uma jovem portuguesa de beleza singular e car√°ter doce. Ela √© filha de Dom Ant√¥nio de Mariz e representa a pureza e a inoc√™ncia feminina idealizadas pelo Romantismo. Ceci desenvolve sentimentos fraternais por Peri, vendo nele um protetor dedicado.

Dom Ant√¥nio de Mariz √© um nobre portugu√™s, fidalgo da Casa Real, que se estabeleceu no Brasil ap√≥s cometer um crime de honra em Portugal. Ele construiu um castelo fortificado nas margens do rio Paquequer, onde vive com sua fam√≠lia. Dom Ant√¥nio √© caracterizado como um homem honrado, mas marcado pelo passado.

√Ålvaro √© um jovem portugu√™s, primo de Cec√≠lia, que tamb√©m habita o castelo. Ele encarna o ideal do cavaleiro medieval, sendo corajoso, nobre e apaixonado por Ceci. √Ålvaro representa a civiliza√ß√£o europeia em contraste com a natureza selvagem de Peri.

Isabel √© irm√£ de Cec√≠lia, uma jovem impetuosa e apaixonada. Ela se enamora de √Ålvaro, criando um tri√¢ngulo amoroso que adiciona complexidade √†s rela√ß√µes familiares. Isabel possui um temperamento mais forte que sua irm√£.

Os aimor√©s s√£o a tribo ind√≠gena antagonista, inimigos mortais de Peri e de sua tribo goitac√°. Eles representam o perigo constante que amea√ßa a seguran√ßa dos habitantes do castelo. Os aimor√©s s√£o descritos como selvagens e canibais.

Loredano √© um dos antagonistas da hist√≥ria, um aventureiro italiano que se infiltra no castelo com inten√ß√µes mal√©volas. Ele planeja assassinar Dom Ant√¥nio e se apossar de suas riquezas, representando a trai√ß√£o e a vilania."""
    
    def _log(self, message: str):
        """Log seguro"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.processing_log.append(log_entry)
        print(f"üìù {log_entry}")
    
    def fase1_analisar_texto(self):
        """Fase 1: An√°lise do texto"""
        self._log("=== FASE 1: AN√ÅLISE DO TEXTO ===")
        
        if not self.texto_guarani:
            self._log("‚ùå Texto n√£o carregado!")
            return False
        
        chars = len(self.texto_guarani)
        words = self.texto_guarani.split()
        sentences = self._segmentar_sentencas(self.texto_guarani)
        
        word_tokens = re.findall(r'\b\w+\b', self.texto_guarani.lower())
        unique_words = set(word_tokens)
        content_words = unique_words - self.stop_words
        
        self._log(f"Caracteres: {chars}")
        self._log(f"Palavras: {len(words)}")
        self._log(f"Senten√ßas: {len(sentences)}")
        self._log(f"Vocabul√°rio √∫nico: {len(unique_words)}")
        self._log(f"Palavras de conte√∫do: {len(content_words)}")
        
        return True
    
    def _segmentar_sentencas(self, texto: str) -> List[str]:
        """Segmenta√ß√£o robusta de senten√ßas"""
        # Limpeza inicial
        texto = re.sub(r'\n+', ' ', texto)
        texto = re.sub(r'\s+', ' ', texto).strip()
        
        # Segmenta√ß√£o por pontua√ß√£o
        sentences = re.split(r'[.!?]+', texto)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.split()) > 2]
        
        return sentences
    
    def fase2_criar_chunks_e_embeddings(self):
        """Fase 2: Cria√ß√£o de chunks e gera√ß√£o de embeddings sem√¢nticos"""
        self._log("=== FASE 2: CRIA√á√ÉO DE CHUNKS E EMBEDDINGS ===")
        
        if not self.texto_guarani:
            self._log("‚ùå Texto n√£o carregado!")
            return False
        
        if not self.sentence_model:
            self._log("‚ùå Modelo sem√¢ntico n√£o carregado!")
            return False
        
        # Criar chunks de texto
        sentences = self._segmentar_sentencas(self.texto_guarani)
        
        chunks = []
        chunk_sentences_map = []
        current_chunk_sentences = []
        current_word_count = 0
        
        for sentence in sentences:
            words = sentence.split()
            sentence_word_count = len(words)
            
            # Verificar se cabe no chunk atual
            if current_word_count + sentence_word_count <= self.chunk_size:
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
            else:
                # Finalizar chunk atual
                if current_chunk_sentences:
                    chunk_text = '. '.join(current_chunk_sentences) + '.'
                    chunks.append(chunk_text)
                    chunk_sentences_map.append(current_chunk_sentences.copy())
                
                # Aplicar sobreposi√ß√£o
                overlap_size = int(len(current_chunk_sentences) * self.overlap)
                if overlap_size > 0 and len(current_chunk_sentences) > overlap_size:
                    current_chunk_sentences = current_chunk_sentences[-overlap_size:]
                    current_word_count = sum(len(s.split()) for s in current_chunk_sentences)
                else:
                    current_chunk_sentences = []
                    current_word_count = 0
                
                # Adicionar nova senten√ßa
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
        
        # Finalizar √∫ltimo chunk
        if current_chunk_sentences:
            chunk_text = '. '.join(current_chunk_sentences) + '.'
            chunks.append(chunk_text)
            chunk_sentences_map.append(current_chunk_sentences.copy())
        
        self.text_chunks = chunks
        self.chunk_sentences = chunk_sentences_map
        
        # Verificar se existe cache de embeddings
        cache_exists = self._verificar_cache_embeddings()
        
        if cache_exists:
            self._log("üìÅ Cache de embeddings encontrado, carregando...")
            if self._carregar_cache_embeddings():
                self._log("‚úÖ Embeddings carregados do cache!")
            else:
                self._log("‚ùå Falha ao carregar cache, gerando novos embeddings...")
                self._gerar_embeddings()
        else:
            self._log("üß† Gerando embeddings sem√¢nticos dos chunks...")
            self._gerar_embeddings()
        
        # Estat√≠sticas
        if chunks and self.chunk_embeddings is not None:
            chunk_sizes = [len(chunk.split()) for chunk in chunks]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            self._log(f"Chunks criados: {len(chunks)}")
            self._log(f"Tamanho m√©dio: {avg_size:.1f} palavras")
            self._log(f"Embeddings shape: {self.chunk_embeddings.shape}")
        
        return True
    
    def _verificar_cache_embeddings(self) -> bool:
        """Verifica se existe cache de embeddings v√°lido"""
        try:
            if not os.path.exists(self.embeddings_cache_file):
                return False
            
            # Verificar se o cache n√£o est√° muito antigo
            cache_time = os.path.getmtime(self.embeddings_cache_file)
            text_time = os.path.getmtime("guarani.txt") if os.path.exists("guarani.txt") else 0
            
            # Se o texto foi modificado depois do cache, invalidar
            if text_time > cache_time:
                self._log("‚ö†Ô∏è Texto modificado, cache inv√°lido")
                return False
            
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erro ao verificar cache: {e}")
            return False
    
    def _carregar_cache_embeddings(self) -> bool:
        """Carrega embeddings do cache"""
        try:
            with open(self.embeddings_cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # Verificar integridade do cache
            if ('embeddings' in cache_data and 
                'chunks' in cache_data and 
                len(cache_data['chunks']) == len(self.text_chunks)):
                
                # Verificar se os chunks s√£o os mesmos
                if cache_data['chunks'] == self.text_chunks:
                    self.chunk_embeddings = cache_data['embeddings']
                    return True
                else:
                    self._log("‚ö†Ô∏è Chunks diferentes do cache, regenerando...")
                    return False
            else:
                self._log("‚ö†Ô∏è Cache inv√°lido ou corrompido")
                return False
                
        except Exception as e:
            self._log(f"‚ùå Erro ao carregar cache: {e}")
            return False
    
    def _salvar_cache_embeddings(self):
        """Salva embeddings no cache"""
        try:
            cache_data = {
                'embeddings': self.chunk_embeddings,
                'chunks': self.text_chunks,
                'model_name': self.model_name,
                'timestamp': datetime.now()
            }
            
            with open(self.embeddings_cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            self._log(f"üíæ Cache salvo: {self.embeddings_cache_file}")
            
        except Exception as e:
            self._log(f"‚ùå Erro ao salvar cache: {e}")
    
    def _gerar_embeddings(self):
        """Gera embeddings sem√¢nticos para todos os chunks"""
        try:
            start_time = time.time()
            
            # Gerar embeddings para todos os chunks de uma vez (mais eficiente)
            self._log(f"üß† Processando {len(self.text_chunks)} chunks...")
            
            self.chunk_embeddings = self.sentence_model.encode(
                self.text_chunks,
                convert_to_numpy=True,
                show_progress_bar=True,
                batch_size=32  # Processamento em lotes para efici√™ncia
            )
            
            generation_time = time.time() - start_time
            self._log(f"‚úÖ Embeddings gerados em {generation_time:.2f}s")
            self._log(f"üìä Shape dos embeddings: {self.chunk_embeddings.shape}")
            
            # Salvar no cache
            self._salvar_cache_embeddings()
            
        except Exception as e:
            self._log(f"‚ùå Erro ao gerar embeddings: {e}")
            raise e
    
    def calcular_similaridade_semantica(self, pergunta: str, chunks: Optional[List[str]] = None) -> List[float]:
        """Calcula similaridade sem√¢ntica usando embeddings"""
        try:
            if chunks is None:
                chunks = self.text_chunks
            
            if self.chunk_embeddings is None:
                self._log("‚ùå Embeddings n√£o carregados!")
                return [0.0] * len(chunks)
            
            # Gerar embedding da pergunta
            question_embedding = self.sentence_model.encode([pergunta], convert_to_numpy=True)
            
            # Calcular similaridade coseno
            similarities = cosine_similarity(question_embedding, self.chunk_embeddings)[0]
            
            # Converter para lista de floats
            similarities = [float(sim) for sim in similarities]
            
            return similarities
            
        except Exception as e:
            self._log(f"‚ùå Erro no c√°lculo de similaridade sem√¢ntica: {e}")
            return [0.0] * len(chunks if chunks else self.text_chunks)
    
    def fase3_responder_pergunta(self, pergunta: str) -> str:
        """Fase 3: Resposta √† pergunta usando similaridade sem√¢ntica"""
        start_time = time.time()
        self._log(f"=== CONSULTA SEM√ÇNTICA: {pergunta} ===")
        
        if not self.text_chunks:
            return "‚ùå Sistema n√£o processado. Execute as fases anteriores."
        
        if self.chunk_embeddings is None:
            return "‚ùå Embeddings n√£o carregados. Execute a Fase 2."
        
        try:
            # Calcular similaridades sem√¢nticas
            similarities = self.calcular_similaridade_semantica(pergunta)
            
            # Verificar se temos similaridades v√°lidas
            if not similarities:
                return "‚ùå Erro no c√°lculo de similaridades sem√¢nticas."
            
            # Criar resultados de forma segura
            chunk_results = []
            for i, sim in enumerate(similarities):
                chunk_results.append({
                    'chunk_id': i,
                    'chunk': self.text_chunks[i],
                    'similarity': float(sim),
                    'sentences': self.chunk_sentences[i] if i < len(self.chunk_sentences) else []
                })
            
            # Ordenar por similaridade
            chunk_results.sort(key=lambda x: x['similarity'], reverse=True)
            
            # Estat√≠sticas seguras
            max_sim = chunk_results[0]['similarity'] if chunk_results else 0.0
            mean_sim = sum(similarities) / len(similarities) if similarities else 0.0
            
            self._log(f"Similaridade sem√¢ntica m√°xima: {max_sim:.3f}")
            self._log(f"Similaridade sem√¢ntica m√©dia: {mean_sim:.3f}")
            
            # Filtrar chunks relevantes (threshold mais alto para embeddings)
            relevant_chunks = []
            for chunk in chunk_results:
                if chunk['similarity'] >= self.similarity_threshold:
                    relevant_chunks.append(chunk)
            
            self._log(f"Chunks semanticamente relevantes: {len(relevant_chunks)}")
            
            # Gerar resposta
            if not relevant_chunks:
                response = self._resposta_nao_encontrada_semantica(pergunta, max_sim)
            else:
                response = self._gerar_resposta_semantica(pergunta, relevant_chunks[:self.top_chunks])
            
            # M√©tricas
            processing_time = time.time() - start_time
            self.performance_metrics.append({
                'pergunta': pergunta,
                'tempo': processing_time,
                'max_similarity': max_sim,
                'chunks_relevantes': len(relevant_chunks),
                'metodo': 'semantico'
            })
            
            # Hist√≥rico
            self.conversation_history.append({
                'pergunta': pergunta,
                'resposta': response,
                'similaridade_max': max_sim,
                'chunks_usados': len(relevant_chunks),
                'tempo_resposta': processing_time,
                'metodo': 'semantico',
                'timestamp': datetime.now()
            })
            
            self._log(f"Resposta sem√¢ntica gerada em {processing_time:.3f}s")
            return response
            
        except Exception as e:
            error_msg = f"‚ùå Erro inesperado na an√°lise sem√¢ntica: {e}"
            self._log(error_msg)
            return error_msg
    
    def _resposta_nao_encontrada_semantica(self, pergunta: str, max_sim: float) -> str:
        """Resposta quando n√£o encontra informa√ß√µes semanticamente relevantes"""
        base_msg = "N√£o encontrei informa√ß√µes semanticamente relevantes sobre sua pergunta no texto de 'O Guarani'."
        
        if max_sim > 0.2:
            suggestion = "\n\nüí° Tente reformular usando termos mais espec√≠ficos ou sin√¥nimos."
        elif max_sim > 0.1:
            suggestion = "\n\nüí° Use nomes de personagens ou conceitos centrais da obra."
        else:
            suggestion = "\n\nüí° Sua pergunta pode estar completamente fora do escopo da obra."
        
        examples = """
\nüìù Exemplos de perguntas que funcionam bem com an√°lise sem√¢ntica:
‚Ä¢ "Como √© a personalidade de Peri?"
‚Ä¢ "Qual o sentimento entre Peri e Cec√≠lia?"
‚Ä¢ "Descreva o conflito principal da obra"
‚Ä¢ "Quais s√£o os antagonistas da hist√≥ria?"
‚Ä¢ "Como √© retratada a natureza brasileira?"
‚Ä¢ "Qual o papel da fam√≠lia Mariz?"
‚Ä¢ "Quais s√£o os temas do romance?"
‚Ä¢ "Como √© caracterizado o amor imposs√≠vel?"
"""
        
        confidence = f"\n\nüî¥ Similaridade sem√¢ntica baixa (m√°xima: {max_sim:.3f})"
        
        return base_msg + suggestion + examples + confidence
    
    def _gerar_resposta_semantica(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta usando an√°lise sem√¢ntica"""
        if not chunks:
            return self._resposta_nao_encontrada_semantica(pergunta, 0.0)
        
        try:
            best_chunk = chunks[0]
            
            # Para embeddings sem√¢nticos, vamos usar o chunk completo mais relevante
            # pois a similaridade j√° captura o significado geral
            
            if len(chunks) == 1:
                main_content = chunks[0]['chunk']
                intro = "Com base na an√°lise sem√¢ntica de 'O Guarani':\n\n"
            else:
                # Combinar os chunks mais relevantes semanticamente
                combined_chunks = []
                total_length = 0
                similarity_scores = []
                
                for chunk in chunks[:3]:  # M√°ximo 3 chunks semanticamente relevantes
                    chunk_text = chunk['chunk']
                    if total_length + len(chunk_text) < 800:  # Limite um pouco maior para sem√¢ntica
                        combined_chunks.append(chunk_text)
                        similarity_scores.append(chunk['similarity'])
                        total_length += len(chunk_text)
                    else:
                        break
                
                main_content = "\n\n".join(combined_chunks)
                avg_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0
                intro = f"Combinando informa√ß√µes semanticamente relevantes de 'O Guarani' (similaridade m√©dia: {avg_similarity:.3f}):\n\n"
            
            # Truncar se muito longo, mas manter mais contexto para an√°lise sem√¢ntica
            if len(main_content) > 700:
                main_content = main_content[:700] + "..."
            
            confidence = self._calcular_confianca_semantica(best_chunk['similarity'])
            return intro + main_content + "\n\n" + confidence
            
        except Exception as e:
            self._log(f"Erro na gera√ß√£o de resposta sem√¢ntica: {e}")
            return f"‚ùå Erro ao gerar resposta sem√¢ntica: {e}"
    
    def _calcular_confianca_semantica(self, similarity: float) -> str:
        """Calcula indicador de confian√ßa para similaridade sem√¢ntica"""
        try:
            sim = float(similarity)
            if sim > 0.7:
                return "üü¢ Confian√ßa sem√¢ntica muito alta"
            elif sim > 0.5:
                return "üü¢ Confian√ßa sem√¢ntica alta"
            elif sim > 0.4:
                return "üü° Confian√ßa sem√¢ntica moderada"
            elif sim > 0.3:
                return "üü† Confian√ßa sem√¢ntica baixa"
            else:
                return "üî¥ Confian√ßa sem√¢ntica muito baixa"
        except:
            return "‚ö†Ô∏è Confian√ßa sem√¢ntica indeterminada"
    
    def executar_sistema_completo(self):
        """Executa todas as fases do sistema sem√¢ntico"""
        try:
            self._log("üöÄ EXECUTANDO SISTEMA SEM√ÇNTICO COMPLETO")
            
            if not self.fase1_analisar_texto():
                raise Exception("Erro na Fase 1")
            
            if not self.fase2_criar_chunks_e_embeddings():
                raise Exception("Erro na Fase 2 (Embeddings)")
            
            self._log("‚úÖ Sistema sem√¢ntico pronto para consultas!")
            return True
            
        except Exception as e:
            self._log(f"‚ùå Erro na execu√ß√£o: {e}")
            return False
    
    def comparar_metodos(self, pergunta: str) -> Dict:
        """Compara m√©todo sem√¢ntico com m√©todo tradicional (Jaccard)"""
        self._log(f"üî¨ COMPARANDO M√âTODOS: {pergunta}")
        
        try:
            # M√©todo sem√¢ntico
            start_semantic = time.time()
            similarities_semantic = self.calcular_similaridade_semantica(pergunta)
            time_semantic = time.time() - start_semantic
            
            # M√©todo Jaccard (tradicional) para compara√ß√£o
            start_jaccard = time.time()
            similarities_jaccard = []
            for chunk in self.text_chunks:
                sim = self._calcular_jaccard_simples(pergunta, chunk)
                similarities_jaccard.append(sim)
            time_jaccard = time.time() - start_jaccard
            
            # An√°lise comparativa
            max_semantic = max(similarities_semantic) if similarities_semantic else 0
            max_jaccard = max(similarities_jaccard) if similarities_jaccard else 0
            
            # Contar chunks relevantes para cada m√©todo
            relevant_semantic = sum(1 for s in similarities_semantic if s >= 0.3)
            relevant_jaccard = sum(1 for s in similarities_jaccard if s >= 0.15)
            
            comparison = {
                'pergunta': pergunta,
                'semantic': {
                    'max_similarity': max_semantic,
                    'relevant_chunks': relevant_semantic,
                    'time': time_semantic,
                    'threshold': 0.3
                },
                'jaccard': {
                    'max_similarity': max_jaccard,
                    'relevant_chunks': relevant_jaccard,
                    'time': time_jaccard,
                    'threshold': 0.15
                }
            }
            
            self._log(f"üìä Sem√¢ntico: {max_semantic:.3f} sim, {relevant_semantic} chunks, {time_semantic:.3f}s")
            self._log(f"üìä Jaccard: {max_jaccard:.3f} sim, {relevant_jaccard} chunks, {time_jaccard:.3f}s")
            
            return comparison
            
        except Exception as e:
            self._log(f"‚ùå Erro na compara√ß√£o: {e}")
            return {}
    
    def _calcular_jaccard_simples(self, pergunta: str, texto: str) -> float:
        """Implementa√ß√£o simples de Jaccard para compara√ß√£o"""
        try:
            pergunta_words = set(re.findall(r'\b\w+\b', pergunta.lower()))
            texto_words = set(re.findall(r'\b\w+\b', texto.lower()))
            
            # Remover stop words
            pergunta_words = pergunta_words - self.stop_words
            texto_words = texto_words - self.stop_words
            
            if not pergunta_words or not texto_words:
                return 0.0
            
            intersection = len(pergunta_words & texto_words)
            union = len(pergunta_words | texto_words)
            
            return intersection / union if union > 0 else 0.0
            
        except:
            return 0.0
    
    def executar_testes_comparativos(self):
        """Executa testes comparando m√©todo sem√¢ntico com Jaccard"""
        perguntas_teste = [
            # Testes diretos (devem funcionar bem com ambos)
            "Quem √© Peri?",
            "Fale sobre Cec√≠lia",
            "Quem √© Dom Ant√¥nio de Mariz?",
            
            # Testes sem√¢nticos (devem funcionar melhor com embeddings)
            "Como √© a personalidade do protagonista?",
            "Qual √© o sentimento entre os personagens principais?",
            "Descreva o conflito central da obra",
            "Como √© retratado o amor imposs√≠vel?",
            "Qual o papel da natureza na narrativa?",
            "Fale sobre os antagonistas da hist√≥ria",
            "Como s√£o caracterizados os valores europeus?",
            "Qual a import√¢ncia do castelo na obra?",
            
            # Testes de sin√¥nimos (embeddings devem ser superiores)
            "Quem √© o her√≥i da hist√≥ria?",  # Peri
            "Fale sobre a donzela da obra",  # Cec√≠lia
            "Descreva os inimigos dos protagonistas",  # Aimor√©s
            "Como √© a floresta na narrativa?",  # Natureza
            
            # Testes negativos
            "Como fazer um bolo?",
            "Qual a capital da Fran√ßa?"
        ]
        
        print(f"\nüß™ EXECUTANDO TESTES COMPARATIVOS ({len(perguntas_teste)} perguntas)")
        print("=" * 80)
        print("üî¨ Comparando M√©todo Sem√¢ntico vs M√©todo Jaccard")
        print("=" * 80)
        
        resultados_comparativos = []
        
        for i, pergunta in enumerate(perguntas_teste, 1):
            print(f"\nüìã Teste {i:2d}/{len(perguntas_teste)}: {pergunta}")
            
            try:
                comparison = self.comparar_metodos(pergunta)
                
                if comparison:
                    resultados_comparativos.append(comparison)
                    
                    sem = comparison['semantic']
                    jac = comparison['jaccard']
                    
                    print(f"   üß† Sem√¢ntico: {sem['max_similarity']:.3f} | {sem['relevant_chunks']} chunks | {sem['time']:.3f}s")
                    print(f"   üìù Jaccard:   {jac['max_similarity']:.3f} | {jac['relevant_chunks']} chunks | {jac['time']:.3f}s")
                    
                    # Determinar qual m√©todo foi melhor
                    if sem['max_similarity'] > jac['max_similarity']:
                        print(f"   üèÜ Sem√¢ntico venceu!")
                    elif jac['max_similarity'] > sem['max_similarity']:
                        print(f"   üèÜ Jaccard venceu!")
                    else:
                        print(f"   ü§ù Empate!")
                
            except Exception as e:
                print(f"   ‚ùå ERRO: {e}")
        
        # Relat√≥rio comparativo final
        self._relatorio_comparativo(resultados_comparativos)
        return resultados_comparativos
    
    def _relatorio_comparativo(self, resultados: List[Dict]):
        """Gera relat√≥rio comparativo entre os m√©todos"""
        print(f"\nüìã RELAT√ìRIO COMPARATIVO")
        print("=" * 60)
        
        if not resultados:
            print("‚ùå Nenhum resultado para analisar")
            return
        
        try:
            # M√©tricas sem√¢nticas
            semantic_scores = [r['semantic']['max_similarity'] for r in resultados]
            semantic_times = [r['semantic']['time'] for r in resultados]
            semantic_chunks = [r['semantic']['relevant_chunks'] for r in resultados]
            
            # M√©tricas Jaccard
            jaccard_scores = [r['jaccard']['max_similarity'] for r in resultados]
            jaccard_times = [r['jaccard']['time'] for r in resultados]
            jaccard_chunks = [r['jaccard']['relevant_chunks'] for r in resultados]
            
            # Calcular m√©dias
            sem_avg_score = sum(semantic_scores) / len(semantic_scores)
            jac_avg_score = sum(jaccard_scores) / len(jaccard_scores)
            sem_avg_time = sum(semantic_times) / len(semantic_times)
            jac_avg_time = sum(jaccard_times) / len(jaccard_times)
            sem_avg_chunks = sum(semantic_chunks) / len(semantic_chunks)
            jac_avg_chunks = sum(jaccard_chunks) / len(jaccard_chunks)
            
            print(f"üìä COMPARA√á√ÉO DE PERFORMANCE:")
            print(f"   üß† M√©todo Sem√¢ntico:")
            print(f"      ‚Ä¢ Similaridade m√©dia: {sem_avg_score:.3f}")
            print(f"      ‚Ä¢ Tempo m√©dio: {sem_avg_time:.3f}s")
            print(f"      ‚Ä¢ Chunks relevantes (m√©dia): {sem_avg_chunks:.1f}")
            
            print(f"   üìù M√©todo Jaccard:")
            print(f"      ‚Ä¢ Similaridade m√©dia: {jac_avg_score:.3f}")
            print(f"      ‚Ä¢ Tempo m√©dio: {jac_avg_time:.3f}s")
            print(f"      ‚Ä¢ Chunks relevantes (m√©dia): {jac_avg_chunks:.1f}")
            
            # An√°lise de vit√≥rias
            semantic_wins = 0
            jaccard_wins = 0
            ties = 0
            
            for r in resultados:
                sem_score = r['semantic']['max_similarity']
                jac_score = r['jaccard']['max_similarity']
                
                if sem_score > jac_score:
                    semantic_wins += 1
                elif jac_score > sem_score:
                    jaccard_wins += 1
                else:
                    ties += 1
            
            total = len(resultados)
            print(f"\nüèÜ AN√ÅLISE DE VIT√ìRIAS:")
            print(f"   ‚Ä¢ Sem√¢ntico: {semantic_wins}/{total} ({semantic_wins/total*100:.1f}%)")
            print(f"   ‚Ä¢ Jaccard: {jaccard_wins}/{total} ({jaccard_wins/total*100:.1f}%)")
            print(f"   ‚Ä¢ Empates: {ties}/{total} ({ties/total*100:.1f}%)")
            
            # Conclus√£o
            print(f"\nüìà CONCLUS√ÉO:")
            if semantic_wins > jaccard_wins:
                advantage = ((semantic_wins - jaccard_wins) / total) * 100
                print(f"   ‚úÖ M√©todo Sem√¢ntico superior em {advantage:.1f}% dos casos")
                print(f"   üéØ Melhor para perguntas conceituais e sin√¥nimos")
            elif jaccard_wins > semantic_wins:
                advantage = ((jaccard_wins - semantic_wins) / total) * 100
                print(f"   ‚úÖ M√©todo Jaccard superior em {advantage:.1f}% dos casos")
                print(f"   üéØ Melhor para correspond√™ncias exatas de palavras")
            else:
                print(f"   ü§ù M√©todos equivalentes na maioria dos casos")
            
            print(f"   ‚è±Ô∏è Diferen√ßa de tempo: {sem_avg_time - jac_avg_time:.3f}s (Sem√¢ntico - Jaccard)")
            
        except Exception as e:
            print(f"‚ùå Erro no relat√≥rio comparativo: {e}")
    
    def verificar_arquivo_info(self):
        """Mostra informa√ß√µes sobre o arquivo carregado"""
        print(f"\nüìÅ INFORMA√á√ïES DO ARQUIVO")
        print("=" * 40)
        
        arquivo_path = "guarani.txt"
        
        try:
            if os.path.exists(arquivo_path):
                file_stats = os.stat(arquivo_path)
                file_size = file_stats.st_size
                mod_time = datetime.fromtimestamp(file_stats.st_mtime)
                
                print(f"üìÑ Arquivo: {arquivo_path}")
                print(f"üìè Tamanho: {file_size} bytes")
                print(f"üìÖ Modificado em: {mod_time.strftime('%d/%m/%Y %H:%M:%S')}")
                print(f"‚úÖ Status: Encontrado")
            else:
                print(f"üìÑ Arquivo: {arquivo_path}")
                print(f"‚ùå Status: N√£o encontrado")
                print(f"üí° O sistema criar√° um arquivo de exemplo se necess√°rio")
            
            # Info do cache de embeddings
            if os.path.exists(self.embeddings_cache_file):
                cache_stats = os.stat(self.embeddings_cache_file)
                cache_size = cache_stats.st_size
                cache_time = datetime.fromtimestamp(cache_stats.st_mtime)
                print(f"\nüíæ Cache de embeddings: {self.embeddings_cache_file}")
                print(f"üìè Tamanho do cache: {cache_size:,} bytes")
                print(f"üìÖ Gerado em: {cache_time.strftime('%d/%m/%Y %H:%M:%S')}")
            else:
                print(f"\nüíæ Cache de embeddings: N√£o existe")
            
            if self.texto_guarani:
                words = len(self.texto_guarani.split())
                lines = len(self.texto_guarani.split('\n'))
                chars = len(self.texto_guarani)
                
                print(f"\nüìä CONTE√öDO CARREGADO:")
                print(f"   ‚Ä¢ Caracteres: {chars:,}")
                print(f"   ‚Ä¢ Palavras: {words:,}")
                print(f"   ‚Ä¢ Linhas: {lines:,}")
                print(f"   ‚Ä¢ Primeiros 150 chars: {self.texto_guarani[:150]}...")
            
            # Info do modelo sem√¢ntico
            if self.sentence_model:
                print(f"\nüß† MODELO SEM√ÇNTICO:")
                print(f"   ‚Ä¢ Modelo: {self.model_name}")
                print(f"   ‚Ä¢ Status: ‚úÖ Carregado")
                if self.chunk_embeddings is not None:
                    print(f"   ‚Ä¢ Embeddings: {self.chunk_embeddings.shape}")
                else:
                    print(f"   ‚Ä¢ Embeddings: ‚ùå N√£o gerados")
            else:
                print(f"\nüß† MODELO SEM√ÇNTICO: ‚ùå N√£o carregado")
                
        except Exception as e:
            print(f"‚ùå Erro ao verificar arquivo: {e}")
    
    def mostrar_estatisticas(self):
        """Estat√≠sticas do sistema sem√¢ntico"""
        print(f"\nüìä ESTAT√çSTICAS DO SISTEMA SEM√ÇNTICO")
        print("=" * 50)
        
        try:
            print(f"üìù Chunks: {len(self.text_chunks)}")
            print(f"üîß Threshold sem√¢ntico: {self.similarity_threshold}")
            print(f"üìè Tamanho chunks: {self.chunk_size} palavras")
            print(f"üîÑ Sobreposi√ß√£o: {self.overlap * 100}%")
            print(f"üí¨ Consultas: {len(self.conversation_history)}")
            print(f"üß† Modelo: {self.model_name}")
            print(f"üõ†Ô∏è M√©todo: Embeddings sem√¢nticos")
            
            if self.chunk_embeddings is not None:
                print(f"üìä Embeddings: {self.chunk_embeddings.shape}")
                print(f"üíæ Cache: {self.embeddings_cache_file}")
            
            if self.texto_guarani:
                words = len(self.texto_guarani.split())
                chars = len(self.texto_guarani)
                print(f"üìÑ Texto: {chars:,} chars, {words:,} palavras")
            
            if self.performance_metrics:
                tempos = [float(m.get('tempo', 0)) for m in self.performance_metrics if m.get('tempo')]
                if tempos:
                    print(f"‚è±Ô∏è Tempo m√©dio: {sum(tempos)/len(tempos):.3f}s")
            
            if self.conversation_history:
                similarities = [float(c.get('similaridade_max', 0)) for c in self.conversation_history if c.get('similaridade_max')]
                if similarities:
                    print(f"üìà Similaridade sem√¢ntica m√©dia: {sum(similarities)/len(similarities):.3f}")
                    print(f"üìà Similaridade sem√¢ntica m√°xima: {max(similarities):.3f}")
                    
                # Contar m√©todos usados
                metodos = [c.get('metodo', 'indefinido') for c in self.conversation_history]
                semanticos = metodos.count('semantico')
                print(f"üß† Consultas sem√¢nticas: {semanticos}/{len(self.conversation_history)}")
                    
        except Exception as e:
            print(f"‚ùå Erro nas estat√≠sticas: {e}")
    
    def interface_chat(self):
        """Interface de chat com capacidades sem√¢nticas"""
        print(f"\nü§ñ CHATBOT O GUARANI - CHAT SEM√ÇNTICO INTERATIVO")
        print("=" * 60)
        print("Comandos especiais:")
        print("  ‚Ä¢ 'sair' - Encerrar chat")
        print("  ‚Ä¢ 'stats' - Ver estat√≠sticas")
        print("  ‚Ä¢ 'comparar' - Comparar m√©todos na √∫ltima pergunta")
        print("  ‚Ä¢ 'teste' - Executar testes comparativos")
        print("  ‚Ä¢ 'help' - Mostrar ajuda")
        print("  ‚Ä¢ 'arquivo' - Info sobre arquivo")
        print("  ‚Ä¢ 'cache' - Limpar cache de embeddings")
        print("=" * 60)
        
        ultima_pergunta = ""
        
        while True:
            try:
                pergunta = input("\nüí¨ Sua pergunta: ").strip()
                
                if not pergunta:
                    print("‚ö†Ô∏è Digite uma pergunta ou comando.")
                    continue
                
                if pergunta.lower() in ['sair', 'exit', 'quit', 'tchau']:
                    print("üëã At√© logo!")
                    break
                elif pergunta.lower() in ['stats', 'estatisticas', 'estat√≠sticas']:
                    self.mostrar_estatisticas()
                    continue
                elif pergunta.lower() in ['comparar', 'compare'] and ultima_pergunta:
                    self.comparar_metodos(ultima_pergunta)
                    continue
                elif pergunta.lower() in ['teste', 'testes', 'test']:
                    self.executar_testes_comparativos()
                    continue
                elif pergunta.lower() in ['help', 'ajuda', '?']:
                    self._mostrar_ajuda_semantica()
                    continue
                elif pergunta.lower() in ['arquivo', 'file', 'info']:
                    self.verificar_arquivo_info()
                    continue
                elif pergunta.lower() in ['cache', 'clear', 'limpar']:
                    self._limpar_cache()
                    continue
                
                # Processar pergunta normal
                try:
                    resposta = self.fase3_responder_pergunta(pergunta)
                    print(f"\nü§ñ {resposta}")
                    ultima_pergunta = pergunta
                except Exception as e:
                    print(f"\n‚ùå Erro ao processar pergunta: {e}")
                    print("Tente reformular sua pergunta.")
                
            except KeyboardInterrupt:
                print("\nüëã Encerrando...")
                break
            except Exception as e:
                print(f"\n‚ùå Erro inesperado: {e}")
                print("Digite 'sair' para encerrar ou continue tentando.")
    
    def _limpar_cache(self):
        """Limpa o cache de embeddings"""
        try:
            if os.path.exists(self.embeddings_cache_file):
                os.remove(self.embeddings_cache_file)
                print("‚úÖ Cache de embeddings removido!")
                print("‚ö†Ô∏è Embeddings ser√£o regenerados na pr√≥xima execu√ß√£o")
            else:
                print("‚ÑπÔ∏è Nenhum cache encontrado")
        except Exception as e:
            print(f"‚ùå Erro ao limpar cache: {e}")
    
    def _mostrar_ajuda_semantica(self):
        """Mostra ajuda espec√≠fica para o sistema sem√¢ntico"""
        help_text = """
üÜò AJUDA - CHATBOT O GUARANI SEM√ÇNTICO

üß† AN√ÅLISE SEM√ÇNTICA:
   ‚Ä¢ O sistema usa embeddings para entender SIGNIFICADO
   ‚Ä¢ Funciona bem com sin√¥nimos e conceitos relacionados
   ‚Ä¢ N√£o depende apenas de palavras exatas

üìÅ ARQUIVO:
   ‚Ä¢ Carrega texto do arquivo 'guarani.txt'
   ‚Ä¢ Gera cache de embeddings para velocidade
   ‚Ä¢ Use 'arquivo' para informa√ß√µes detalhadas

üìù PERGUNTAS QUE FUNCIONAM MUITO BEM:

üé≠ Conceituais (for√ßa do sistema sem√¢ntico):
   ‚Ä¢ "Como √© a personalidade do protagonista?"
   ‚Ä¢ "Qual o sentimento entre os personagens?"
   ‚Ä¢ "Descreva o conflito central da obra"
   ‚Ä¢ "Como √© retratado o amor imposs√≠vel?"
   ‚Ä¢ "Qual o papel da natureza na narrativa?"

üßë Usando sin√¥nimos:
   ‚Ä¢ "Quem √© o her√≥i da hist√≥ria?" (= Peri)
   ‚Ä¢ "Fale sobre a donzela" (= Cec√≠lia)
   ‚Ä¢ "Descreva os inimigos" (= aimor√©s)
   ‚Ä¢ "Como √© a floresta?" (= natureza)

üíï Relacionamentos e temas:
   ‚Ä¢ "Qual a devo√ß√£o de Peri?"
   ‚Ä¢ "Como s√£o os valores europeus?"
   ‚Ä¢ "Quais s√£o os antagonistas?"
   ‚Ä¢ "Fale sobre lealdade na obra"

üè∞ Contextuais:
   ‚Ä¢ "Onde acontece a hist√≥ria?"
   ‚Ä¢ "Como √© o ambiente da obra?"
   ‚Ä¢ "Qual a √©poca retratada?"

üí° VANTAGENS SEM√ÇNTICAS:
   ‚Ä¢ Entende sin√¥nimos e conceitos relacionados
   ‚Ä¢ N√£o precisa de palavras exatas
   ‚Ä¢ Melhor para perguntas conceituais
   ‚Ä¢ Compreende contexto e significado

üîß COMANDOS ESPECIAIS:
   ‚Ä¢ 'comparar' - Compara √∫ltimo resultado com Jaccard
   ‚Ä¢ 'teste' - Teste comparativo completo
   ‚Ä¢ 'cache' - Limpar cache de embeddings
   ‚Ä¢ 'stats' - Estat√≠sticas do sistema sem√¢ntico

‚ö° PERFORMANCE:
   ‚Ä¢ Primeira execu√ß√£o: mais lenta (gera embeddings)
   ‚Ä¢ Execu√ß√µes seguintes: r√°pida (usa cache)
   ‚Ä¢ Melhor qualidade sem√¢ntica que m√©todos tradicionais
        """
        print(help_text)

def main():
    """Fun√ß√£o principal do sistema sem√¢ntico"""
    print("üéØ CHATBOT O GUARANI - VERS√ÉO SEM√ÇNTICA AVAN√áADA")
    print("=" * 70)
    print("üß† Esta vers√£o usa embeddings sem√¢nticos para compreens√£o avan√ßada")
    print("üì¶ Requer: sentence-transformers, scikit-learn")
    print("‚ö° Primeira execu√ß√£o pode ser mais lenta (download do modelo)")
    print()
    
    try:
        chatbot = GuaraniChatbotSemantico()
        
        # Mostrar informa√ß√µes do arquivo carregado
        chatbot.verificar_arquivo_info()
        
        if chatbot.executar_sistema_completo():
            print("\n‚úÖ Sistema sem√¢ntico inicializado com sucesso!")
            print("üß† Compreens√£o sem√¢ntica ativada!")
            print("üìÅ Texto carregado do arquivo guarani.txt")
            print("üíæ Cache de embeddings configurado")
            
            # Menu principal
            while True:
                print("\nüéØ MENU PRINCIPAL SEM√ÇNTICO:")
                print("1. üí¨ Chat sem√¢ntico interativo")
                print("2. üî¨ Testes comparativos (Sem√¢ntico vs Jaccard)")
                print("3. üìä Estat√≠sticas do sistema")
                print("4. üìÅ Informa√ß√µes do arquivo e cache")
                print("5. üßπ Limpar cache de embeddings")
                print("6. üÜò Ajuda e exemplos")
                print("7. üö™ Sair")
                
                try:
                    opcao = input("\nEscolha uma op√ß√£o (1-7): ").strip()
                    
                    if opcao == '1':
                        chatbot.interface_chat()
                    elif opcao == '2':
                        chatbot.executar_testes_comparativos()
                    elif opcao == '3':
                        chatbot.mostrar_estatisticas()
                    elif opcao == '4':
                        chatbot.verificar_arquivo_info()
                    elif opcao == '5':
                        chatbot._limpar_cache()
                    elif opcao == '6':
                        chatbot._mostrar_ajuda_semantica()
                    elif opcao == '7':
                        print("üëã Encerrando sistema sem√¢ntico...")
                        break
                    else:
                        print("‚ùå Op√ß√£o inv√°lida. Digite um n√∫mero de 1 a 7.")
                        
                except KeyboardInterrupt:
                    print("\nüëã Encerrando...")
                    break
                except Exception as e:
                    print(f"‚ùå Erro no menu: {e}")
                    print("Tente novamente ou digite 7 para sair.")
        else:
            print("‚ùå Falha na inicializa√ß√£o do sistema")
            
    except ImportError as e:
        print(f"\n‚ùå ERRO DE DEPEND√äNCIAS:")
        print(f"   {e}")
        print(f"\nüì¶ INSTALE AS DEPEND√äNCIAS:")
        print(f"   pip install sentence-transformers scikit-learn")
        print(f"\nüí° OU execute a vers√£o anterior sem embeddings sem√¢nticos")
        
    except Exception as e:
        print(f"‚ùå Erro cr√≠tico: {e}")
        print("Verifique se todas as depend√™ncias est√£o instaladas:")
        print("  pip install sentence-transformers scikit-learn numpy")

if __name__ == "__main__":
    main()