#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chatbot "O Guarani" - VersÃ£o com CompreensÃ£o SemÃ¢ntica
Usando sentence-transformers para embeddings semÃ¢nticos
"""

import numpy as np
import re
import os
from datetime import datetime
from typing import List, Dict, Optional
import time
import pickle
from pathlib import Path

# ImportaÃ§Ãµes para embeddings semÃ¢nticos
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    SEMANTIC_AVAILABLE = True
    print("âœ… Bibliotecas semÃ¢nticas carregadas com sucesso!")
except ImportError as e:
    SEMANTIC_AVAILABLE = False
    print(f"âŒ Erro ao importar bibliotecas semÃ¢nticas: {e}")
    print("ğŸ“¦ Instale com: pip install sentence-transformers scikit-learn")

class GuaraniChatbotSemantico:
    """
    Chatbot O Guarani - VersÃ£o com CompreensÃ£o SemÃ¢ntica AvanÃ§ada
    Usa sentence-transformers para entender o significado das perguntas
    """
    
    def __init__(self):
        print("ğŸš€ Inicializando Chatbot O Guarani (VersÃ£o SemÃ¢ntica)")
        print("=" * 60)
        
        # Verificar dependÃªncias
        if not SEMANTIC_AVAILABLE:
            raise ImportError("âŒ Bibliotecas semÃ¢nticas nÃ£o disponÃ­veis. Execute: pip install sentence-transformers scikit-learn")
        
        # ConfiguraÃ§Ãµes otimizadas
        self.chunk_size = 150
        self.overlap = 0.3
        self.similarity_threshold = 0.3  # Ajustado para embeddings (valores mais altos)
        self.top_chunks = 3
        
        # Estruturas de dados
        self.conversation_history = []
        self.processing_log = []
        self.performance_metrics = []
        self.text_chunks = []
        self.chunk_sentences = []
        self.chunk_embeddings = None  # Para armazenar embeddings dos chunks
        
        # ConfiguraÃ§Ã£o do modelo semÃ¢ntico
        self.model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        self.sentence_model = None
        self.embeddings_cache_file = "guarani_embeddings_cache.pkl"
        
        # Stop words expandidas (ainda Ãºteis para prÃ©-processamento)
        self.stop_words = {
            'a', 'o', 'e', 'de', 'da', 'do', 'em', 'um', 'uma', 'com', 'para',
            'por', 'que', 'se', 'na', 'no', 'ao', 'aos', 'as', 'os', 'mais',
            'mas', 'ou', 'ter', 'ser', 'estar', 'seu', 'sua', 'seus', 'suas',
            'foi', 'sÃ£o', 'dos', 'das', 'pela', 'pelo', 'sobre', 'atÃ©', 'sem',
            'muito', 'bem', 'jÃ¡', 'ainda', 'sÃ³', 'pode', 'tem', 'vai', 'vem',
            'ele', 'ela', 'eles', 'elas', 'isso', 'isto', 'aquilo', 'quando',
            'onde', 'como', 'porque', 'entÃ£o', 'assim', 'aqui', 'ali', 'lÃ¡',
            'me', 'te', 'nos', 'vos', 'lhe', 'lhes', 'meu', 'teu', 'nosso'
        }
        
        # Carregar texto do arquivo
        self.texto_guarani = self._carregar_texto_arquivo()
        
        if not self.texto_guarani:
            raise Exception("Falha ao carregar o arquivo guarani.txt")
        
        # Inicializar modelo semÃ¢ntico
        self._inicializar_modelo_semantico()
        
        self._log("Sistema inicializado com sucesso")
    
    def _inicializar_modelo_semantico(self):
        """Inicializa o modelo de sentence transformers"""
        try:
            self._log("ğŸ§  Carregando modelo de embeddings semÃ¢nticos...")
            self._log(f"ğŸ“¦ Modelo: {self.model_name}")
            
            # Carregar modelo (primeira vez pode demorar para download)
            start_time = time.time()
            self.sentence_model = SentenceTransformer(self.model_name)
            load_time = time.time() - start_time
            
            self._log(f"âœ… Modelo carregado em {load_time:.2f}s")
            
            # Teste rÃ¡pido do modelo
            test_embedding = self.sentence_model.encode(["Teste de funcionamento"])
            self._log(f"ğŸ“ DimensÃ£o dos embeddings: {test_embedding.shape[1]}")
            
        except Exception as e:
            self._log(f"âŒ Erro ao carregar modelo: {e}")
            raise e
    
    def _carregar_texto_arquivo(self) -> str:
        """Carrega o texto de O Guarani do arquivo guarani.txt"""
        arquivo_path = "guarani.txt"
        
        try:
            self._log("Tentando carregar guarani.txt...")
            
            # Verificar se o arquivo existe
            if not os.path.exists(arquivo_path):
                self._log(f"âŒ Arquivo {arquivo_path} nÃ£o encontrado!")
                self._log("Criando arquivo de exemplo...")
                self._criar_arquivo_exemplo(arquivo_path)
                return self._carregar_arquivo_exemplo()
            
            # Tentar diferentes encodings
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
            
            for encoding in encodings:
                try:
                    with open(arquivo_path, 'r', encoding=encoding) as file:
                        texto = file.read().strip()
                        
                    if texto and len(texto) > 100:  # Verificar se o texto nÃ£o estÃ¡ vazio
                        self._log(f"âœ… Arquivo carregado com encoding {encoding}")
                        self._log(f"ğŸ“„ Tamanho do texto: {len(texto)} caracteres")
                        self._log(f"ğŸ“ Primeiras 100 chars: {texto[:100]}...")
                        return texto
                    else:
                        self._log(f"âš ï¸ Arquivo vazio ou muito pequeno com encoding {encoding}")
                        
                except UnicodeDecodeError:
                    self._log(f"âŒ Falha com encoding {encoding}")
                    continue
                except Exception as e:
                    self._log(f"âŒ Erro ao ler com {encoding}: {e}")
                    continue
            
            # Se chegou aqui, todas as tentativas falharam
            self._log("âŒ Falha ao carregar com todos os encodings testados")
            self._log("ğŸ“ Usando texto de exemplo...")
            return self._carregar_arquivo_exemplo()
            
        except Exception as e:
            self._log(f"âŒ Erro crÃ­tico ao carregar arquivo: {e}")
            self._log("ğŸ“ Usando texto de exemplo...")
            return self._carregar_arquivo_exemplo()
    
    def _criar_arquivo_exemplo(self, arquivo_path: str):
        """Cria um arquivo de exemplo com texto bÃ¡sico de O Guarani"""
        texto_exemplo = """O Guarani Ã© um romance indianista de JosÃ© de Alencar, publicado em 1857. A narrativa se desenvolve no sÃ©culo XVII, durante o perÃ­odo colonial brasileiro, nas montanhas fluminenses prÃ³ximas ao rio Paquequer.

Peri Ã© o protagonista da obra, um Ã­ndio goitacÃ¡ de forÃ§a hercÃºlea e lealdade inabalÃ¡vel. Ele Ã© descrito como um guerreiro corajoso, de estatura imponente e carÃ¡ter nobre. Peri demonstra uma devoÃ§Ã£o absoluta a CecÃ­lia (Ceci), filha do fidalgo portuguÃªs Dom AntÃ´nio de Mariz.

CecÃ­lia, chamada carinhosamente de Ceci, Ã© uma jovem portuguesa de beleza singular e carÃ¡ter doce. Ela Ã© filha de Dom AntÃ´nio de Mariz e representa a pureza e a inocÃªncia feminina idealizadas pelo Romantismo.

Dom AntÃ´nio de Mariz Ã© um nobre portuguÃªs, fidalgo da Casa Real, que se estabeleceu no Brasil apÃ³s cometer um crime de honra em Portugal. Ele construiu um castelo fortificado nas margens do rio Paquequer.

Ãlvaro Ã© um jovem portuguÃªs, primo de CecÃ­lia, que tambÃ©m habita o castelo. Ele encarna o ideal do cavaleiro medieval, sendo corajoso, nobre e apaixonado por Ceci.

Isabel Ã© irmÃ£ de CecÃ­lia, uma jovem impetuosa e apaixonada. Ela se enamora de Ãlvaro, criando um triÃ¢ngulo amoroso que adiciona complexidade Ã s relaÃ§Ãµes familiares.

Os aimorÃ©s sÃ£o a tribo indÃ­gena antagonista, inimigos mortais de Peri e de sua tribo goitacÃ¡. Eles representam o perigo constante que ameaÃ§a a seguranÃ§a dos habitantes do castelo.

Loredano Ã© um dos antagonistas da histÃ³ria, um aventureiro italiano que se infiltra no castelo com intenÃ§Ãµes malÃ©volas. Ele planeja assassinar Dom AntÃ´nio e se apossar de suas riquezas.

A natureza brasileira desempenha papel fundamental na narrativa, sendo descrita com exuberÃ¢ncia e riqueza de detalhes. Alencar retrata as florestas, rios e montanhas como cenÃ¡rio Ã©pico.

O romance explora temas centrais como o amor impossÃ­vel entre raÃ§as diferentes, representado pela relaÃ§Ã£o entre Peri e Ceci. A lealdade e o sacrifÃ­cio sÃ£o exemplificados pela devoÃ§Ã£o absoluta do Ã­ndio Ã  famÃ­lia Mariz."""
        
        try:
            with open(arquivo_path, 'w', encoding='utf-8') as file:
                file.write(texto_exemplo)
            self._log(f"âœ… Arquivo de exemplo criado: {arquivo_path}")
        except Exception as e:
            self._log(f"âŒ Erro ao criar arquivo de exemplo: {e}")
    
    def _carregar_arquivo_exemplo(self) -> str:
        """Retorna texto de exemplo quando o arquivo nÃ£o pode ser carregado"""
        return """O Guarani Ã© um romance indianista de JosÃ© de Alencar, publicado em 1857. A narrativa se desenvolve no sÃ©culo XVII, durante o perÃ­odo colonial brasileiro, nas montanhas fluminenses prÃ³ximas ao rio Paquequer.

Peri Ã© o protagonista da obra, um Ã­ndio goitacÃ¡ de forÃ§a hercÃºlea e lealdade inabalÃ¡vel. Ele Ã© descrito como um guerreiro corajoso, de estatura imponente e carÃ¡ter nobre. Peri demonstra uma devoÃ§Ã£o absoluta a CecÃ­lia (Ceci), filha do fidalgo portuguÃªs Dom AntÃ´nio de Mariz. Esta devoÃ§Ã£o representa o amor impossÃ­vel entre duas raÃ§as distintas.

CecÃ­lia, chamada carinhosamente de Ceci, Ã© uma jovem portuguesa de beleza singular e carÃ¡ter doce. Ela Ã© filha de Dom AntÃ´nio de Mariz e representa a pureza e a inocÃªncia feminina idealizadas pelo Romantismo. Ceci desenvolve sentimentos fraternais por Peri, vendo nele um protetor dedicado.

Dom AntÃ´nio de Mariz Ã© um nobre portuguÃªs, fidalgo da Casa Real, que se estabeleceu no Brasil apÃ³s cometer um crime de honra em Portugal. Ele construiu um castelo fortificado nas margens do rio Paquequer, onde vive com sua famÃ­lia. Dom AntÃ´nio Ã© caracterizado como um homem honrado, mas marcado pelo passado.

Ãlvaro Ã© um jovem portuguÃªs, primo de CecÃ­lia, que tambÃ©m habita o castelo. Ele encarna o ideal do cavaleiro medieval, sendo corajoso, nobre e apaixonado por Ceci. Ãlvaro representa a civilizaÃ§Ã£o europeia em contraste com a natureza selvagem de Peri.

Isabel Ã© irmÃ£ de CecÃ­lia, uma jovem impetuosa e apaixonada. Ela se enamora de Ãlvaro, criando um triÃ¢ngulo amoroso que adiciona complexidade Ã s relaÃ§Ãµes familiares. Isabel possui um temperamento mais forte que sua irmÃ£.

Os aimorÃ©s sÃ£o a tribo indÃ­gena antagonista, inimigos mortais de Peri e de sua tribo goitacÃ¡. Eles representam o perigo constante que ameaÃ§a a seguranÃ§a dos habitantes do castelo. Os aimorÃ©s sÃ£o descritos como selvagens e canibais.

Loredano Ã© um dos antagonistas da histÃ³ria, um aventureiro italiano que se infiltra no castelo com intenÃ§Ãµes malÃ©volas. Ele planeja assassinar Dom AntÃ´nio e se apossar de suas riquezas, representando a traiÃ§Ã£o e a vilania."""
    
    def _log(self, message: str):
        """Log seguro"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.processing_log.append(log_entry)
        print(f"ğŸ“ {log_entry}")
    
    def fase1_analisar_texto(self):
        """Fase 1: AnÃ¡lise do texto"""
        self._log("=== FASE 1: ANÃLISE DO TEXTO ===")
        
        if not self.texto_guarani:
            self._log("âŒ Texto nÃ£o carregado!")
            return False
        
        chars = len(self.texto_guarani)
        words = self.texto_guarani.split()
        sentences = self._segmentar_sentencas(self.texto_guarani)
        
        word_tokens = re.findall(r'\b\w+\b', self.texto_guarani.lower())
        unique_words = set(word_tokens)
        content_words = unique_words - self.stop_words
        
        self._log(f"Caracteres: {chars}")
        self._log(f"Palavras: {len(words)}")
        self._log(f"SentenÃ§as: {len(sentences)}")
        self._log(f"VocabulÃ¡rio Ãºnico: {len(unique_words)}")
        self._log(f"Palavras de conteÃºdo: {len(content_words)}")
        
        return True
    
    def _segmentar_sentencas(self, texto: str) -> List[str]:
        """SegmentaÃ§Ã£o robusta de sentenÃ§as"""
        # Limpeza inicial
        texto = re.sub(r'\n+', ' ', texto)
        texto = re.sub(r'\s+', ' ', texto).strip()
        
        # SegmentaÃ§Ã£o por pontuaÃ§Ã£o
        sentences = re.split(r'[.!?]+', texto)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.split()) > 2]
        
        return sentences
    
    def fase2_criar_chunks_e_embeddings(self):
        """Fase 2: CriaÃ§Ã£o de chunks e geraÃ§Ã£o de embeddings semÃ¢nticos"""
        self._log("=== FASE 2: CRIAÃ‡ÃƒO DE CHUNKS E EMBEDDINGS ===")
        
        if not self.texto_guarani:
            self._log("âŒ Texto nÃ£o carregado!")
            return False
        
        if not self.sentence_model:
            self._log("âŒ Modelo semÃ¢ntico nÃ£o carregado!")
            return False
        
        # Criar chunks de texto
        sentences = self._segmentar_sentencas(self.texto_guarani)
        
        chunks = []
        chunk_sentences_map = []
        current_chunk_sentences = []
        current_word_count = 0
        
        for sentence in sentences:
            words = sentence.split()
            sentence_word_count = len(words)
            
            # Verificar se cabe no chunk atual
            if current_word_count + sentence_word_count <= self.chunk_size:
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
            else:
                # Finalizar chunk atual
                if current_chunk_sentences:
                    chunk_text = '. '.join(current_chunk_sentences) + '.'
                    chunks.append(chunk_text)
                    chunk_sentences_map.append(current_chunk_sentences.copy())
                
                # Aplicar sobreposiÃ§Ã£o
                overlap_size = int(len(current_chunk_sentences) * self.overlap)
                if overlap_size > 0 and len(current_chunk_sentences) > overlap_size:
                    current_chunk_sentences = current_chunk_sentences[-overlap_size:]
                    current_word_count = sum(len(s.split()) for s in current_chunk_sentences)
                else:
                    current_chunk_sentences = []
                    current_word_count = 0
                
                # Adicionar nova sentenÃ§a
                current_chunk_sentences.append(sentence)
                current_word_count += sentence_word_count
        
        # Finalizar Ãºltimo chunk
        if current_chunk_sentences:
            chunk_text = '. '.join(current_chunk_sentences) + '.'
            chunks.append(chunk_text)
            chunk_sentences_map.append(current_chunk_sentences.copy())
        
        self.text_chunks = chunks
        self.chunk_sentences = chunk_sentences_map
        
        # Verificar se existe cache de embeddings
        cache_exists = self._verificar_cache_embeddings()
        
        if cache_exists:
            self._log("ğŸ“ Cache de embeddings encontrado, carregando...")
            if self._carregar_cache_embeddings():
                self._log("âœ… Embeddings carregados do cache!")
            else:
                self._log("âŒ Falha ao carregar cache, gerando novos embeddings...")
                self._gerar_embeddings()
        else:
            self._log("ğŸ§  Gerando embeddings semÃ¢nticos dos chunks...")
            self._gerar_embeddings()
        
        # EstatÃ­sticas
        if chunks and self.chunk_embeddings is not None:
            chunk_sizes = [len(chunk.split()) for chunk in chunks]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            self._log(f"Chunks criados: {len(chunks)}")
            self._log(f"Tamanho mÃ©dio: {avg_size:.1f} palavras")
            self._log(f"Embeddings shape: {self.chunk_embeddings.shape}")
        
        return True
    
    def _verificar_cache_embeddings(self) -> bool:
        """Verifica se existe cache de embeddings vÃ¡lido"""
        try:
            if not os.path.exists(self.embeddings_cache_file):
                return False
            
            # Verificar se o cache nÃ£o estÃ¡ muito antigo
            cache_time = os.path.getmtime(self.embeddings_cache_file)
            text_time = os.path.getmtime("guarani.txt") if os.path.exists("guarani.txt") else 0
            
            # Se o texto foi modificado depois do cache, invalidar
            if text_time > cache_time:
                self._log("âš ï¸ Texto modificado, cache invÃ¡lido")
                return False
            
            return True
            
        except Exception as e:
            self._log(f"âŒ Erro ao verificar cache: {e}")
            return False
    
    def _carregar_cache_embeddings(self) -> bool:
        """Carrega embeddings do cache"""
        try:
            with open(self.embeddings_cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # Verificar integridade do cache
            if ('embeddings' in cache_data and 
                'chunks' in cache_data and 
                len(cache_data['chunks']) == len(self.text_chunks)):
                
                # Verificar se os chunks sÃ£o os mesmos
                if cache_data['chunks'] == self.text_chunks:
                    self.chunk_embeddings = cache_data['embeddings']
                    return True
                else:
                    self._log("âš ï¸ Chunks diferentes do cache, regenerando...")
                    return False
            else:
                self._log("âš ï¸ Cache invÃ¡lido ou corrompido")
                return False
                
        except Exception as e:
            self._log(f"âŒ Erro ao carregar cache: {e}")
            return False
    
    def _salvar_cache_embeddings(self):
        """Salva embeddings no cache"""
        try:
            cache_data = {
                'embeddings': self.chunk_embeddings,
                'chunks': self.text_chunks,
                'model_name': self.model_name,
                'timestamp': datetime.now()
            }
            
            with open(self.embeddings_cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            self._log(f"ğŸ’¾ Cache salvo: {self.embeddings_cache_file}")
            
        except Exception as e:
            self._log(f"âŒ Erro ao salvar cache: {e}")
    
    def _gerar_embeddings(self):
        """Gera embeddings semÃ¢nticos para todos os chunks"""
        try:
            start_time = time.time()
            
            # Gerar embeddings para todos os chunks de uma vez (mais eficiente)
            self._log(f"ğŸ§  Processando {len(self.text_chunks)} chunks...")
            
            self.chunk_embeddings = self.sentence_model.encode(
                self.text_chunks,
                convert_to_numpy=True,
                show_progress_bar=True,
                batch_size=32  # Processamento em lotes para eficiÃªncia
            )
            
            generation_time = time.time() - start_time
            self._log(f"âœ… Embeddings gerados em {generation_time:.2f}s")
            self._log(f"ğŸ“Š Shape dos embeddings: {self.chunk_embeddings.shape}")
            
            # Salvar no cache
            self._salvar_cache_embeddings()
            
        except Exception as e:
            self._log(f"âŒ Erro ao gerar embeddings: {e}")
            raise e
    
    def calcular_similaridade_semantica(self, pergunta: str, chunks: Optional[List[str]] = None) -> List[float]:
        """Calcula similaridade semÃ¢ntica usando embeddings"""
        try:
            if chunks is None:
                chunks = self.text_chunks
            
            if self.chunk_embeddings is None:
                self._log("âŒ Embeddings nÃ£o carregados!")
                return [0.0] * len(chunks)
            
            # Gerar embedding da pergunta
            question_embedding = self.sentence_model.encode([pergunta], convert_to_numpy=True)
            
            # Calcular similaridade coseno
            similarities = cosine_similarity(question_embedding, self.chunk_embeddings)[0]
            
            # Converter para lista de floats
            similarities = [float(sim) for sim in similarities]
            
            return similarities
            
        except Exception as e:
            self._log(f"âŒ Erro no cÃ¡lculo de similaridade semÃ¢ntica: {e}")
            return [0.0] * len(chunks if chunks else self.text_chunks)
    
    def fase3_responder_pergunta(self, pergunta: str) -> str:
        """Fase 3: Resposta Ã  pergunta usando similaridade semÃ¢ntica"""
        start_time = time.time()
        self._log(f"=== CONSULTA SEMÃ‚NTICA: {pergunta} ===")
        
        if not self.text_chunks:
            return "âŒ Sistema nÃ£o processado. Execute as fases anteriores."
        
        if self.chunk_embeddings is None:
            return "âŒ Embeddings nÃ£o carregados. Execute a Fase 2."
        
        try:
            # Calcular similaridades semÃ¢nticas
            similarities = self.calcular_similaridade_semantica(pergunta)
            
            # Verificar se temos similaridades vÃ¡lidas
            if not similarities:
                return "âŒ Erro no cÃ¡lculo de similaridades semÃ¢nticas."
            
            # Criar resultados de forma segura
            chunk_results = []
            for i, sim in enumerate(similarities):
                chunk_results.append({
                    'chunk_id': i,
                    'chunk': self.text_chunks[i],
                    'similarity': float(sim),
                    'sentences': self.chunk_sentences[i] if i < len(self.chunk_sentences) else []
                })
            
            # Ordenar por similaridade
            chunk_results.sort(key=lambda x: x['similarity'], reverse=True)
            
            # EstatÃ­sticas seguras
            max_sim = chunk_results[0]['similarity'] if chunk_results else 0.0
            mean_sim = sum(similarities) / len(similarities) if similarities else 0.0
            
            self._log(f"Similaridade semÃ¢ntica mÃ¡xima: {max_sim:.3f}")
            self._log(f"Similaridade semÃ¢ntica mÃ©dia: {mean_sim:.3f}")
            
            # Filtrar chunks relevantes (threshold mais alto para embeddings)
            relevant_chunks = []
            for chunk in chunk_results:
                if chunk['similarity'] >= self.similarity_threshold:
                    relevant_chunks.append(chunk)
            
            self._log(f"Chunks semanticamente relevantes: {len(relevant_chunks)}")
            
            # Gerar resposta
            if not relevant_chunks:
                response = self._resposta_nao_encontrada_semantica(pergunta, max_sim)
            else:
                response = self._gerar_resposta_semantica(pergunta, relevant_chunks[:self.top_chunks])
            
            # MÃ©tricas
            processing_time = time.time() - start_time
            self.performance_metrics.append({
                'pergunta': pergunta,
                'tempo': processing_time,
                'max_similarity': max_sim,
                'chunks_relevantes': len(relevant_chunks),
                'metodo': 'semantico'
            })
            
            # HistÃ³rico
            self.conversation_history.append({
                'pergunta': pergunta,
                'resposta': response,
                'similaridade_max': max_sim,
                'chunks_usados': len(relevant_chunks),
                'tempo_resposta': processing_time,
                'metodo': 'semantico',
                'timestamp': datetime.now()
            })
            
            self._log(f"Resposta semÃ¢ntica gerada em {processing_time:.3f}s")
            return response
            
        except Exception as e:
            error_msg = f"âŒ Erro inesperado na anÃ¡lise semÃ¢ntica: {e}"
            self._log(error_msg)
            return error_msg
    
    def _resposta_nao_encontrada_semantica(self, pergunta: str, max_sim: float) -> str:
        """Resposta quando nÃ£o encontra informaÃ§Ãµes semanticamente relevantes"""
        base_msg = "NÃ£o encontrei informaÃ§Ãµes semanticamente relevantes sobre sua pergunta no texto de 'O Guarani'."
        
        if max_sim > 0.2:
            suggestion = "\n\nğŸ’¡ Tente reformular usando termos mais especÃ­ficos ou sinÃ´nimos."
        elif max_sim > 0.1:
            suggestion = "\n\nğŸ’¡ Use nomes de personagens ou conceitos centrais da obra."
        else:
            suggestion = "\n\nğŸ’¡ Sua pergunta pode estar completamente fora do escopo da obra."
        
        examples = """
\nğŸ“ Exemplos de perguntas que funcionam bem com anÃ¡lise semÃ¢ntica:
â€¢ "Como Ã© a personalidade de Peri?"
â€¢ "Qual o sentimento entre Peri e CecÃ­lia?"
â€¢ "Descreva o conflito principal da obra"
â€¢ "Quais sÃ£o os antagonistas da histÃ³ria?"
â€¢ "Como Ã© retratada a natureza brasileira?"
â€¢ "Qual o papel da famÃ­lia Mariz?"
â€¢ "Quais sÃ£o os temas do romance?"
â€¢ "Como Ã© caracterizado o amor impossÃ­vel?"
"""
        
        confidence = f"\n\nğŸ”´ Similaridade semÃ¢ntica baixa (mÃ¡xima: {max_sim:.3f})"
        
        return base_msg + suggestion + examples + confidence
    
    def _gerar_resposta_semantica(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta usando anÃ¡lise semÃ¢ntica"""
        if not chunks:
            return self._resposta_nao_encontrada_semantica(pergunta, 0.0)
        
        try:
            best_chunk = chunks[0]
            
            # Para embeddings semÃ¢nticos, vamos usar o chunk completo mais relevante
            # pois a similaridade jÃ¡ captura o significado geral
            
            if len(chunks) == 1:
                main_content = chunks[0]['chunk']
                intro = "Com base na anÃ¡lise semÃ¢ntica de 'O Guarani':\n\n"
            else:
                # Combinar os chunks mais relevantes semanticamente
                combined_chunks = []
                total_length = 0
                similarity_scores = []
                
                for chunk in chunks[:3]:  # MÃ¡ximo 3 chunks semanticamente relevantes
                    chunk_text = chunk['chunk']
                    if total_length + len(chunk_text) < 800:  # Limite um pouco maior para semÃ¢ntica
                        combined_chunks.append(chunk_text)
                        similarity_scores.append(chunk['similarity'])
                        total_length += len(chunk_text)
                    else:
                        break
                
                main_content = "\n\n".join(combined_chunks)
                avg_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0
                intro = f"Combinando informaÃ§Ãµes semanticamente relevantes de 'O Guarani' (similaridade mÃ©dia: {avg_similarity:.3f}):\n\n"
            
            # Truncar se muito longo, mas manter mais contexto para anÃ¡lise semÃ¢ntica
            if len(main_content) > 700:
                main_content = main_content[:700] + "..."
            
            confidence = self._calcular_confianca_semantica(best_chunk['similarity'])
            return intro + main_content + "\n\n" + confidence
            
        except Exception as e:
            self._log(f"Erro na geraÃ§Ã£o de resposta semÃ¢ntica: {e}")
            return f"âŒ Erro ao gerar resposta semÃ¢ntica: {e}"
    
    def _calcular_confianca_semantica(self, similarity: float) -> str:
        """Calcula indicador de confianÃ§a para similaridade semÃ¢ntica"""
        try:
            sim = float(similarity)
            if sim > 0.7:
                return "ğŸŸ¢ ConfianÃ§a semÃ¢ntica muito alta"
            elif sim > 0.5:
                return "ğŸŸ¢ ConfianÃ§a semÃ¢ntica alta"
            elif sim > 0.4:
                return "ğŸŸ¡ ConfianÃ§a semÃ¢ntica moderada"
            elif sim > 0.3:
                return "ğŸŸ  ConfianÃ§a semÃ¢ntica baixa"
            else:
                return "ğŸ”´ ConfianÃ§a semÃ¢ntica muito baixa"
        except:
            return "âš ï¸ ConfianÃ§a semÃ¢ntica indeterminada"
    
    def executar_sistema_completo(self):
        """Executa todas as fases do sistema semÃ¢ntico"""
        try:
            self._log("ğŸš€ EXECUTANDO SISTEMA SEMÃ‚NTICO COMPLETO")
            
            if not self.fase1_analisar_texto():
                raise Exception("Erro na Fase 1")
            
            if not self.fase2_criar_chunks_e_embeddings():
                raise Exception("Erro na Fase 2 (Embeddings)")
            
            self._log("âœ… Sistema semÃ¢ntico pronto para consultas!")
            return True
            
        except Exception as e:
            self._log(f"âŒ Erro na execuÃ§Ã£o: {e}")
            return False
    
    def comparar_metodos(self, pergunta: str) -> Dict:
        """Compara mÃ©todo semÃ¢ntico com mÃ©todo tradicional (Jaccard)"""
        self._log(f"ğŸ”¬ COMPARANDO MÃ‰TODOS: {pergunta}")
        
        try:
            # MÃ©todo semÃ¢ntico
            start_semantic = time.time()
            similarities_semantic = self.calcular_similaridade_semantica(pergunta)
            time_semantic = time.time() - start_semantic
            
            # MÃ©todo Jaccard (tradicional) para comparaÃ§Ã£o
            start_jaccard = time.time()
            similarities_jaccard = []
            for chunk in self.text_chunks:
                sim = self._calcular_jaccard_simples(pergunta, chunk)
                similarities_jaccard.append(sim)
            time_jaccard = time.time() - start_jaccard
            
            # AnÃ¡lise comparativa
            max_semantic = max(similarities_semantic) if similarities_semantic else 0
            max_jaccard = max(similarities_jaccard) if similarities_jaccard else 0
            
            # Contar chunks relevantes para cada mÃ©todo
            relevant_semantic = sum(1 for s in similarities_semantic if s >= 0.3)
            relevant_jaccard = sum(1 for s in similarities_jaccard if s >= 0.15)
            
            comparison = {
                'pergunta': pergunta,
                'semantic': {
                    'max_similarity': max_semantic,
                    'relevant_chunks': relevant_semantic,
                    'time': time_semantic,
                    'threshold': 0.3
                },
                'jaccard': {
                    'max_similarity': max_jaccard,
                    'relevant_chunks': relevant_jaccard,
                    'time': time_jaccard,
                    'threshold': 0.15
                }
            }
            
            self._log(f"ğŸ“Š SemÃ¢ntico: {max_semantic:.3f} sim, {relevant_semantic} chunks, {time_semantic:.3f}s")
            self._log(f"ğŸ“Š Jaccard: {max_jaccard:.3f} sim, {relevant_jaccard} chunks, {time_jaccard:.3f}s")
            
            return comparison
            
        except Exception as e:
            self._log(f"âŒ Erro na comparaÃ§Ã£o: {e}")
            return {}
    
    def _calcular_jaccard_simples(self, pergunta: str, texto: str) -> float:
        """ImplementaÃ§Ã£o simples de Jaccard para comparaÃ§Ã£o"""
        try:
            pergunta_words = set(re.findall(r'\b\w+\b', pergunta.lower()))
            texto_words = set(re.findall(r'\b\w+\b', texto.lower()))
            
            # Remover stop words
            pergunta_words = pergunta_words - self.stop_words
            texto_words = texto_words - self.stop_words
            
            if not pergunta_words or not texto_words:
                return 0.0
            
            intersection = len(pergunta_words & texto_words)
            union = len(pergunta_words | texto_words)
            
            return intersection / union if union > 0 else 0.0
            
        except:
            return 0.0
    
    def executar_testes_comparativos(self):
        """Executa testes comparando mÃ©todo semÃ¢ntico com Jaccard"""
        perguntas_teste = [
            # Testes diretos (devem funcionar bem com ambos)
            "Quem Ã© Peri?",
            "Fale sobre CecÃ­lia",
            "Quem Ã© Dom AntÃ´nio de Mariz?",
            
            # Testes semÃ¢nticos (devem funcionar melhor com embeddings)
            "Como Ã© a personalidade do protagonista?",
            "Qual Ã© o sentimento entre os personagens principais?",
            "Descreva o conflito central da obra",
            "Como Ã© retratado o amor impossÃ­vel?",
            "Qual o papel da natureza na narrativa?",
            "Fale sobre os antagonistas da histÃ³ria",
            "Como sÃ£o caracterizados os valores europeus?",
            "Qual a importÃ¢ncia do castelo na obra?",
            
            # Testes de sinÃ´nimos (embeddings devem ser superiores)
            "Quem Ã© o herÃ³i da histÃ³ria?",  # Peri
            "Fale sobre a donzela da obra",  # CecÃ­lia
            "Descreva os inimigos dos protagonistas",  # AimorÃ©s
            "Como Ã© a floresta na narrativa?",  # Natureza
            
            # Testes negativos
            "Como fazer um bolo?",
            "Qual a capital da FranÃ§a?"
        ]
        
        print(f"\nğŸ§ª EXECUTANDO TESTES COMPARATIVOS ({len(perguntas_teste)} perguntas)")
        print("=" * 80)
        print("ğŸ”¬ Comparando MÃ©todo SemÃ¢ntico vs MÃ©todo Jaccard")
        print("=" * 80)
        
        resultados_comparativos = []
        
        for i, pergunta in enumerate(perguntas_teste, 1):
            print(f"\nğŸ“‹ Teste {i:2d}/{len(perguntas_teste)}: {pergunta}")
            
            try:
                comparison = self.comparar_metodos(pergunta)
                
                if comparison:
                    resultados_comparativos.append(comparison)
                    
                    sem = comparison['semantic']
                    jac = comparison['jaccard']
                    
                    print(f"   ğŸ§  SemÃ¢ntico: {sem['max_similarity']:.3f} | {sem['relevant_chunks']} chunks | {sem['time']:.3f}s")
                    print(f"   ğŸ“ Jaccard:   {jac['max_similarity']:.3f} | {jac['relevant_chunks']} chunks | {jac['time']:.3f}s")
                    
                    # Determinar qual mÃ©todo foi melhor
                    if sem['max_similarity'] > jac['max_similarity']:
                        print(f"   ğŸ† SemÃ¢ntico venceu!")
                    elif jac['max_similarity'] > sem['max_similarity']:
                        print(f"   ğŸ† Jaccard venceu!")
                    else:
                        print(f"   ğŸ¤ Empate!")
                
            except Exception as e:
                print(f"   âŒ ERRO: {e}")
        
        # RelatÃ³rio comparativo final
        self._relatorio_comparativo(resultados_comparativos)
        return resultados_comparativos
    
    def _relatorio_comparativo(self, resultados: List[Dict]):
        """Gera relatÃ³rio comparativo entre os mÃ©todos"""
        print(f"\nğŸ“‹ RELATÃ“RIO COMPARATIVO")
        print("=" * 60)
        
        if not resultados:
            print("âŒ Nenhum resultado para analisar")
            return
        
        try:
            # MÃ©tricas semÃ¢nticas
            semantic_scores = [r['semantic']['max_similarity'] for r in resultados]
            semantic_times = [r['semantic']['time'] for r in resultados]
            semantic_chunks = [r['semantic']['relevant_chunks'] for r in resultados]
            
            # MÃ©tricas Jaccard
            jaccard_scores = [r['jaccard']['max_similarity'] for r in resultados]
            jaccard_times = [r['jaccard']['time'] for r in resultados]
            jaccard_chunks = [r['jaccard']['relevant_chunks'] for r in resultados]
            
            # Calcular mÃ©dias
            sem_avg_score = sum(semantic_scores) / len(semantic_scores)
            jac_avg_score = sum(jaccard_scores) / len(jaccard_scores)
            sem_avg_time = sum(semantic_times) / len(semantic_times)
            jac_avg_time = sum(jaccard_times) / len(jaccard_times)
            sem_avg_chunks = sum(semantic_chunks) / len(semantic_chunks)
            jac_avg_chunks = sum(jaccard_chunks) / len(jaccard_chunks)
            
            print(f"ğŸ“Š COMPARAÃ‡ÃƒO DE PERFORMANCE:")
            print(f"   ğŸ§  MÃ©todo SemÃ¢ntico:")
            print(f"      â€¢ Similaridade mÃ©dia: {sem_avg_score:.3f}")
            print(f"      â€¢ Tempo mÃ©dio: {sem_avg_time:.3f}s")
            print(f"      â€¢ Chunks relevantes (mÃ©dia): {sem_avg_chunks:.1f}")
            
            print(f"   ğŸ“ MÃ©todo Jaccard:")
            print(f"      â€¢ Similaridade mÃ©dia: {jac_avg_score:.3f}")
            print(f"      â€¢ Tempo mÃ©dio: {jac_avg_time:.3f}s")
            print(f"      â€¢ Chunks relevantes (mÃ©dia): {jac_avg_chunks:.1f}")
            
            # AnÃ¡lise de vitÃ³rias
            semantic_wins = 0
            jaccard_wins = 0
            ties = 0
            
            for r in resultados:
                sem_score = r['semantic']['max_similarity']
                jac_score = r['jaccard']['max_similarity']
                
                if sem_score > jac_score:
                    semantic_wins += 1
                elif jac_score > sem_score:
                    jaccard_wins += 1
                else:
                    ties += 1
            
            total = len(resultados)
            print(f"\nğŸ† ANÃLISE DE VITÃ“RIAS:")
            print(f"   â€¢ SemÃ¢ntico: {semantic_wins}/{total} ({semantic_wins/total*100:.1f}%)")
            print(f"   â€¢ Jaccard: {jaccard_wins}/{total} ({jaccard_wins/total*100:.1f}%)")
            print(f"   â€¢ Empates: {ties}/{total} ({ties/total*100:.1f}%)")
            
            # ConclusÃ£o
            print(f"\nğŸ“ˆ CONCLUSÃƒO:")
            if semantic_wins > jaccard_wins:
                advantage = ((semantic_wins - jaccard_wins) / total) * 100
                print(f"   âœ… MÃ©todo SemÃ¢ntico superior em {advantage:.1f}% dos casos")
                print(f"   ğŸ¯ Melhor para perguntas conceituais e sinÃ´nimos")
            elif jaccard_wins > semantic_wins:
                advantage = ((jaccard_wins - semantic_wins) / total) * 100
                print(f"   âœ… MÃ©todo Jaccard superior em {advantage:.1f}% dos casos")
                print(f"   ğŸ¯ Melhor para correspondÃªncias exatas de palavras")
            else:
                print(f"   ğŸ¤ MÃ©todos equivalentes na maioria dos casos")
            
            print(f"   â±ï¸ DiferenÃ§a de tempo: {sem_avg_time - jac_avg_time:.3f}s (SemÃ¢ntico - Jaccard)")
            
        except Exception as e:
            print(f"âŒ Erro no relatÃ³rio comparativo: {e}")
    
    def verificar_arquivo_info(self):
        """Mostra informaÃ§Ãµes sobre o arquivo carregado"""
        print(f"\nğŸ“ INFORMAÃ‡Ã•ES DO ARQUIVO")
        print("=" * 40)
        
        arquivo_path = "guarani.txt"
        
        try:
            if os.path.exists(arquivo_path):
                file_stats = os.stat(arquivo_path)
                file_size = file_stats.st_size
                mod_time = datetime.fromtimestamp(file_stats.st_mtime)
                
                print(f"ğŸ“„ Arquivo: {arquivo_path}")
                print(f"ğŸ“ Tamanho: {file_size} bytes")
                print(f"ğŸ“… Modificado em: {mod_time.strftime('%d/%m/%Y %H:%M:%S')}")
                print(f"âœ… Status: Encontrado")
            else:
                print(f"ğŸ“„ Arquivo: {arquivo_path}")
                print(f"âŒ Status: NÃ£o encontrado")
                print(f"ğŸ’¡ O sistema criarÃ¡ um arquivo de exemplo se necessÃ¡rio")
            
            # Info do cache de embeddings
            if os.path.exists(self.embeddings_cache_file):
                cache_stats = os.stat(self.embeddings_cache_file)
                cache_size = cache_stats.st_size
                cache_time = datetime.fromtimestamp(cache_stats.st_mtime)
                print(f"\nğŸ’¾ Cache de embeddings: {self.embeddings_cache_file}")
                print(f"ğŸ“ Tamanho do cache: {cache_size:,} bytes")
                print(f"ğŸ“… Gerado em: {cache_time.strftime('%d/%m/%Y %H:%M:%S')}")
            else:
                print(f"\nğŸ’¾ Cache de embeddings: NÃ£o existe")
            
            if self.texto_guarani:
                words = len(self.texto_guarani.split())
                lines = len(self.texto_guarani.split('\n'))
                chars = len(self.texto_guarani)
                
                print(f"\nğŸ“Š CONTEÃšDO CARREGADO:")
                print(f"   â€¢ Caracteres: {chars:,}")
                print(f"   â€¢ Palavras: {words:,}")
                print(f"   â€¢ Linhas: {lines:,}")
                print(f"   â€¢ Primeiros 150 chars: {self.texto_guarani[:150]}...")
            
            # Info do modelo semÃ¢ntico
            if self.sentence_model:
                print(f"\nğŸ§  MODELO SEMÃ‚NTICO:")
                print(f"   â€¢ Modelo: {self.model_name}")
                print(f"   â€¢ Status: âœ… Carregado")
                if self.chunk_embeddings is not None:
                    print(f"   â€¢ Embeddings: {self.chunk_embeddings.shape}")
                else:
                    print(f"   â€¢ Embeddings: âŒ NÃ£o gerados")
            else:
                print(f"\nğŸ§  MODELO SEMÃ‚NTICO: âŒ NÃ£o carregado")
                
        except Exception as e:
            print(f"âŒ Erro ao verificar arquivo: {e}")
    
    def mostrar_estatisticas(self):
        """EstatÃ­sticas do sistema semÃ¢ntico"""
        print(f"\nğŸ“Š ESTATÃSTICAS DO SISTEMA SEMÃ‚NTICO")
        print("=" * 50)
        
        try:
            print(f"ğŸ“ Chunks: {len(self.text_chunks)}")
            print(f"ğŸ”§ Threshold semÃ¢ntico: {self.similarity_threshold}")
            print(f"ğŸ“ Tamanho chunks: {self.chunk_size} palavras")
            print(f"ğŸ”„ SobreposiÃ§Ã£o: {self.overlap * 100}%")
            print(f"ğŸ’¬ Consultas: {len(self.conversation_history)}")
            print(f"ğŸ§  Modelo: {self.model_name}")
            print(f"ğŸ› ï¸ MÃ©todo: Embeddings semÃ¢nticos")
            
            if self.chunk_embeddings is not None:
                print(f"ğŸ“Š Embeddings: {self.chunk_embeddings.shape}")
                print(f"ğŸ’¾ Cache: {self.embeddings_cache_file}")
            
            if self.texto_guarani:
                words = len(self.texto_guarani.split())
                chars = len(self.texto_guarani)
                print(f"ğŸ“„ Texto: {chars:,} chars, {words:,} palavras")
            
            if self.performance_metrics:
                tempos = [float(m.get('tempo', 0)) for m in self.performance_metrics if m.get('tempo')]
                if tempos:
                    print(f"â±ï¸ Tempo mÃ©dio: {sum(tempos)/len(tempos):.3f}s")
            
            if self.conversation_history:
                similarities = [float(c.get('similaridade_max', 0)) for c in self.conversation_history if c.get('similaridade_max')]
                if similarities:
                    print(f"ğŸ“ˆ Similaridade semÃ¢ntica mÃ©dia: {sum(similarities)/len(similarities):.3f}")
                    print(f"ğŸ“ˆ Similaridade semÃ¢ntica mÃ¡xima: {max(similarities):.3f}")
                    
                # Contar mÃ©todos usados
                metodos = [c.get('metodo', 'indefinido') for c in self.conversation_history]
                semanticos = metodos.count('semantico')
                print(f"ğŸ§  Consultas semÃ¢nticas: {semanticos}/{len(self.conversation_history)}")
                    
        except Exception as e:
            print(f"âŒ Erro nas estatÃ­sticas: {e}")
    
    def interface_chat(self):
        """Interface de chat com capacidades semÃ¢nticas"""
        print(f"\nğŸ¤– CHATBOT O GUARANI - CHAT SEMÃ‚NTICO INTERATIVO")
        print("=" * 60)
        print("Comandos especiais:")
        print("  â€¢ 'sair' - Encerrar chat")
        print("  â€¢ 'stats' - Ver estatÃ­sticas")
        print("  â€¢ 'comparar' - Comparar mÃ©todos na Ãºltima pergunta")
        print("  â€¢ 'teste' - Executar testes comparativos")
        print("  â€¢ 'help' - Mostrar ajuda")
        print("  â€¢ 'arquivo' - Info sobre arquivo")
        print("  â€¢ 'cache' - Limpar cache de embeddings")
        print("=" * 60)
        
        ultima_pergunta = ""
        
        while True:
            try:
                pergunta = input("\nğŸ’¬ Sua pergunta: ").strip()
                
                if not pergunta:
                    print("âš ï¸ Digite uma pergunta ou comando.")
                    continue
                
                if pergunta.lower() in ['sair', 'exit', 'quit', 'tchau']:
                    print("ğŸ‘‹ AtÃ© logo!")
                    break
                elif pergunta.lower() in ['stats', 'estatisticas', 'estatÃ­sticas']:
                    self.mostrar_estatisticas()
                    continue
                elif pergunta.lower() in ['comparar', 'compare'] and ultima_pergunta:
                    self.comparar_metodos(ultima_pergunta)
                    continue
                elif pergunta.lower() in ['teste', 'testes', 'test']:
                    self.executar_testes_comparativos()
                    continue
                elif pergunta.lower() in ['help', 'ajuda', '?']:
                    self._mostrar_ajuda_semantica()
                    continue
                elif pergunta.lower() in ['arquivo', 'file', 'info']:
                    self.verificar_arquivo_info()
                    continue
                elif pergunta.lower() in ['cache', 'clear', 'limpar']:
                    self._limpar_cache()
                    continue
                
                # Processar pergunta normal
                try:
                    resposta = self.fase3_responder_pergunta(pergunta)
                    print(f"\nğŸ¤– {resposta}")
                    ultima_pergunta = pergunta
                except Exception as e:
                    print(f"\nâŒ Erro ao processar pergunta: {e}")
                    print("Tente reformular sua pergunta.")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Encerrando...")
                break
            except Exception as e:
                print(f"\nâŒ Erro inesperado: {e}")
                print("Digite 'sair' para encerrar ou continue tentando.")
    
    def _limpar_cache(self):
        """Limpa o cache de embeddings"""
        try:
            if os.path.exists(self.embeddings_cache_file):
                os.remove(self.embeddings_cache_file)
                print("âœ… Cache de embeddings removido!")
                print("âš ï¸ Embeddings serÃ£o regenerados na prÃ³xima execuÃ§Ã£o")
            else:
                print("â„¹ï¸ Nenhum cache encontrado")
        except Exception as e:
            print(f"âŒ Erro ao limpar cache: {e}")
    
    def _mostrar_ajuda_semantica(self):
        """Mostra ajuda especÃ­fica para o sistema semÃ¢ntico"""
        help_text = """
ğŸ†˜ AJUDA - CHATBOT O GUARANI SEMÃ‚NTICO

ğŸ§  ANÃLISE SEMÃ‚NTICA:
   â€¢ O sistema usa embeddings para entender SIGNIFICADO
   â€¢ Funciona bem com sinÃ´nimos e conceitos relacionados
   â€¢ NÃ£o depende apenas de palavras exatas

ğŸ“ ARQUIVO:
   â€¢ Carrega texto do arquivo 'guarani.txt'
   â€¢ Gera cache de embeddings para velocidade
   â€¢ Use 'arquivo' para informaÃ§Ãµes detalhadas

ğŸ“ PERGUNTAS QUE FUNCIONAM MUITO BEM:

ğŸ­ Conceituais (forÃ§a do sistema semÃ¢ntico):
   â€¢ "Como Ã© a personalidade do protagonista?"
   â€¢ "Qual o sentimento entre os personagens?"
   â€¢ "Descreva o conflito central da obra"
   â€¢ "Como Ã© retratado o amor impossÃ­vel?"
   â€¢ "Qual o papel da natureza na narrativa?"

ğŸ§‘ Usando sinÃ´nimos:
   â€¢ "Quem Ã© o herÃ³i da histÃ³ria?" (= Peri)
   â€¢ "Fale sobre a donzela" (= CecÃ­lia)
   â€¢ "Descreva os inimigos" (= aimorÃ©s)
   â€¢ "Como Ã© a floresta?" (= natureza)

ğŸ’• Relacionamentos e temas:
   â€¢ "Qual a devoÃ§Ã£o de Peri?"
   â€¢ "Como sÃ£o os valores europeus?"
   â€¢ "Quais sÃ£o os antagonistas?"
   â€¢ "Fale sobre lealdade na obra"

ğŸ° Contextuais:
   â€¢ "Onde acontece a histÃ³ria?"
   â€¢ "Como Ã© o ambiente da obra?"
   â€¢ "Qual a Ã©poca retratada?"

ğŸ’¡ VANTAGENS SEMÃ‚NTICAS:
   â€¢ Entende sinÃ´nimos e conceitos relacionados
   â€¢ NÃ£o precisa de palavras exatas
   â€¢ Melhor para perguntas conceituais
   â€¢ Compreende contexto e significado

ğŸ”§ COMANDOS ESPECIAIS:
   â€¢ 'comparar' - Compara Ãºltimo resultado com Jaccard
   â€¢ 'teste' - Teste comparativo completo
   â€¢ 'cache' - Limpar cache de embeddings
   â€¢ 'stats' - EstatÃ­sticas do sistema semÃ¢ntico

âš¡ PERFORMANCE:
   â€¢ Primeira execuÃ§Ã£o: mais lenta (gera embeddings)
   â€¢ ExecuÃ§Ãµes seguintes: rÃ¡pida (usa cache)
   â€¢ Melhor qualidade semÃ¢ntica que mÃ©todos tradicionais
        """
        print(help_text)

def main():
    """FunÃ§Ã£o principal do sistema semÃ¢ntico"""
    print("ğŸ¯ CHATBOT O GUARANI - VERSÃƒO SEMÃ‚NTICA AVANÃ‡ADA")
    print("=" * 70)
    print("ğŸ§  Esta versÃ£o usa embeddings semÃ¢nticos para compreensÃ£o avanÃ§ada")
    print("ğŸ“¦ Requer: sentence-transformers, scikit-learn")
    print("âš¡ Primeira execuÃ§Ã£o pode ser mais lenta (download do modelo)")
    print()
    
    try:
        chatbot = GuaraniChatbotSemantico()
        
        # Mostrar informaÃ§Ãµes do arquivo carregado
        chatbot.verificar_arquivo_info()
        
        if chatbot.executar_sistema_completo():
            print("\nâœ… Sistema semÃ¢ntico inicializado com sucesso!")
            print("ğŸ§  CompreensÃ£o semÃ¢ntica ativada!")
            print("ğŸ“ Texto carregado do arquivo guarani.txt")
            print("ğŸ’¾ Cache de embeddings configurado")
            
            # Menu principal
            while True:
                print("\nğŸ¯ MENU PRINCIPAL SEMÃ‚NTICO:")
                print("1. ğŸ’¬ Chat semÃ¢ntico interativo")
                print("2. ğŸ”¬ Testes comparativos (SemÃ¢ntico vs Jaccard)")
                print("3. ğŸ“Š EstatÃ­sticas do sistema")
                print("4. ğŸ“ InformaÃ§Ãµes do arquivo e cache")
                print("5. ğŸ§¹ Limpar cache de embeddings")
                print("6. ğŸ†˜ Ajuda e exemplos")
                print("7. ğŸšª Sair")
                
                try:
                    opcao = input("\nEscolha uma opÃ§Ã£o (1-7): ").strip()
                    
                    if opcao == '1':
                        chatbot.interface_chat()
                    elif opcao == '2':
                        chatbot.executar_testes_comparativos()
                    elif opcao == '3':
                        chatbot.mostrar_estatisticas()
                    elif opcao == '4':
                        chatbot.verificar_arquivo_info()
                    elif opcao == '5':
                        chatbot._limpar_cache()
                    elif opcao == '6':
                        chatbot._mostrar_ajuda_semantica()
                    elif opcao == '7':
                        print("ğŸ‘‹ Encerrando sistema semÃ¢ntico...")
                        break
                    else:
                        print("âŒ OpÃ§Ã£o invÃ¡lida. Digite um nÃºmero de 1 a 7.")
                        
                except KeyboardInterrupt:
                    print("\nğŸ‘‹ Encerrando...")
                    break
                except Exception as e:
                    print(f"âŒ Erro no menu: {e}")
                    print("Tente novamente ou digite 7 para sair.")
        else:
            print("âŒ Falha na inicializaÃ§Ã£o do sistema")
            
    except ImportError as e:
        print(f"\nâŒ ERRO DE DEPENDÃŠNCIAS:")
        print(f"   {e}")
        print(f"\nğŸ“¦ INSTALE AS DEPENDÃŠNCIAS:")
        print(f"   pip install sentence-transformers scikit-learn")
        print(f"\nğŸ’¡ OU execute a versÃ£o anterior sem embeddings semÃ¢nticos")
        
    except Exception as e:
        print(f"âŒ Erro crÃ­tico: {e}")
        print("Verifique se todas as dependÃªncias estÃ£o instaladas:")
        print("  pip install sentence-transformers scikit-learn numpy")

if __name__ == "__main__":
    main()