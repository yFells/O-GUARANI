"""
Chatbot "O Guarani" - Sistema de PLN para consultas sobre a obra de Jos√© de Alencar
Implementa√ß√£o completa das 5 fases descritas no projeto
"""

import os
import numpy as np
import pandas as pd
import re
import requests
from typing import List, Dict, Tuple
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Bibliotecas de PLN
try:
    import spacy
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.stem import RSLPStemmer
except ImportError:
    print("Instalando bibliotecas necess√°rias...")
    os.system("pip install spacy nltk scikit-learn matplotlib seaborn")
    import spacy
    import nltk

# Downloads necess√°rios do NLTK
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/rslp')
except LookupError:
    print("Baixando recursos do NLTK...")
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('rslp')

class GuaraniChatbot:
    """
    Chatbot especializado em responder perguntas sobre "O Guarani" de Jos√© de Alencar
    """
    
    def __init__(self):
        print("üöÄ Inicializando Chatbot O Guarani...")
        
        # Configura√ß√µes
        self.chunk_size = 250  # palavras por chunk
        self.overlap = 0.5     # 50% de sobreposi√ß√£o
        self.similarity_threshold = 0.3  # limiar m√≠nimo de similaridade
        self.top_chunks = 3    # top chunks para resposta
        
        # Componentes do sistema
        self.stemmer = RSLPStemmer()
        self.stop_words = set(stopwords.words('portuguese'))
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words=list(self.stop_words),
            ngram_range=(1, 2)
        )
        
        # Dados do sistema
        self.text_chunks = []
        self.chunk_vectors = None
        self.original_text = ""
        
        # Hist√≥rico
        self.conversation_history = []
        self.processing_log = []
        
        self._log("Sistema inicializado com sucesso")
    
    def _log(self, message: str):
        """Registra eventos no hist√≥rico de processamento"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.processing_log.append(log_entry)
        print(f"üìù {log_entry}")
    
    def fase1_preparar_ambiente(self):
        """Fase 1: Prepara√ß√£o do ambiente e obten√ß√£o dos dados"""
        self._log("=== FASE 1: PREPARA√á√ÉO DO AMBIENTE ===")
        
        # Simulando obten√ß√£o do texto (normalmente seria um arquivo)
        sample_text = """
        O Guarani √© um romance de Jos√© de Alencar publicado em 1857. A hist√≥ria se passa no s√©culo XVII, 
        durante a coloniza√ß√£o do Brasil. O protagonista √© Peri, um √≠ndio goitac√° de for√ßa excepcional 
        e lealdade inquebrant√°vel.
        
        Peri √© caracterizado por sua devo√ß√£o a Cec√≠lia (Ceci), filha do fidalgo portugu√™s Dom Ant√¥nio de Mariz. 
        A rela√ß√£o entre Peri e Ceci representa o encontro entre duas culturas: a ind√≠gena e a europeia.
        
        Dom Ant√¥nio de Mariz √© um nobre portugu√™s que se estabeleceu no Brasil com sua fam√≠lia. 
        Ele possui um castelo fortificado √†s margens do rio Paquequer, onde vive com sua esposa, 
        filhos e alguns agregados.
        
        A obra retrata os conflitos entre diferentes grupos: os portugueses colonizadores, 
        os √≠ndios aimor√©s (inimigos de Peri) e os aventureiros que buscam ouro na regi√£o.
        
        √Ålvaro √© um jovem portugu√™s, primo de Cec√≠lia, que tamb√©m habita o castelo. 
        Ele representa o europeu civilizado, contrastando com a natureza selvagem de Peri.
        
        Isabel √© irm√£ de Cec√≠lia, uma jovem impetuosa que se apaixona por √Ålvaro. 
        Sua hist√≥ria pessoal adiciona complexidade aos relacionamentos na narrativa.
        
        Os aimor√©s s√£o os antagonistas principais da hist√≥ria, representando o perigo 
        constante que amea√ßa a seguran√ßa dos habitantes do castelo.
        
        A natureza brasileira √© quase um personagem na obra, sendo descrita com detalhes 
        que evidenciam a vis√£o rom√¢ntica de Alencar sobre a paisagem nacional.
        
        O romance explora temas como o amor imposs√≠vel entre Peri e Ceci, 
        a lealdade, o sacrif√≠cio e o choque entre civiliza√ß√µes.
        
        A linguagem de Alencar mescla o portugu√™s culto com express√µes que buscam 
        retratar a fala dos personagens ind√≠genas, criando um estilo √∫nico.
        """
        
        self.original_text = sample_text
        self._log(f"Texto carregado: {len(sample_text)} caracteres")
        return True
    
    def fase2_processar_dados(self):
        """Fase 2: Processamento e estrutura√ß√£o dos dados"""
        self._log("=== FASE 2: PROCESSAMENTO DOS DADOS ===")
        
        # Limpeza do texto
        cleaned_text = self._clean_text(self.original_text)
        self._log("Texto limpo e normalizado")
        
        # Tokeniza√ß√£o e segmenta√ß√£o em chunks
        self.text_chunks = self._create_chunks(cleaned_text)
        self._log(f"Criados {len(self.text_chunks)} chunks de texto")
        
        # Processamento dos chunks
        processed_chunks = []
        for i, chunk in enumerate(self.text_chunks):
            processed = self._preprocess_text(chunk)
            processed_chunks.append(processed)
        
        self._log("Chunks pr√©-processados (tokeniza√ß√£o, remo√ß√£o de stopwords)")
        return processed_chunks
    
    def fase3_armazenar_indexar(self, processed_chunks: List[str]):
        """Fase 3: Armazenamento e indexa√ß√£o"""
        self._log("=== FASE 3: ARMAZENAMENTO E INDEXA√á√ÉO ===")
        
        # Cria√ß√£o de vetores TF-IDF (substituto simplificado do Word2Vec)
        self.chunk_vectors = self.vectorizer.fit_transform(processed_chunks)
        self._log(f"Vetores criados: {self.chunk_vectors.shape}")
        
        # Salvamento dos dados
        self._save_data()
        self._log("Dados indexados e salvos")
        
        return True
    
    def fase4_mecanismo_resposta(self, pergunta: str) -> str:
        """Fase 4: Mecanismo de recupera√ß√£o e gera√ß√£o de resposta"""
        self._log(f"=== CONSULTANDO: {pergunta} ===")
        
        # Pr√©-processamento da pergunta
        processed_question = self._preprocess_text(pergunta)
        question_vector = self.vectorizer.transform([processed_question])
        
        # Busca por similaridade
        similarities = cosine_similarity(question_vector, self.chunk_vectors).flatten()
        
        # Ranqueamento e filtragem
        relevant_chunks = []
        for i, similarity in enumerate(similarities):
            if similarity >= self.similarity_threshold:
                relevant_chunks.append({
                    'chunk_id': i,
                    'text': self.text_chunks[i],
                    'similarity': similarity
                })
        
        # Ordena√ß√£o por similaridade
        relevant_chunks.sort(key=lambda x: x['similarity'], reverse=True)
        top_chunks = relevant_chunks[:self.top_chunks]
        
        self._log(f"Encontrados {len(relevant_chunks)} chunks relevantes")
        
        # Gera√ß√£o da resposta
        if not top_chunks:
            response = "Desculpe, n√£o encontrei informa√ß√µes relevantes sobre sua pergunta."
        else:
            response = self._generate_response(pergunta, top_chunks)
        
        # Registro no hist√≥rico
        self.conversation_history.append({
            'pergunta': pergunta,
            'resposta': response,
            'chunks_usados': len(top_chunks),
            'similaridade_max': max([c['similarity'] for c in top_chunks]) if top_chunks else 0
        })
        
        return response
    
    def fase5_interface_usuario(self):
        """Fase 5: Interface com o usu√°rio"""
        self._log("=== FASE 5: INTERFACE ATIVA ===")
        
        print("\n" + "="*60)
        print("ü§ñ CHATBOT O GUARANI")
        print("Especialista na obra de Jos√© de Alencar")
        print("Digite 'sair' para encerrar")
        print("="*60)
        
        while True:
            try:
                pergunta = input("\nüí¨ Sua pergunta: ").strip()
                
                if pergunta.lower() in ['sair', 'exit', 'quit']:
                    print("üëã At√© logo!")
                    break
                
                if not pergunta:
                    continue
                
                resposta = self.fase4_mecanismo_resposta(pergunta)
                print(f"\nü§ñ Resposta: {resposta}")
                
            except KeyboardInterrupt:
                print("\nüëã Encerrando...")
                break
    
    def _clean_text(self, text: str) -> str:
        """Limpa e normaliza o texto"""
        # Remove quebras de linha desnecess√°rias
        text = re.sub(r'\n+', ' ', text)
        # Remove espa√ßos m√∫ltiplos
        text = re.sub(r'\s+', ' ', text)
        # Remove caracteres especiais mantendo pontua√ß√£o b√°sica
        text = re.sub(r'[^\w\s.,!?;:]', '', text)
        return text.strip()
    
    def _preprocess_text(self, text: str) -> str:
        """Pr√©-processa texto: tokeniza√ß√£o, remo√ß√£o de stopwords, stemming"""
        # Tokeniza√ß√£o
        tokens = word_tokenize(text.lower(), language='portuguese')
        
        # Remo√ß√£o de stopwords e tokens muito pequenos
        filtered_tokens = [
            token for token in tokens 
            if token not in self.stop_words and len(token) > 2 and token.isalpha()
        ]
        
        # Stemming
        stemmed_tokens = [self.stemmer.stem(token) for token in filtered_tokens]
        
        return ' '.join(stemmed_tokens)
    
    def _create_chunks(self, text: str) -> List[str]:
        """Cria chunks de texto com sobreposi√ß√£o"""
        sentences = sent_tokenize(text, language='portuguese')
        chunks = []
        
        current_chunk = []
        current_word_count = 0
        
        for sentence in sentences:
            words = sentence.split()
            
            if current_word_count + len(words) <= self.chunk_size:
                current_chunk.append(sentence)
                current_word_count += len(words)
            else:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                
                # Sobreposi√ß√£o: mant√©m parte do chunk anterior
                overlap_sentences = int(len(current_chunk) * self.overlap)
                current_chunk = current_chunk[-overlap_sentences:] if overlap_sentences > 0 else []
                current_chunk.append(sentence)
                current_word_count = sum(len(s.split()) for s in current_chunk)
        
        # Adiciona o √∫ltimo chunk
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _generate_response(self, pergunta: str, chunks: List[Dict]) -> str:
        """Gera resposta baseada nos chunks mais relevantes"""
        # Template de resposta
        intro = "Com base na obra 'O Guarani':\n\n"
        
        # Extrai informa√ß√µes dos chunks mais relevantes
        main_content = chunks[0]['text']
        
        # Formata√ß√£o simples da resposta
        response = intro + main_content
        
        # Adiciona informa√ß√£o de confian√ßa
        confidence = chunks[0]['similarity']
        if confidence > 0.7:
            confidence_text = " (Alta confian√ßa)"
        elif confidence > 0.5:
            confidence_text = " (Confian√ßa moderada)"
        else:
            confidence_text = " (Baixa confian√ßa)"
        
        return response + confidence_text
    
    def _save_data(self):
        """Salva os dados processados"""
        data = {
            'chunks': self.text_chunks,
            'vectors': self.chunk_vectors,
            'vectorizer': self.vectorizer
        }
        
        # Em um sistema real, salvaria em arquivo
        # pickle.dump(data, open('guarani_data.pkl', 'wb'))
        self._log("Dados salvos em mem√≥ria")
    
    def executar_sistema_completo(self):
        """Executa todas as fases do sistema"""
        try:
            # Fase 1: Prepara√ß√£o
            if not self.fase1_preparar_ambiente():
                raise Exception("Erro na Fase 1")
            
            # Fase 2: Processamento
            processed_chunks = self.fase2_processar_dados()
            if not processed_chunks:
                raise Exception("Erro na Fase 2")
            
            # Fase 3: Indexa√ß√£o
            if not self.fase3_armazenar_indexar(processed_chunks):
                raise Exception("Erro na Fase 3")
            
            # Fase 4 e 5: Sistema ativo
            self._log("Sistema pronto para consultas!")
            
            # Exemplos de teste
            self.testar_sistema()
            
            return True
            
        except Exception as e:
            self._log(f"Erro na execu√ß√£o: {e}")
            return False
    
    def testar_sistema(self):
        """Testa o sistema com perguntas exemplo"""
        perguntas_teste = [
            "Quem √© Peri?",
            "Fale sobre Cec√≠lia",
            "Qual √© o enredo do livro?",
            "Quem s√£o os personagens principais?",
            "Onde se passa a hist√≥ria?"
        ]
        
        print("\n" + "="*50)
        print("üß™ TESTE AUTOM√ÅTICO DO SISTEMA")
        print("="*50)
        
        for pergunta in perguntas_teste:
            print(f"\n‚ùì Pergunta: {pergunta}")
            resposta = self.fase4_mecanismo_resposta(pergunta)
            print(f"ü§ñ Resposta: {resposta[:200]}...")
    
    def mostrar_historico(self):
        """Mostra o hist√≥rico de processamento e conversas"""
        print("\n" + "="*50)
        print("üìä HIST√ìRICO DE PROCESSAMENTO")
        print("="*50)
        
        for log in self.processing_log:
            print(log)
        
        if self.conversation_history:
            print("\n" + "="*50)
            print("üí¨ HIST√ìRICO DE CONVERSAS")
            print("="*50)
            
            for i, conv in enumerate(self.conversation_history, 1):
                print(f"\n{i}. {conv['pergunta']}")
                print(f"   Resposta: {conv['resposta'][:100]}...")
                print(f"   Chunks usados: {conv['chunks_usados']}")
                print(f"   Similaridade: {conv['similaridade_max']:.3f}")
    
    def estatisticas_sistema(self):
        """Mostra estat√≠sticas do sistema"""
        print("\n" + "="*50)
        print("üìà ESTAT√çSTICAS DO SISTEMA")
        print("="*50)
        
        print(f"Total de chunks: {len(self.text_chunks)}")
        print(f"Tamanho do vocabul√°rio: {len(self.vectorizer.vocabulary_) if hasattr(self.vectorizer, 'vocabulary_') else 'N/A'}")
        print(f"Consultas realizadas: {len(self.conversation_history)}")
        print(f"Threshold de similaridade: {self.similarity_threshold}")
        print(f"Eventos de log: {len(self.processing_log)}")


# Fun√ß√£o principal para execu√ß√£o
def main():
    """Fun√ß√£o principal para executar o chatbot"""
    chatbot = GuaraniChatbot()
    
    print("Executando sistema completo...")
    if chatbot.executar_sistema_completo():
        print("\n‚úÖ Sistema executado com sucesso!")
        
        # Mostra estat√≠sticas
        chatbot.estatisticas_sistema()
        
        # Mostra hist√≥rico
        chatbot.mostrar_historico()
        
        # Oferece interface interativa
        print("\nDeseja iniciar a interface de chat? (s/n)")
        resposta = input().strip().lower()
        if resposta in ['s', 'sim', 'y', 'yes']:
            chatbot.fase5_interface_usuario()
    
    else:
        print("‚ùå Erro na execu√ß√£o do sistema")

# Execu√ß√£o
if __name__ == "__main__":
    main()
