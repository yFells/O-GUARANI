#!/usr/bin/env python3
"""
Chatbot "O Guarani" - VersÃ£o Otimizada Final
Sistema corrigido que encontra chunks relevantes com eficiÃªncia
"""

import os
import numpy as np
import re
from typing import List, Dict, Tuple
from datetime import datetime

# VerificaÃ§Ã£o e instalaÃ§Ã£o de dependÃªncias
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    print("ðŸ“¦ Instalando dependÃªncias necessÃ¡rias...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "scikit-learn", "numpy"])
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

class GuaraniChatbotOtimizado:
    """
    Chatbot otimizado para responder perguntas sobre "O Guarani"
    VersÃ£o corrigida que resolve problemas de detecÃ§Ã£o de chunks
    """
    
    def __init__(self, debug=True):
        self.debug = debug
        self._log("ðŸš€ Inicializando Chatbot O Guarani (VersÃ£o Otimizada)")
        
        # ConfiguraÃ§Ãµes otimizadas
        self.similarity_threshold = 0.05  # Limiar muito baixo
        self.min_adaptive_threshold = 0.01  # Limiar mÃ­nimo adaptativo
        self.top_chunks = 3
        self.chunk_overlap = 0.3  # SobreposiÃ§Ã£o entre chunks
        
        # Dados do sistema
        self.text_chunks = []
        self.chunk_vectors = None
        self.vectorizer = None
        self.original_text = ""
        
        # HistÃ³rico detalhado
        self.conversation_history = []
        self.processing_log = []
        
        # Executar inicializaÃ§Ã£o
        if self._carregar_e_processar():
            self._log("âœ… Sistema inicializado com sucesso!")
        else:
            self._log("âŒ Erro na inicializaÃ§Ã£o")
    
    def _log(self, message: str):
        """Registra eventos no histÃ³rico"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.processing_log.append(log_entry)
        if self.debug:
            print(f"ðŸ“ {log_entry}")
    
    def _carregar_e_processar(self) -> bool:
        """Carrega texto e processa em chunks"""
        try:
            # Fase 1: Carregar texto
            texto = self._carregar_texto()
            if not texto:
                return False
            
            # Fase 2: Criar chunks otimizados
            self.text_chunks = self._criar_chunks_otimizados(texto)
            if not self.text_chunks:
                self._log("Erro: Nenhum chunk criado")
                return False
            
            # Fase 3: VetorizaÃ§Ã£o otimizada
            return self._vetorizar_chunks()
            
        except Exception as e:
            self._log(f"Erro no processamento: {e}")
            return False
    
    def _carregar_texto(self) -> str:
        """Carrega texto do arquivo ou usa fallback"""
        # Tentar carregar guarani.txt
        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-8-sig']
        
        for encoding in encodings:
            try:
                with open('guarani.txt', 'r', encoding=encoding) as arquivo:
                    texto = arquivo.read()
                    self._log(f"ðŸ“– Arquivo carregado com {encoding}: {len(texto):,} chars")
                    
                    if len(texto) < 1000:
                        self._log("âš ï¸  Arquivo pequeno, pode estar incompleto")
                    
                    self.original_text = texto
                    return texto
                    
            except UnicodeDecodeError:
                continue
            except FileNotFoundError:
                self._log("ðŸ“„ Arquivo guarani.txt nÃ£o encontrado, usando texto de demonstraÃ§Ã£o")
                break
        
        # Usar texto de fallback expandido
        self.original_text = self._texto_demonstracao()
        return self.original_text
    
    def _texto_demonstracao(self) -> str:
        """Texto de demonstraÃ§Ã£o expandido para testes"""
        return """
        CAPÃTULO I - A CHEGADA DE PERI
        
        Dom AntÃ´nio de Mariz era um fidalgo portuguÃªs que se estabeleceu no Brasil em 1604.
        Ele construiu um castelo fortificado Ã s margens do rio Paquequer, na regiÃ£o que hoje
        Ã© conhecida como PetrÃ³polis. Dom AntÃ´nio era um homem justo e honrado, respeitado 
        por todos os habitantes da regiÃ£o.
        
        Peri era um Ã­ndio goitacÃ¡ de forÃ§a excepcional e lealdade inquebrantÃ¡vel. Quando
        apareceu pela primeira vez diante do castelo, Dom AntÃ´nio o recebeu com desconfianÃ§a,
        como era natural entre colonos portugueses e indÃ­genas naquela Ã©poca.
        
        CAPÃTULO II - A EVOLUÃ‡ÃƒO DA RELAÃ‡ÃƒO
        
        A relaÃ§Ã£o entre Peri e Dom AntÃ´nio evoluiu gradualmente ao longo da histÃ³ria.
        No inÃ­cio, Dom AntÃ´nio mantinha distÃ¢ncia do Ã­ndio, vendo-o com os preconceitos
        tÃ­picos de sua Ã©poca. PorÃ©m, Peri demonstrou sua lealdade salvando CecÃ­lia vÃ¡rias vezes.
        
        Com o passar do tempo, Dom AntÃ´nio passou a reconhecer a nobreza de carÃ¡ter de Peri.
        O fidalgo portuguÃªs comeÃ§ou a confiar no Ã­ndio e atÃ© mesmo a consultÃ¡-lo sobre
        assuntos importantes relacionados Ã  defesa do castelo.
        
        CAPÃTULO III - CECÃLIA E O AMOR DE PERI
        
        CecÃ­lia era a filha querida de Dom AntÃ´nio de Mariz. Ela era conhecida por todos
        como Ceci. CecÃ­lia representava a pureza e inocÃªncia da jovem portuguesa no Brasil.
        Peri devotava amor incondicional a CecÃ­lia, adorando-a como uma divindade.
        
        A relaÃ§Ã£o entre Peri e CecÃ­lia era platÃ´nica e idealizada. O Ã­ndio via na jovem
        portuguesa a personificaÃ§Ã£o de todas as virtudes. CecÃ­lia, por sua vez, desenvolvia
        afeiÃ§Ã£o especial por Peri, reconhecendo sua pureza de alma.
        
        CAPÃTULO IV - OS DEMAIS PERSONAGENS
        
        Ãlvaro era primo de CecÃ­lia e habitava o castelo. Ele representava o jovem portuguÃªs
        civilizado e culto. Ãlvaro desenvolveu amizade e respeito por Peri, reconhecendo
        suas qualidades excepcionais e sua honra natural.
        
        Isabel era irmÃ£ de CecÃ­lia, uma jovem de temperamento impetuoso e apaixonado.
        Seus amores por Ãlvaro eram conhecidos de todos no castelo. Isabel representava
        a paixÃ£o ardente em contraste com a pureza de CecÃ­lia.
        
        CAPÃTULO V - OS INIMIGOS AIMORÃ‰S
        
        Os aimorÃ©s eram uma tribo feroz que habitava as florestas circunvizinhas ao castelo.
        Eram inimigos naturais dos goitacÃ¡s, tribo Ã  qual pertencia Peri. Constantemente
        atacavam os habitantes da regiÃ£o, espalhando terror e destruiÃ§Ã£o por onde passavam.
        
        Peri lutou bravamente contra os aimorÃ©s para proteger a famÃ­lia de Dom AntÃ´nio.
        Esta proteÃ§Ã£o incansÃ¡vel fortaleceu ainda mais os laÃ§os entre o Ã­ndio e o fidalgo
        portuguÃªs, consolidando uma relaÃ§Ã£o de confianÃ§a mÃºtua.
        
        CAPÃTULO VI - A NATUREZA BRASILEIRA
        
        A natureza brasileira era exuberante e selvagem. JosÃ© de Alencar descreveu
        detalhadamente as florestas imensas, os rios cristalinos e a fauna diversificada
        da regiÃ£o. A paisagem servia como cenÃ¡rio grandioso para as aventuras dos personagens.
        
        Peri conhecia todos os segredos da floresta. CaÃ§ava com destreza incomparÃ¡vel,
        seguia rastros como nenhum outro Ã­ndio, e movia-se pela mata com a agilidade
        de um felino. Sua forÃ§a era lendÃ¡ria entre os Ã­ndios da regiÃ£o.
        
        CAPÃTULO VII - O CLÃMAX E A REDENÃ‡ÃƒO
        
        No final da histÃ³ria, Dom AntÃ´nio confiou completamente em Peri. Quando o castelo
        foi atacado pelos aimorÃ©s, Dom AntÃ´nio sabia que apenas Peri poderia salvar sua filha.
        Esta confianÃ§a total representa o Ã¡pice da evoluÃ§Ã£o de sua relaÃ§Ã£o.
        
        A evoluÃ§Ã£o da relaÃ§Ã£o entre Dom AntÃ´nio e Peri mostra como o respeito mÃºtuo
        pode superar preconceitos raciais e culturais. O fidalgo portuguÃªs reconheceu
        que Peri tinha mais honra que muitos homens considerados civilizados.
        
        EPÃLOGO - O SIMBOLISMO DA OBRA
        
        O romance de JosÃ© de Alencar simboliza o encontro entre duas culturas: a indÃ­gena
        e a europeia. A uniÃ£o final de Peri e CecÃ­lia representa a formaÃ§Ã£o da raÃ§a brasileira,
        mestiÃ§a e forte, que herdaria as melhores qualidades de ambos os povos.
        """
    
    def _criar_chunks_otimizados(self, texto: str) -> List[str]:
        """Cria chunks otimizados para busca"""
        self._log("ðŸ”„ Criando chunks otimizados...")
        
        # Limpeza inicial
        texto_limpo = self._limpar_texto(texto)
        
        chunks = []
        
        # EstratÃ©gia 1: Dividir por capÃ­tulos se existirem
        if 'CAPÃTULO' in texto_limpo or 'CAPITULO' in texto_limpo:
            capitulos = re.split(r'CAP[ÃI]TULO\s+[IVX\d]+', texto_limpo)
            for cap in capitulos:
                if len(cap.strip()) > 100:
                    # Subdividir capÃ­tulos longos em parÃ¡grafos
                    paragrafos = [p.strip() for p in cap.split('\n\n') if len(p.strip()) > 50]
                    chunks.extend(paragrafos)
        else:
            # EstratÃ©gia 2: Dividir por parÃ¡grafos
            paragrafos = [p.strip() for p in texto_limpo.split('\n\n') if len(p.strip()) > 50]
            if paragrafos:
                chunks = paragrafos
            else:
                # EstratÃ©gia 3: Dividir por sentenÃ§as
                sentencas = re.split(r'[.!?]+', texto_limpo)
                chunks = [s.strip() for s in sentencas if len(s.strip()) > 30]
        
        # Criar chunks com sobreposiÃ§Ã£o para capturar contexto
        chunks_com_overlap = self._adicionar_overlap(chunks)
        
        self._log(f"ðŸ“ Criados {len(chunks_com_overlap)} chunks (incluindo overlap)")
        
        # Debug: mostrar exemplos
        if self.debug and chunks_com_overlap:
            for i, chunk in enumerate(chunks_com_overlap[:3]):
                self._log(f"Exemplo chunk {i+1}: {chunk[:80]}...")
        
        return chunks_com_overlap
    
    def _adicionar_overlap(self, chunks: List[str]) -> List[str]:
        """Adiciona sobreposiÃ§Ã£o entre chunks para manter contexto"""
        if len(chunks) <= 1:
            return chunks
        
        chunks_overlap = []
        
        for i, chunk in enumerate(chunks):
            chunks_overlap.append(chunk)
            
            # Adicionar chunk combinado com o prÃ³ximo (se existir)
            if i < len(chunks) - 1:
                chunk_combinado = chunk + " " + chunks[i + 1]
                if len(chunk_combinado) < 1000:  # Evitar chunks muito longos
                    chunks_overlap.append(chunk_combinado)
        
        return chunks_overlap
    
    def _limpar_texto(self, texto: str) -> str:
        """Limpeza mÃ­nima preservando contexto"""
        # Normalizar espaÃ§os e quebras de linha
        texto = re.sub(r'\n+', '\n\n', texto)
        texto = re.sub(r'\s+', ' ', texto)
        
        # Remover apenas caracteres realmente problemÃ¡ticos
        texto = re.sub(r'[^\w\sÃ¡Ã©Ã­Ã³ÃºÃ¢ÃªÃ®Ã´Ã»Ã£ÃµÃ§ÃÃ‰ÃÃ“ÃšÃ‚ÃŠÃŽÃ”Ã›ÃƒÃ•Ã‡.,!?;:()\-"]', '', texto)
        
        return texto.strip()
    
    def _vetorizar_chunks(self) -> bool:
        """Cria vetores TF-IDF otimizados"""
        self._log("ðŸ”¤ Criando vetores TF-IDF...")
        
        try:
            # ConfiguraÃ§Ã£o otimizada do TF-IDF
            self.vectorizer = TfidfVectorizer(
                max_features=2000,      # VocabulÃ¡rio amplo
                ngram_range=(1, 3),     # Uni, bi e trigramas
                stop_words=None,        # NÃ£o remover stop words
                lowercase=True,
                min_df=1,               # FrequÃªncia mÃ­nima
                max_df=0.95,           # FrequÃªncia mÃ¡xima
                token_pattern=r'\b\w+\b',  # Qualquer palavra
                norm='l2',              # NormalizaÃ§Ã£o L2
                use_idf=True,           # Usar IDF
                smooth_idf=True,        # Suavizar IDF
                sublinear_tf=True       # Usar log(tf) + 1
            )
            
            # PrÃ©-processar chunks minimamente
            chunks_processados = [self._preprocessar_chunk(chunk) for chunk in self.text_chunks]
            
            # Criar matriz de vetores
            self.chunk_vectors = self.vectorizer.fit_transform(chunks_processados)
            
            # EstatÃ­sticas
            vocab_size = len(self.vectorizer.vocabulary_)
            matrix_shape = self.chunk_vectors.shape
            density = self.chunk_vectors.nnz / (matrix_shape[0] * matrix_shape[1])
            
            self._log(f"âœ… VocabulÃ¡rio: {vocab_size} termos")
            self._log(f"ðŸ“Š Matriz: {matrix_shape} (densidade: {density:.3f})")
            
            return True
            
        except Exception as e:
            self._log(f"Erro na vetorizaÃ§Ã£o: {e}")
            return False
    
    def _preprocessar_chunk(self, chunk: str) -> str:
        """PrÃ©-processamento mÃ­nimo para manter contexto"""
        # Apenas conversÃ£o para minÃºsculas e normalizaÃ§Ã£o de espaÃ§os
        chunk = chunk.lower()
        chunk = re.sub(r'\s+', ' ', chunk)
        return chunk.strip()
    
    def consultar(self, pergunta: str, debug_detalhado=False) -> Dict:
        """Consulta principal do sistema"""
        if debug_detalhado:
            self._log(f"ðŸ” CONSULTA: {pergunta}")
        
        # PrÃ©-processar pergunta
        pergunta_proc = self._preprocessar_chunk(pergunta)
        
        # Vetorizar pergunta
        try:
            vetor_pergunta = self.vectorizer.transform([pergunta_proc])
        except Exception as e:
            self._log(f"Erro na vetorizaÃ§Ã£o da pergunta: {e}")
            return self._resposta_erro()
        
        # Calcular similaridades
        similaridades = cosine_similarity(vetor_pergunta, self.chunk_vectors).flatten()
        
        # EstatÃ­sticas de similaridade
        max_sim = np.max(similaridades) if len(similaridades) > 0 else 0
        mean_sim = np.mean(similaridades) if len(similaridades) > 0 else 0
        
        if debug_detalhado:
            self._log(f"ðŸ“Š Sim. mÃ¡xima: {max_sim:.4f}, mÃ©dia: {mean_sim:.4f}")
        
        # Busca com limiar adaptativo
        chunks_relevantes = self._buscar_chunks_relevantes(similaridades, debug_detalhado)
        
        # Gerar resposta
        resposta_data = self._gerar_resposta(pergunta, chunks_relevantes, max_sim)
        
        # Registrar no histÃ³rico
        self.conversation_history.append({
            'timestamp': datetime.now().strftime("%H:%M:%S"),
            'pergunta': pergunta,
            'resposta': resposta_data['resposta'][:100] + "..." if len(resposta_data['resposta']) > 100 else resposta_data['resposta'],
            'chunks_encontrados': len(chunks_relevantes),
            'similaridade_max': max_sim,
            'confianca': resposta_data['confianca']
        })
        
        return resposta_data
    
    def _buscar_chunks_relevantes(self, similaridades: np.ndarray, debug: bool = False) -> List[Dict]:
        """Busca chunks relevantes com estratÃ©gias mÃºltiplas"""
        chunks_relevantes = []
        
        # EstratÃ©gia 1: Limiar fixo
        for i, sim in enumerate(similaridades):
            if sim >= self.similarity_threshold:
                chunks_relevantes.append({
                    'indice': i,
                    'texto': self.text_chunks[i],
                    'similaridade': sim
                })
        
        # EstratÃ©gia 2: Limiar adaptativo se nÃ£o encontrou nada
        if not chunks_relevantes and np.max(similaridades) > 0:
            limiar_adaptativo = max(self.min_adaptive_threshold, np.max(similaridades) * 0.3)
            
            if debug:
                self._log(f"ðŸ”§ Usando limiar adaptativo: {limiar_adaptativo:.4f}")
            
            for i, sim in enumerate(similaridades):
                if sim >= limiar_adaptativo:
                    chunks_relevantes.append({
                        'indice': i,
                        'texto': self.text_chunks[i],
                        'similaridade': sim
                    })
        
        # EstratÃ©gia 3: Top-K absoluto como Ãºltimo recurso
        if not chunks_relevantes:
            if debug:
                self._log("ðŸ”§ Usando top-3 absoluto")
            
            indices_ordenados = np.argsort(similaridades)[::-1]
            for i in indices_ordenados[:3]:
                if similaridades[i] > 0:
                    chunks_relevantes.append({
                        'indice': i,
                        'texto': self.text_chunks[i],
                        'similaridade': similaridades[i]
                    })
        
        # Ordenar por similaridade
        chunks_relevantes.sort(key=lambda x: x['similaridade'], reverse=True)
        
        if debug:
            self._log(f"âœ… Encontrados {len(chunks_relevantes)} chunks relevantes")
        
        return chunks_relevantes[:self.top_chunks]
    
    def _gerar_resposta(self, pergunta: str, chunks: List[Dict], sim_max: float) -> Dict:
        """Gera resposta baseada nos chunks encontrados"""
        if not chunks:
            return {
                'resposta': "NÃ£o encontrei informaÃ§Ãµes especÃ­ficas sobre sua pergunta no texto de 'O Guarani'. Tente reformular a pergunta ou ser mais especÃ­fico sobre personagens, eventos ou temas da obra.",
                'confianca': 'N/A',
                'chunks_usados': 0,
                'similaridade_max': 0
            }
        
        # Combinar informaÃ§Ãµes dos melhores chunks
        if len(chunks) == 1:
            texto_resposta = chunks[0]['texto']
            intro = "Com base no texto de 'O Guarani':\n\n"
        else:
            # Combinar mÃºltiplos chunks relevantes
            textos = [chunk['texto'] for chunk in chunks[:2]]
            texto_resposta = " ".join(textos)
            intro = "Combinando informaÃ§Ãµes de 'O Guarani':\n\n"
        
        # Truncar se muito longo
        if len(texto_resposta) > 600:
            texto_resposta = texto_resposta[:600] + "..."
        
        resposta_final = intro + texto_resposta
        
        # Determinar confianÃ§a
        if sim_max > 0.4:
            confianca = "Alta"
        elif sim_max > 0.15:
            confianca = "Moderada"
        else:
            confianca = "Baixa"
        
        return {
            'resposta': resposta_final,
            'confianca': confianca,
            'chunks_usados': len(chunks),
            'similaridade_max': sim_max
        }
    
    def _resposta_erro(self) -> Dict:
        """Resposta padrÃ£o para erros"""
        return {
            'resposta': "Ocorreu um erro ao processar sua pergunta. Tente novamente.",
            'confianca': 'Erro',
            'chunks_usados': 0,
            'similaridade_max': 0
        }
    
    def chat_interativo(self):
        """Interface de chat interativo melhorada"""
        print("\n" + "="*70)
        print("ðŸ’¬ CHATBOT O GUARANI - VERSÃƒO OTIMIZADA")
        print("Especialista na obra de JosÃ© de Alencar")
        print("="*70)
        print("ðŸ“š Perguntas sugeridas:")
        print("  â€¢ Como evolui a relaÃ§Ã£o entre Peri e Dom AntÃ´nio?")
        print("  â€¢ Quem Ã© CecÃ­lia e qual sua importÃ¢ncia?")
        print("  â€¢ Descreva os personagens principais")
        print("  â€¢ Qual Ã© o enredo da histÃ³ria?")
        print("\nðŸ’¡ Digite 'sair' para encerrar, 'historico' para ver conversas anteriores")
        print("="*70)
        
        while True:
            try:
                pergunta = input("\nðŸ™‹ Sua pergunta: ").strip()
                
                if not pergunta:
                    continue
                
                if pergunta.lower() in ['sair', 'exit', 'quit']:
                    print("ðŸ‘‹ Obrigado por usar o Chatbot O Guarani!")
                    break
                
                if pergunta.lower() == 'historico':
                    self._mostrar_historico()
                    continue
                
                if pergunta.lower() == 'debug':
                    debug = input("Ativar debug detalhado? (s/n): ").lower().startswith('s')
                    self.debug = debug
                    print(f"Debug {'ativado' if debug else 'desativado'}")
                    continue
                
                # Processar pergunta
                resultado = self.consultar(pergunta, debug_detalhado=True)
                
                # Exibir resposta
                print(f"\nðŸ¤– Resposta:")
                print(resultado['resposta'])
                print(f"\nðŸ“Š ConfianÃ§a: {resultado['confianca']} | ")
                print(f"Chunks: {resultado['chunks_usados']} | ")
                print(f"Similaridade: {resultado['similaridade_max']:.3f}")
                
            except KeyboardInterrupt:
                print("\nðŸ‘‹ Encerrando chat...")
                break
            except Exception as e:
                print(f"\nâŒ Erro: {e}")
    
    def _mostrar_historico(self):
        """Mostra histÃ³rico de conversas"""
        if not self.conversation_history:
            print("ðŸ“ Nenhuma conversa no histÃ³rico ainda.")
            return
        
        print("\n" + "="*50)
        print("ðŸ“š HISTÃ“RICO DE CONVERSAS")
        print("="*50)
        
        for i, conv in enumerate(self.conversation_history[-10:], 1):  # Ãšltimas 10
            print(f"\n{i}. [{conv['timestamp']}] {conv['pergunta']}")
            print(f"   ðŸ¤– {conv['resposta']}")
            print(f"   ðŸ“Š {conv['confianca']} | Chunks: {conv['chunks_encontrados']} | Sim: {conv['similaridade_max']:.3f}")
    
    def teste_sistema(self):
        """Executa bateria de testes do sistema"""
        perguntas_teste = [
            "Como evolui a relaÃ§Ã£o entre Peri e Dom AntÃ´nio ao longo da histÃ³ria?",
            "Quem Ã© Peri e quais suas caracterÃ­sticas?",
            "Fale sobre CecÃ­lia e sua importÃ¢ncia na obra",
            "Qual Ã© o enredo principal de O Guarani?",
            "Quem sÃ£o os aimorÃ©s e qual seu papel?",
            "Descreva a natureza brasileira na obra",
            "Como JosÃ© de Alencar retrata o amor entre Peri e CecÃ­lia?",
            "Qual o simbolismo da obra?"
        ]
        
        print("\nðŸ§ª EXECUTANDO BATERIA DE TESTES")
        print("="*60)
        
        sucessos = 0
        for i, pergunta in enumerate(perguntas_teste, 1):
            print(f"\nðŸ“‹ TESTE {i}: {pergunta}")
            resultado = self.consultar(pergunta, debug_detalhado=False)
            
            # Avaliar sucesso
            sucesso = (resultado['chunks_usados'] > 0 and 
                      resultado['confianca'] != 'N/A' and 
                      len(resultado['resposta']) > 50)
            
            if sucesso:
                sucessos += 1
                print(f"âœ… SUCESSO")
            else:
                print(f"âŒ FALHOU")
            
            print(f"   ðŸ“Š ConfianÃ§a: {resultado['confianca']}")
            print(f"   ðŸ” Chunks: {resultado['chunks_usados']}")
            print(f"   ðŸ“ˆ Similaridade: {resultado['similaridade_max']:.3f}")
            print(f"   ðŸ¤– Resposta: {resultado['resposta'][:80]}...")
        
        # EstatÃ­sticas finais
        taxa_sucesso = (sucessos / len(perguntas_teste)) * 100
        print(f"\nðŸ“ˆ RESULTADOS FINAIS:")
        print(f"   Testes executados: {len(perguntas_teste)}")
        print(f"   Sucessos: {sucessos}")
        print(f"   Taxa de sucesso: {taxa_sucesso:.1f}%")
        
        return taxa_sucesso
    
    def diagnosticar_sistema(self):
        """DiagnÃ³stica o sistema e mostra estatÃ­sticas"""
        print("\nðŸ”§ DIAGNÃ“STICO DO SISTEMA")
        print("="*50)
        
        print(f"ðŸ“– Texto original: {len(self.original_text):,} caracteres")
        print(f"ðŸ“ Total de chunks: {len(self.text_chunks)}")
        
        if self.chunk_vectors is not None:
            print(f"ðŸ”¤ VocabulÃ¡rio: {len(self.vectorizer.vocabulary_):,} termos")
            print(f"ðŸ“Š Matriz de vetores: {self.chunk_vectors.shape}")
            densidade = self.chunk_vectors.nnz / (self.chunk_vectors.shape[0] * self.chunk_vectors.shape[1])
            print(f"ðŸ“ˆ Densidade da matriz: {densidade:.4f}")
        
        print(f"âš™ï¸  Limiar de similaridade: {self.similarity_threshold}")
        print(f"ðŸŽ¯ Top chunks retornados: {self.top_chunks}")
        print(f"ðŸ’¬ Conversas no histÃ³rico: {len(self.conversation_history)}")
        print(f"ðŸ“ Logs de processamento: {len(self.processing_log)}")
        
        # Testar consulta simples
        print(f"\nðŸ§ª Teste rÃ¡pido:")
        resultado = self.consultar("Quem Ã© Peri?", debug_detalhado=False)
        print(f"   Status: {'âœ… OK' if resultado['chunks_usados'] > 0 else 'âŒ Falhou'}")
        print(f"   ConfianÃ§a: {resultado['confianca']}")


def main():
    """FunÃ§Ã£o principal para execuÃ§Ã£o"""
    print("ðŸš€ CHATBOT O GUARANI - VERSÃƒO OTIMIZADA")
    print("="*60)
    
    # Inicializar chatbot
    chatbot = GuaraniChatbotOtimizado(debug=True)
    
    # Menu de opÃ§Ãµes
    while True:
        print(f"\nðŸŽ¯ OPÃ‡Ã•ES DISPONÃVEIS:")
        print("1. Chat interativo")
        print("2. Teste do sistema")
        print("3. DiagnÃ³stico completo")
        print("4. Consulta Ãºnica")
        print("5. Ver histÃ³rico")
        print("6. Sair")
        
        try:
            opcao = input("\nEscolha uma opÃ§Ã£o (1-6): ").strip()
            
            if opcao == '1':
                chatbot.chat_interativo()
            
            elif opcao == '2':
                taxa_sucesso = chatbot.teste_sistema()
                if taxa_sucesso >= 80:
                    print("ðŸŽ‰ Sistema funcionando bem!")
                elif taxa_sucesso >= 60:
                    print("âš ï¸  Sistema funcional mas pode melhorar")
                else:
                    print("âŒ Sistema precisa de ajustes")
            
            elif opcao == '3':
                chatbot.diagnosticar_sistema()
            
            elif opcao == '4':
                pergunta = input("Digite sua pergunta: ").strip()
                if pergunta:
                    resultado = chatbot.consultar(pergunta, debug_detalhado=True)
                    print(f"\nðŸ¤– Resposta:")
                    print(resultado['resposta'])
                    print(f"\nðŸ“Š ConfianÃ§a: {resultado['confianca']}")
            
            elif opcao == '5':
                chatbot._mostrar_historico()
            
            elif opcao == '6':
                print("ðŸ‘‹ AtÃ© logo!")
                break
            
            else:
                print("âŒ OpÃ§Ã£o invÃ¡lida. Tente novamente.")
                
        except KeyboardInterrupt:
            print("\nðŸ‘‹ Encerrando...")
            break
        except Exception as e:
            print(f"âŒ Erro: {e}")


# ExecuÃ§Ã£o rÃ¡pida para teste
def teste_rapido():
    """ExecuÃ§Ã£o rÃ¡pida para validaÃ§Ã£o"""
    print("âš¡ TESTE RÃPIDO DO SISTEMA")
    print("="*40)
    
    chatbot = GuaraniChatbotOtimizado(debug=False)
    
    # Testar pergunta problemÃ¡tica original
    pergunta_teste = "Como evolui a relaÃ§Ã£o entre Peri e Dom Antonio ao longa da historia?"
    print(f"\nðŸ” Testando: {pergunta_teste}")
    
    resultado = chatbot.consultar(pergunta_teste, debug_detalhado=True)
    
    print(f"\nâœ… Resultado:")
    print(f"   Chunks encontrados: {resultado['chunks_usados']}")
    print(f"   ConfianÃ§a: {resultado['confianca']}")
    print(f"   Similaridade: {resultado['similaridade_max']:.3f}")
    print(f"\nðŸ¤– Resposta:")
    print(resultado['resposta'])
    
    return chatbot


if __name__ == "__main__":
    import sys
    
    # Verificar argumentos
    if len(sys.argv) > 1 and sys.argv[1] == "--teste":
        teste_rapido()
    else:
        main()